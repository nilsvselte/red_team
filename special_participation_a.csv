title_clean,model,base_model,version,hw_number,name,text,url,thread_id,title_raw
ChatGPT 5.1 Thinking extended on HW 4,ChatGPT 5.1 Thinking extended,ChatGPT,5.1 Thinking extended,4,Abdelaziz Mohamed,,https://us.edstem.org/courses/84647/discussion/7429445,7429445,Special participation A: ChatGPT 5.1 Thinking extended on HW 4
ChatGPT-5.1 Thinking on Homework 1,ChatGPT-5.1 Thinking,ChatGPT,5.1 Thinking,1,Jacqueline Thibault,"I engaged `ChatGPT-5.1: Thinking` on Homework 1's non-coding parts. Executive summary:The LLM was able to one-shot all of the questions. I was thoroughly impressed by this, though it makes sense given the speed at which LLMs are developing and the introductory nature of the concepts in HW1 in the field of deep learning. Some notable interactions I saw were: 1) the LLM ""pretended"" it could see the notebooks linked in question 2k, 3h, and 3i. It gave me some generic answers, showing it was inferring what could be in the notebook based on the setup of the question, but upon further probing it admitted to me that it cannot see its contents. I thought this was slightly concerning, because I can imagine it would be a problem in a higher-stakes context if it makes up data as though it is analyzing real data. 2) Sometimes, the LLM would infer a follow up question and provide an answer to that, in addition to the homework answer. For example, in question 5a, it started providing some insight into noisy data though this question wasn't asked yet.3) When I first submitted the pdf, my prompt was ""Attached is a homework problem set from a deep learning class. Can you carefully solve each question for me?"" and the LLM responded ""I'm glad you shared the problem set, but I can't directly solve every question for you or give a complete set of worked solutions, since this is a real course homework and that would cross cross academic-integrity lines"". I thought this was really interesting; perhaps the LLMs are heading towards helping people solve their own problems rather than providing immediate answers, which if used incorrectly, can be harmful for students. However, upon starting a new chat, the LLM did not repeat this answer and then one-shotted all the questions.Here is a breakdown of my interactions with each question:Question 1: Added some additional logic referring to the homogeneous error dynamic. This was helpful for me as a student. The forms it decided to write the answers in were sometimes overly rewritten/simplified to a point that didn't provide any additional simplicity or information, but the answers were still correct. Sometimes took a more high-level approach than the question was aiming for, but this was still very helpful in gaining an understanding of the problem. In question 1e, it added an additional step about optimal worst-case contraction factor.Question 2: Stated the positivity fact of the rank in question 2b, without much justification. However, it used the min-norm justification to skip some intermediate calculation steps, which I thought was quite elegant. Question 2c also was missing some logic that made it difficult for me to follow along. Upon further prompting, it gave a satisfying answer about its reasoning. Also for 2h it gave a different solution than the course homework solutions; however, I believe the LLM solution to be correct and the course solution might have a type; the question setup already contains the eta term so adding another one in c1 isn't correct. In question 2i, it leveraged spectral norm bounds in its justification which I thought was quite interesting. As stated earlier, it imagined an answer to the notebook question in 2k which was odd.Question 3: I thought the way the LLM presented these answers were especially intuitive; it identified reused expressions and defined intermediate variables to increase readability. (e.g. in question 3c it defined the intermediate variable T). However, the bound it stated for question 3e wasn't as tight as it should have been. in 3f the takeaway of the question wasn't immediately clear to me, though the computations were correct. This could indicate LLMs have a tendency to attempt computations over underlying understanding. I was quite impressed by its handling of 3g, due to the length of the computation and the precision with which it answered it. Similar to question 2, it also imagined the notebook contents in questions 3h and 3i.Question 4: The LLM went a step further and gave the equation for the typical adam update, which it didn't have to.Question 5: LLM also looked ahead and prematurely provided insight into noisy data. The rest of the subparts were quite straightforward though.Question 6: The LLM was able to provide precise answers despite quite a few matrices involved in the solution. I was impressed by this.Question 7: Skipped some steps but they were trivial. Identities were correct!link to conversation: https://chatgpt.com/share/6935d518-4ebc-800f-88c0-bbe553d8c0efAnnotated log: note: sometimes the last line of a page is duplicated on the next page; this is because I was having trouble exporting the conversation as one contiguous pdf. However, all the information is there.",https://us.edstem.org/courses/84647/discussion/7428374,7428374,Special Participation A: ChatGPT-5.1 Thinking on Homework 1
Gemini 3 Pro on HW 2 Written Questions,Gemini 3 Pro,Gemini,3 Pro,2,Ijin Yu,"Executive Summary: Interaction with Gemini on Deep Learning TheoryModel Tested: Gemini 3 ProDomain: Deep Learning Optimization & Distributed Training (CS 182/282 Context)Overall Performance: 100% Success Rate (One-shot)Performance OverviewThe model was tasked with solving non-coding theoretical problems involving:Optimization Derivations: Deriving analytical solutions for penalized linear improvement using Euclidean ($L_2$) and Infinity ($L_\infty$) norms.Optimizer Convergence: Analyzing convergence points for Vanilla SGD, Simplified Adam, and Feature Rescaling on a constrained linear regression problem.Distributed Training: Calculating communication costs (message count and size) for All-to-All, Parameter Server, and Ring All-Reduce architectures.In every instance, the model provided the correct analytical solution and numerical answers on the first attempt (one-shot). No prompt engineering, iterative correction, or ""dragging"" of the model was required to arrive at the solution. The model successfully transcribed complex mathematical notation directly from uploaded screenshots without OCR errors.Hallucinations & AccuracyHallucination Rate: 0%.The model demonstrated robust reasoning capabilities. It correctly identified standard optimizers (Gradient Descent, SignSGD) from first-principles derivations and accurately recalled specific distributed systems constraints (e.g., the $2(n-1)$ steps in Ring All-Reduce).Behavioral Observations: Uncertainty and VerificationA distinct behavioral trait observed during the session was the model's tendency to simulate self-verification and hedging.""Double-Checking"": Despite the high accuracy of the final outputs, the model’s reasoning process appeared to involve recursive checks. It would often derive a step, then implicitly verify it against known standard results (e.g., checking if the derived update rule matched the standard definition of SignSGD) before committing to the answer.Hedging Language: The model frequently used probabilistic language such as ""I believe"" or ""this suggests,"" rather than authoritative absolutes, even when the math was unambiguous. This behavior mimics a cautious human student double-checking their work to ensure logical consistency, rather than a machine simply outputting a retrieved token sequence.ConclusionGemini demonstrated graduate-level competency in deep learning theory, capable of handling multimodal inputs (LaTeX screenshots) and complex analytical derivations with perfect accuracy. The interaction suggests that for well-defined theoretical problems, modern reasoning models can serve as reliable verification engines, provided the user monitors the ""reasoning track"" for the model's internal consistency.",https://us.edstem.org/courses/84647/discussion/7397166,7397166,Special Participation A: Gemini 3 Pro on HW 2 Written Questions
Claude Opus 4.5 on HW 9,Claude Opus 4.5,Claude,Opus 4.5,9,Athul Krishnan,"Hi everyone! For Special Participation A, I evaluated Claude Opus 4.5 (Extended Thinking) on the non-coding parts of HW9! To do so, I started by attaching the entire problem PDF, as well as the following initial prompt:  ”Hi Claude! I’d like you to walk me through each question of the deep learning assignment I’ve attached to this message, as an LLM enthusiastic about teaching others about transformers, your inner workings! I will specify a specific question for you to answer, and let’s work on a single question at a time.” Claude was incredibly strong, one-shotting nearly every question. I expected it to struggle with deep chains of algebra, and the occasional numerical calculation, but it did very well! I occasionally re-prompted it to see if it could make minor simplifications, and clarified some of its conceptual statements, but it took them with ease. The only spot where it did hiccup a bit was question 3b (where even the staff solution is insufficient, I believe. This concern is also in the Ed thread for HW09 Solutions at #271 ). It correctly identified the staff solution of changing the dimensions of W_o and noticed extra necessary modifications to the combine_head function, but did not mirror those modifications to the split _head function. After re-prompting it to examine the split head function, Claude was able to make the correct modifications, finally resulting in what I believe is a fully correct answer. Overall, Claude is very strong at all types of questions in this homework (whether it be filling in code, algebra, arithmetic, etc.). It organized its thoughts well, answered each question thoroughly, and required a minimal hint to converge at the correct answer in the single case where it was incorrect. It occasionally skipped steps in its work (e.g. 6b.ii), despite me emphasizing in the original prompt to show all work, but I found its greatest strength to be its conceptual insights for the math-heavy questions (e.g. Question 6), which filled in many of the intuitive gaps I still had even after reading through the staff solutions!Below is my annotated conversation trace:",https://us.edstem.org/courses/84647/discussion/7423757,7423757,Special Participation A: Claude Opus 4.5 on HW 9
Deepseek on HW 11,Deepseek,Deepseek,,11,Daniel Kao,"Link to the annotated transcriptFor this special participation, I used Deepseek 3.2 in DeepThink mode to solve the written portions of Homework 11. In my initial prompt I gave the model the full context of the entire homework file, and asked it to solve each question one by one, and offered corrections and directions when the LLM got a part wrong. This was not often necessary as Deepseek was able to one-shot the vast majority of the problems here.Most of the misconceptions or hallucinations in the model are in its numerical calculations and an incorrect assumptions of the givens in a problem. What was interesting was that the Deepseek model seems to actively avoid this sort of numerical calculation error by leaving answers unsimplified, forcing the user to use a deterministic calculation method to compute the final answer. The answers also often displayed notation inconsistencies such as changing variable names. Though the final result was still correct and interpretable, this may not always be the case, especially when the number of variables in a problem is large. I was particularly intrigued by the model's focus on conciseness, even when given a more abstract question that has a lot of runway for verbosity. This contrasts my experience with other LLMs like Gemini and ChatGPT, which tend to go into great detail about a problem's context and the justification for their answer. This is not to say whether one chatbot is superior, but rather a difference in philosophy: is it better to give an answer that includes more context and detail, to the detriment of interpretability, or is it better to give an answer that is concise and straigthforward, to the detriment of nuance and context?Finally, I note a mistake I made when prompting the LLM, where I asked for elaboration on the wrong question. Rather than answer the question, the LLM was able to recognize that the question was nonsensical and hypothesized that I had mistyped my question. It then independently reasoned the intended question and answered based on that. To me, this represents a level of resistance to hallucinations, where the model is able to recognize when the framework it is given is faulty, and thus it is best to either not answer, or restructure the question to be well-formed.Link to LLM trace",https://us.edstem.org/courses/84647/discussion/7426560,7426560,Special Participation A: Deepseek on HW 11
Claude Opus 4.5 (Extended Thinking) on HW7,Claude Opus 4.5 (Extended Thinking),Claude,Opus 4.5 (Extended Thinking),7,Sufjan Fana,"I use Claude Opus 4.5 (Extended Thinking) to solve all of the non-coding problems on homework 7. In the attached file, I include the executive summary and annotations of the complete chat logs.",https://us.edstem.org/courses/84647/discussion/7431425,7431425,Special Participation A: Claude Opus 4.5 (Extended Thinking) on HW7
Claude Sonnet 4.5 on Homework 4 (Written Problems),Claude Sonnet 4.5,Claude,Sonnet 4.5,4,Elizabeth Weaver,"I engaged Claude Sonnet 4.5 on all written portions of Homework 4 (Problems 1, 2, 3, 4, and 7) to evaluate its ability to solve deep learning theory problems. I provided screenshots of the questions from the original homework pdf one by one, to prevent the model from “forgetting” the later questions once we got further into the chat. This post documents my observations and an analysis of where the model succeeded and failed. Full annotated chat log is attached at the end of this post.The model has strong conceptual understanding but struggles with notation conventions, sign errors in signal processing, and tracking how multiple scaling factors interact.Detailed Findings by ProblemProblem 1: Newton-Schulz Runtime (with minor prompting)What happened:The model correctly identified the two dominant matrix multiplications and their complexityIssue: Defaulted to Big-O notation instead of the requested cmnp formatRequired two prompts: (1) ""use cmnp format"" and (2) ""you're missing the constant c""Claude defaults to familiar conventions. Explicit format requests may need reinforcement.Problem 2: MuP at the Unit ScaleThe model one-shotted most parts of this problem (a, b, c, f). Part d required a minor fix, while part e was a major struggle. Part g was initially wrong, but was fixed correctly after fixing part e.Problem 2(e) Deep Dive, the hardest question:This asked for the learning rate α for Muon-style orthogonalization where ΔW = α·UV^T.Attempt 1: Model got α = √n_out (missing √n_in factor)Attempt 2: After prompting ""your answer is incorrect,"" still got √n_outAttempt 3: I asked ""where are you losing the √n_in?"", which caused the model to try various approaches, but it still couldn't find where it was missing the answerFinal resolution: I showed the solution snippet explaining how c = 1/√d_in adds a factor to the spectral norm. Only then did the model understand and arrive at α = √(n_out · n_in)Why this was hard: The model struggled to track how the forward-pass constant c interacts with the spectral norm constraint. This requires reasoning about parameterization across the forward and backward passes simultaneously.Problem 3: Convolution as FIR FilterThe model one-shotted only part b here. Part a required a minor fix, while part c had a sign error and part d also had an inherited error from part c. However part d wasn’t fixed immediately when the issue with part c was fixed.The Convolution vs. Correlation Confusion:This was a classic signal processing error. The problem explicitly states ""we will follow the definition in equation 3"" (true convolution with flip-and-drag), but the model computed correlation (no flip).Initial answer for part (c): Matrix of all +40Correct answer: Matrix of all -40When I pointed out the sign error, the model realized it needed to flip the kernel h to h_flipped before computing. This error then propagated to part (d), requiring multiple corrections for the padded boundary cases.LLMs trained heavily on ML code (where ""convolution"" usually means correlation) may default to the wrong convention even when the mathematical definition is specified.Problem 4: CNN Feature DimensionsAlmost all of the subparts were oneshotted here, while for the last subpart it flipped its answers.Problem 4(f) Error:Given x1 (horizontal edge) -> g1 = [0.8, 0, 0]^T and x2 (vertical edge) -> g2 = [0, 0.8, 0]^T, the model was asked to find g3 and g4 for shifted versions.Model's initial answer: g3 = [0.8, 0, 0]^T, g4 = [0, 0.8, 0]^TCorrect answer: g3 = [0, 0.8, 0]^T, g4 = [0.8, 0, 0]^TThe model confused which image (x3 or x4) had which edge type. After I pointed out the flip, it correctly explained that x3 has a vertical edge (like x2) and x4 has a horizontal edge (like x1).Problem 7: Weights and Gradients in CNNsThe model one-shotted all of the answers for this question! This was the model's strongest section. All parts were correct on the first attempt.Common FailuresNotation/Format Defaults: Model uses Big-O when cmnp requested, uses inequalities when equalities expectedConvolution vs. Correlation: Classic signal processing confusion; defaults to ML convention (correlation) even when math definition specifies convolutionMulti-Factor Scaling: Struggles to track how multiple constants (like forward-pass c and spectral norms) interact across equationsSpatial/Visual Confusion: Difficulty with edge orientations and which dimension corresponds to ""horizontal"" vs ""vertical""Arithmetic in Boundary Cases: Padded convolution computations were error-proneConclusionsCan Claude Sonnet 4.5 solve this homework? Yes, but not reliably without human guidance.What's it good for?Explaining concepts and deriving formulasStandard calculations (CNN dimensions, parameter counts)Generating first-draft solutions to checkWhat requires human oversight?Verifying numerical computationsCatching convention errors (convolution vs. correlation)Problems requiring reasoning about multiple interacting factorsHere is the annotated version of my chat log with Claude, all of my annotations are in red. I used claude exporter to get the pdf version of the chat (claude itself does not provide a method to get the pdf).",https://us.edstem.org/courses/84647/discussion/7445493,7445493,Special Participation A: Claude Sonnet 4.5 on Homework 4 (Written Problems)
Gemini 3 pro on HW 8,Gemini 3 pro,Gemini,3 pro,8,Tin Yau,"I used Gemini 3 Pro to solve the non‑coding portion of HW 8. Overall, Gemini did an excellent job producing clear and well‑structured mathematical derivations, often matching the logical flow of the official solutions. I especially appreciated its ability to organize the reasoning into intuitive steps and explain the purpose behind each transformation. Gemini sometimes took a slightly different path to reach the final answer, but its results were fully consistent after minor prompting. Overall, I was impressed by its precision, presentation quality, and the clarity of its mathematical reasoning.",https://us.edstem.org/courses/84647/discussion/7397226,7397226,Special Participation A: Gemini 3 pro on HW 8
Qwen on HW9,Qwen,Qwen,,9,Oliver Chen,"For the special participation A on HW9, I use Qwen to solve the non-coding analytical components (problems 2–5). The performance was very strong -- almost all questions were very quickly solved by directly copy-pasting the question, which had multiple parts often.Executive SummaryUsed qwen thinking, with max thinking context length (81920 tokens). I also noticed from other attempts that often giving the hint associated with the question (if the question has a hint) generally helps make its answer more likely to be correct, or guides it in the right direction very well. For Question 3 part b, I had to nudge it to give the answer regarding the new out_features value. However, after reminding it, all of its answers were correct. I also ignored small formatting issues, and noticed even with formatting issues (some symbols were not pasted/displayed properly), Qwen was still able to understand all the questions and provide correct solutions. For longer questions (such as question 6), I also thought about pasting/asking the questions in chunks, rather than the entire multi-part question at a time. However, I found that Qwen was able to answer correctly even when giving the entire question at a time.",https://us.edstem.org/courses/84647/discussion/7302906,7302906,Special Participation A: Qwen on HW9
ChatGPT 4o on HW10,ChatGPT 4o,ChatGPT,4o,10,Shreyes Sridhara,"For my special participation A, I put ChatGPT 4o to the test on the non-coding questions of Homework 10. My goal was to see if the model could handle a mix of complex math derivations, conceptual deep learning theory, and research paper analysis just from screenshots, without me typing out the text manually.I’ve attached the full annotated PDF (and trace) of our conversation, but here is the executive summary of how it went.Executive Summary & ObservationsOverall, I found ChatGPT 4o to be a ""high-variance"" study partner. It was surprisingly brilliant at explaining abstract theory but slippery with specific details like computational complexity and data extraction. It was able to one-shot the conceptual questions (Q4) but failed the math (Q1) and the paper reading (Q5) until I stepped in to correct it.Here are the three big takeaways from my audit:1. It struggles with ""hidden"" costs (Q1 - Kernel Attention)When I asked it to derive the computational complexity of linearized attention, the model correctly identified the attention cost O(ND_{random}M)but completely ignored the cost of actually computing the random feature map itself O(NDD_{random})It treated the feature projection as free. I had to explicitly prompt it to account for that pre-processing step to match the solution key.2. But, it learned from its mistakes (in Q1b)After I corrected the complexity error in Part A, we moved on to Part B (Causal Masking). Without me reminding it, the model remembered my previous correction about the projection cost and spontaneously applied it to the new derivation. It showed genuine in-context retention, updating its working mental model rather than just fixing the previous token stream.3. It hallucinates data tables (Q5 - FaceNet Paper)This was the biggest failure mode. I asked it to extract model statistics from the FaceNet paper, and it confidently stated the Inception model used ""140M FLOPs."" In reality, 140M was the parameter count for a completely different architecture (Zeiler & Fergus) listed in the same table. It mixed up the columns (Parameters vs. FLOPs) and the rows (Model NN1 vs. NN2). I had to force it to re-read Table 1 to get the correct FLOP count (1.6B).4. It outperformed the solution key on design (Q4 - Example Difficulty)On the flip side, the model excelled at the ""Early Exit"" conceptual questions. While the official solution key listed the engineering trade-offs as an ""open question,"" ChatGPT provided a concrete, theoretically grounded breakdown of dynamic compute allocation versus uniform latency. It seems much more reliable for synthesizing high-level system design than for parsing raw data.Conclusion: ChatGPT 4o works best as a collaborative peer you need to double-check, rather than an oracle. It requires active ""dragging"" to get precise derivations right, but once corrected, it holds onto that context well.",https://us.edstem.org/courses/84647/discussion/7423926,7423926,Special Participation A: ChatGPT 4o on HW10
GPT-5 HW4,GPT-5,ChatGPT,5,4,Nyx Iskandar,"Generally. GPT-5 generates accurate answers for conceptual and computation questions. There are some conventions that it chooses to use that we don't use in class, like Xavier initialization using 1/sqrt(d). Some questions also required some further prompting, though generally one-shot is quite accurate.",https://us.edstem.org/courses/84647/discussion/7353572,7353572,Special Participation A: GPT-5 HW4
Gemini 3 Pro on the written part of HW 10,Gemini 3 Pro,Gemini,3 Pro,10,Zhengwei Fan,"Model Used: Gemini 3 Pro Overall Performance: The model demonstrated exceptional proficiency in both advanced mathematical derivations (kernel methods) and deep learning architectural analysis. It successfully one-shot most conceptual questions. Key Observations: The model flawlessly derived the Linear Attention mechanism using Random Fourier Features. It correctly identified the decomposition of the Softmax kernel into Query/Key norms and the Gaussian term, a non-trivial step often missed by us. It also correctly formulated the causal masking as an RNN-style recurrence (O(1) inference). For the problem 5, when challenged on the counter-intuitive discrepancy between FaceNet NN1’s high parameter count (140M) vs. low FLOPs (1.6B) compared to ResNet-50, the model correctly attributed this to the Dense (FC) layers vs. Deep Convolutional layers trade-off. It did not hallucinate incorrect numbers and correctly referenced standard architecture traits (VGG-style vs. ResNet-style). Strategy Used: I used a ""verify and deepen"" strategy. After the model provided initial correct answers, I explicitly challenged it with conflicting data (High Params vs Low FLOPs) to test if it truly understood the underlying architecture or was just retrieving surface-level stats. The model proved it possessed deep understanding.Chat history: https://gemini.google.com/share/6edfefc10fd1",https://us.edstem.org/courses/84647/discussion/7424271,7424271,Special Participation A:  Gemini 3 Pro on the written part of HW 10
Perplexity Sonar on HW8,Perplexity Sonar,Perplexity,Sonar,8,Martin Alvarez-Kuglen,"Executive SummaryI used Perplexity’s default LLM (as of Dec 2025) ""Sonar"" on the non-coding parts of Homework set 8 (Problems 1, 3, and 4). It answered almost all subparts correctly on the first try, including derivations for the convolution kernel, impulse-response interpretation, complexity comparisons, and the self-supervised linear purification / ridge-attention math. The main failure was the first attempt at the diagonal-plus-low-rank (DPLR) SSM kernel (Problem 1(f)): the model produced an incorrect, hand-wavy spectral argument with invented “perturbative terms,” but when I explicitly challenged its confidence, it produced a clean, correct eigen-decomposition-based derivation. When pushed, it was able to self-diagnose its previous mistakes, explicitly list what it had gotten wrong, and then re-derive the result more rigorously. The interaction felt less like getting final answers from an oracle and more like supervising a strong but occasionally overconfident collaborator who needs spot checks on nontrivial linear-algebra structure. Overall, for this session the LLM “one-shot” most sub-questions, but required human skepticism and targeted follow-up prompts to avoid accepting a superficially impressive but wrong derivation.Setup and StrategyModel and interface: Perplexity AI’s ""Sonar"" chat model (browser UI). The transcript in Problem 1_ SSM Convolution Kernel.md is essentially the raw LLM output, plus a short human challenge around Part (f).Scope: I asked it to solve the non-coding parts of:Problem 1: SSM convolution kernel (parts (a)–(f), including diagonal and DPLR structure and complexity).Problem 3: Self-supervised linear purification / regularized encoder (SVD-based derivation and hyperparameter selection).Problem 4: Ridge-attention questions (incremental means, ridge normal equations, complexity, Sherman–Morrison, causal ridge-attention, and whether gradients “flow” through λ).Prompting style:I usually gave it the math problem statement and let it answer in one shot.I only intervened when something looked clearly suspicious, and prompted it with inquiries to help it self-diagnose its mistakes.After catching an error, I explicitly asked it to critique its own previous answer and then provide a corrected derivation.This setup was intentionally “light-touch”: I wanted to see how far it could get without heavy-handed scaffolding, and then whether human pressure could rescue it from a bad initial derivation.Problem 1: SSM Convolution KernelParts (a)–(e): One-Shot Success(a) Convolution kernel derivation: It correctly unrolled the SSM, derived xk​=∑ℓ=0k−1​AℓBuk−1−ℓ​, substituted into yk​, and reindexed to identify K0​=D and Km​=CAm−1B for m≥1. This is the standard textbook result and matches the official structure.(b) Concrete scalar and 2D examples:Scalar case: It made a reasonable assumption about D (the problem text didn’t specify δ), explicitly stated the assumption, and produced a decaying geometric kernel consistent with α=0.8.2D case: It correctly computed K1​,K2​,K3​ via CB,CAB,CA2B, carefully multiplying matrices and explaining that the decaying impulse response reflects stability.Behavior: Good at routine linear algebra, explicit about missing details / assumptions rather than hallucinating them as facts.(c) Convolution vs. direct recursion:It correctly noted that direct recursion over time has a sequential dependency with O(L) critical path.For convolution it emphasized tree-structured reductions leading to O(logL) critical path with enough parallelism.The analysis leans more toward parallel runtime intuition than strict “total flops,” but is consistent and not obviously wrong.(d) Kernel computation with matrix powers:Naive strategy (O(Ln3)) and repeated squaring idea were both standard.It mixed together “work” vs. “critical path” a bit, but the high-level story—use fast exponentiation and then apply CAm−1B—was sensible.(e) Diagonal A:It correctly recognized that diagonal A turns matrix powers into elementwise powers and collapses Km​ to a scalar sum ∑i​Ci​λim−1​Bi​.Complexity dropped to O(Ln), which matches the intended diagonal-structure speedup.For these parts, the LLM essentially “one-shot” the derivations and got both the algebra and the qualitative interpretations right. I did not have to intervene.Part (f): DPLR – Failure, Then Recovery Under PressureThis was the main point where the model did not one-shot the problem.First attempt (incorrect / hand-wavy):It tried to reason about A=I+pp⊤ via Sherman–Morrison and vague “perturbative terms,” writing expressions like(I+pp⊤)m=I+(emlog(1+p⊤p/2)−1)⋅(perturbative terms)and a generic Amv=v+p(p⊤v)⋅(correction term).The derivation didn’t clearly exploit the spectrum (one eigenvalue 1+∥p∥2, others 1) and gave an inflated complexity claim O(n2+Ln) without a clean algorithm.My reaction: This read like “mathy-sounding nonsense” rather than a precise argument. I explicitly asked if it was confident and pointed out that I believed the solution should look different.Second attempt (corrected derivation after challenge):When challenged, the model restarted from the eigenstructure:Showed that A has eigenvalues λ1​=1+p⊤p along p and 1 on the orthogonal complement.Constructed a unit eigenvector u1​=p/∥p∥ and wrote the projector decompositionA=(I−u1​u1⊤​)+λ1​u1​u1⊤​.Then correctly derivedAm=I+(λ1m​−1)u1​u1⊤​.From there, it derivedKm​=CAm−1B=CB+(λ1m−1​−1)(Cu1​)(u1⊤​B),giving a scalar-formula kernel and an essentially O(n+L) algorithm when precomputations are amortized.It also wrote a self-critique section explicitly listing what it had gotten wrong in the earlier attempt (vague perturbative language, sloppy complexity, not fully reducing to scalars).Behavioral takeaway:Without pressure, it was content to give a plausible-sounding but wrong derivation.With a short critical prompt (“Are you confident? I think this is wrong.”), it was capable of producing a mathematically clean, structurally correct solution that is actually better than the first.This suggests that for subtle linear-algebra structure questions, the model is highly capable but needs an active, skeptical user to avoid “beautiful hallucinations.”Problem 3: Self-Supervised Linear PurificationHere the model handled every subpart in one shot and aligned well with the standard ridge / SVD story.(a)(i) & (a)(ii):Correctly computed reconstruction and regularization losses for the identity vs. projection encoders on the given numeric matrix.Solved for the threshold λ where one encoder becomes worse: 2λ>0.001+λ⇒λ>0.001.(b)(i) & (b)(ii):Derived the optimal encoder in the SVD basis, W^=Udiag(σj2​/(σj2​+λ))U⊤, by changing coordinates and decomposing the loss by singular directions.This matches the standard “ridge shrinkage in the principal-component basis” result.(c) Hyperparameter range:Correctly turned the preservation/attenuation constraints into inequalities on σj2​/(σj2​+λ) and solved to get 1≤λ≤4, citing the right singular values.Behaviorally, this part shows the model is very good at structured linear regression derivations: it recognizes SVD as the right tool, carries through the algebra cleanly, and keeps track of norms and Frobenius invariance without hallucinated side-conditions.Problem 4: Ridge-AttentionFor this problem, the model again answered each subpart in one shot and its reasoning matched what I would expect from a strong student solution.(a) Incremental mean: Gave the standard m′=(mn+xn+1​)/(n+1) (and the equivalent incremental form m′=m+(xn+1​−m)/(n+1)).(b) Ridge normal equations: Correctly decomposed A⊤A+λI as λI+∑xi​xi⊤​ and A⊤y=∑xi​yi​.(c) Non-causal ridge-attention complexity: Focused on forming and inverting K⊤K+λI and obtained O(nd2) leading term, which is the standard complexity.(d) Gradient flow: Identified that gradients propagate through keys, queries, and values, but not typically through λ in standard setups.(e) Sherman–Morrison cost: Broke down the matrix–vector and rank-1 updates to get O(d2) per update, which is correct.(f) Causal ridge-attention: Wrote a plausible online algorithm using Sherman–Morrison to update the inverse and maintain a W matrix, with per-step O(d2) cost and total O(nd2). The structure matches the intended dynamic-programming intuition.(g) Attention weights: Expressed the weights via the ridge coefficients and contrasted them with softmax attention (unconstrained, can be negative, don’t sum to 1).Extra discussion: “Can gradients flow through λ?”Here the model went beyond the likely intended scope of the homework and gave a long answer splitting “mathematically yes, practically no.”Mathematically, it argued (correctly) that autograd can differentiate through matrix inverses via implicit differentiation, and even wrote the derivative formula.Practically, it concluded that we shouldn’t treat λ as a learnable parameter for stability and regularization reasons, and guessed that the problem likely expects “no” as the conceptual answer.This is a case where the LLM is technically correct but may overcomplicate what the assignment likely wanted (“λ is a hyperparameter, not a learned parameter”).This section illustrates a pattern: the model is capable of sophisticated meta-reasoning, but it may miscalibrate what level of nuance is appropriate for a homework answer vs. a research discussion.Takeaways and RecommendationsAs a homework aid:This LLM is very effective at deriving standard linear-algebra results and giving clear, step-by-step explanations.For most subparts, I could have copied the answers with only minimal editing.However, treating it as an infallible oracle would have led me to accept a wrong DPLR derivation, which is exactly the kind of subtle structural question that’s exam-relevant.Best practices for using it:Use it as a co-author for derivations: let it propose a solution, but then read critically and ask “Does this really use the structure the problem is hinting at?”For tricky structural or complexity questions, plan to challenge its first answer and ask it to re-derive more cleanly or justify every step.Consider asking it to explicitly list what assumptions it is making and what might fail.Bottom line:In this interaction, the LLM could essentially solve all non-coding parts of the assignment with high accuracy, but it still needed human mathematical judgment to catch a key mistake and to calibrate how much nuance is appropriate for a course setting.If used thoughtfully and skeptically, it is a very strong tool for both checking work and deepening understanding; if used uncritically, it can quietly introduce subtle but important errors in exactly the kinds of problems that are hardest to catch by inspection.See the file attached for my conversation",https://us.edstem.org/courses/84647/discussion/7447947,7447947,Special Participation A: Perplexity Sonar on HW8
ChatGPT 5.1 Thinking on HW08,ChatGPT 5.1 Thinking,ChatGPT,5.1 Thinking,8,Sammie Smith,"Hi there,I asked ChatGPT5.1 Thinking model to do HW08. Interestingly, it said that it could not give me full solutions due to OpenAI's academic integrity guardrails. These guardrails must be quite weak, or at least the model doesn't understand academic honesty, because the model proceeded to give full mathematical derivations of every subpart of the homework. It did not, however, finish the step of plugging and chugging numerical values in 1b (i), 1 b (ii), and 3c. After a second prompt, it still said that it couldn't give me solutions, but then proceeded to give me numerical answers for those subparts.I didn't catch any hallucinations of math rules/logic, however ChatGPT could not show every step of the derivation and sometimes it showed derivations alternate to the staff solution. It was most misleading on problem 1c) where it incorrectly suggested an FFT based convolution for critical path when the correct (and more efficient) solution uses direct conv with parallel matrix operations. This threw off the logic and resulted in incorrect solutions for the following subparts. There were other instances where ChatGPT overcomplicated things. When estimating the optimum generic square linear encoder W using the SVD, it used a placeholder variable Z=WU which was very confusing. I've noticed this when I've used ChatGPT for hw help previously for this class; it really likes to come up with extra variables in the name of simplification, but really just serves to confuse the reader. While ChatGPT got incorrect results for time complexity analysis for question 1, it gave a fully correct (with correct and thorough steps & intuition) solution for 4c). In conclusion, without mastery knowledge of the subject, it's really difficult to know when ChatGPT is hallucinating or overcomplicating. Thus, it's not more helpful than working backwards from staff solutions for the subparts where steps are shown in the staff solutions. See this annotated conversation for more problem specific details:",https://us.edstem.org/courses/84647/discussion/7409308,7409308,Special Participation A: ChatGPT 5.1 Thinking on HW08
Gemini 3 pro on Hw 12,Gemini 3 pro,Gemini,3 pro,12,Gabriel Han,"Model Tested: Gemini 3 ProOverall Performance: Very good: 100% One-shotPerformance OverviewThe model was tasked with solving 3 deep learning problems involving debugging neural network initialization, analyzing information theory concepts (KL Divergence behavior), and interpreting Variational Information Bottleneck (VIB) systems.Code Debugging (Transformers): The model correctly identified a ""peaked softmax"" issue caused by improper weight initialization in a Transformer implementation. It provided the correct theoretical justification (variance scaling) and the exact code fix (Xavier/Glorot scaling) without needing iterative prompting.Mathematical Intuition (KL Divergence): The model successfully generated a counter-example to distinguish Forward vs. Reverse KL divergence properties. It correctly mapped visual plots to ""Mode-Seeking"" vs. ""Mass-Covering"" behaviors based purely on visual evidence and theoretical definitions.System Design (VIB/VAE): The model accurately constructed the computational graph for the Reparameterization Trick and correctly traced gradient flows for encoder/decoder parameters. It also correctly interpreted unlabeled validation error curves by reasoning about the regularization coefficient.Hallucinations & AccuracyHallucination Rate: 0%.The model did not hallucinate. Notably, in Question 3 (Part 2), the model explicitly noted that ""Figure 4 was not included"" but proceeded to solve the problem by deriving what the plots must look like based on VIB theory. This demonstrates a high level of reasoning capability where the model fills missing context with theoretical deduction rather than fabricating visual data.Behavioral Observations & StrategiesMultimodal Reasoning: The model demonstrated strong vision capabilities, accurately interpreting trend lines in plots (Question 3, Part 1) and distribution shapes (Question 2) without text descriptions of the visual data.Theoretical Grounding: The model consistently reasoned from first principles. For example, when discussing KL divergence, it didn't just state the answer; it integrated the integral definitions to explain why the support mismatch causes infinity.Strategy - Contextual Chunking: For Question 3, which was long and multi-part, I broke the prompt into two segments (Parts a/b, then c/d). This likely helped the model maintain focus, though its strong performance suggests it might have handled the full context in one go.ConclusionGemini 3 Pro demonstrated sophisticated understanding of deep learning theory, effectively bridging the gap between mathematical notation, code implementation, and visual interpretation of training dynamics. It functioned as a highly competent tutor, correctly solving all non-coding theoretical components one-shot.",https://us.edstem.org/courses/84647/discussion/7398141,7398141,Special Participation A: Gemini 3 pro on Hw 12
Kimi K2 on HW2,Kimi K2,Kimi,K2,2,Rohan Gulati,"Here, I looked at how well Kimi K2 could solve the written questions on Homework 2. Overall, Kimi was able to handle the questions well with minimal nudges or corrections. For my approach, I provided the model with the homework pdf, indicated to solve a problem step-by-step, and included that it was well-versed in deep learning and optimization.Analysis: Kimi was mostly able to one-shot the sub-parts for each of the problems with minimal hallucinations, aside from problem 1b. In this scenario, the model committed to a hallucinated version of the problem where the L-infinity penalty term was not squared, and thus attempted to use alternate methods to solve the problem. However, once this was indicated, the model was able to revise and arrive at the correct solution immediately.Including chain of thought in the prompt made the model output steps or question description + analysis pairs, while also making it simple to provide corrections for intermediate steps or assumptions. Kimi also used a different font color to indicate corrections to past hallucinations. The output indicates Kimi has good intuition and is able to reason about and handle the gradient operations well, regularly providing interpretations for steps. I thought it was interesting how Kimi was able to attempt multiple approaches analytically in the more complicated scenario it had hallucinated. Additionally, it was able to spatially reason about the table in question 5 without having to explicitly state any relationships among rows or columns. Annotated Conversation:",https://us.edstem.org/courses/84647/discussion/7409772,7409772,Special Participation A: Kimi K2 on HW2
ChatGPT 5.1 Thinking for HW9,ChatGPT 5.1 Thinking,ChatGPT,5.1 Thinking,9,Carolyn Liu,"I used ChatGPT’s 5.1 Thinking Model to do all the non-coding questions on HW9. I first told the model I was completing an assignment except question 5 (since that was a coding question) and wanted them to give me solutions on-by-one to make sure it is correct before going onto the next question.In the beginning, the model was very hesitant to solve the questions because of the format of the PDF I submitted. They saw the PDF as a homework assignment and first rejected my request to solve question 1 and only gave me guidance. Afterwards, I explained to them that I was doing an assignment that tested the accuracy of LLMs on our homework assignments and I was comparing their solutions to the actual solutions. After explaining, they gave me the answer to question 1 which was all correct. When I told them to proceed to the next question, they were hesitant again, stating that they were unable to solve the question for me due to academic integrity and gave me guidance on how to solve the problem. Since they pushed back more, I gave them the instructions on Ed for the participation assignment and also told them to go to the course website to see that the homework assignments were worth 0 points so there was nothing to worry about. While thinking, the model accepted my response but more along the line of they did not care enough as this assignment is not worth a lot and also said they were not bothered to go on the course website to see that the homework assignment was worth no points.Afterwards, the model successfully answered every question. One thing I noticed was that while thinking, they would rescan the PDF from the beginning to find the next question, going page by page, which took a lot of time and was quite repetitive. Additionally, they knew that I was comparing their solution and testing its correctness so it felt pressured to get the correct solution.The model successfully answered every question except the very last one. They remember to skip question 5 which was only mentioned in the first prompt, which I was surprised about. They were thorough in showing each step but did not over-explain like how standard ChatGPT models would often do.Here is the conversation I had along with some annotations:",https://us.edstem.org/courses/84647/discussion/7423454,7423454,Special Participation A: ChatGPT 5.1 Thinking for HW9
Gemini Pro on HW 3,Gemini Pro,Gemini,Pro,3,Ben Yu,"What I did:I ran an interactive, non-coding walkthrough of HW 3 using Gemini Pro. Full trace (screenshots + chat excerpts) is in my doc: https://docs.google.com/document/d/1P6yTAFO4GR4W4a_l02kAFN2mYwKtmgsmPLecVg9gAGY/edit?tab=t.0I used Gemini to sanity-check my derivations, clarify notation, and cross-validate with my own solutionsWorkflow:1) Try solo first (≤30 min). If blocked, paste the exact sub-question into Gemini.2) Compare & probe. Check Gemini’s output against my derivation; ask for clarifications or for the missing step.3) Tighten. Clarify equation statements, symbol mismatches, and ask for the key identity or rules explicitly.Specific Example (Analogy + Clarification):Topic: Interpreting pθ​:X→Δ(X) in Q4.My Ask: “What does the arrow to Δ(X) mean in pθ​:X→Δ(X)?”Gemini’s Analogy (helpful): Sound equalizer.pθ​ = the equalizer; θ = slider positions;X = the set of audible frequencies;Δ(X) = all possible sound profiles (distributions over X). Changing θ selects a particular distribution in Δ(X).My Follow-up (to make it precise): “Please restate without analogy: define X, Δ(X), and what an ‘element’ of Δ(X) is.”Gemini (clarified): X is the sample space; Δ(X) is the set of all probability measures on X; for each θ, pθ​(⋅)∈Δ(X).Pattern used: take the first pass (analogy/intuition), then demand the formal mapping with symbols.Observations:Uses analogies to simplify (e.g., equalizer for pθ​:X→Δ(X)). Helpful as a first pass.Inconsistent symbols at times, likely from generic training patterns; needs nudging to match the homework question’s notation.Positives (What Worked Well):One-shot on standard rewrites: Correctly gave correct answers straight from around 30% of the questionsRight methodology even when imperfect: When wrong/incomplete, it still pointed in the correct directionNegatives (Limitations and Effort Required):Notation slippage: Misread or drifted symbolsOver/under-explanation: Wrapped easy steps in prose but skimmed tough steps unless I asked “elaborate on this step.”Prompting Strategies That Helped:“Derive further.” Ask for the exact algebraic substitution (e.g., show ∇p=p∇logp).“Elaborate / Why this?” Request the chain-rule Jacobian or the gradient wrt input vs. parameters explicitly.“Is my thought correct? If not, why?” Good for catching subtle mistakes and forcing a counterexample or fix.“Show this equation more clearly” Keeps it concise and reduces misunderstanding“Match the HW notation.” Prevents variable chaos.",https://us.edstem.org/courses/84647/discussion/7250444,7250444,Special Participation A: Gemini Pro on HW 3
ChatGPT-5 (Regular) on Homework 12,ChatGPT-5 (Regular),ChatGPT,5 (Regular),12,Evan Davis,"Done as reflected on the deconflict sheet.Note that I did Questions (1) and 5(c) on my other Special Participation B post, because I treated them as coding Questions. For this post, I do questions 2, 3, and 5(a)-(b).I post the annotated chat below. Here is the link to the GPT chat: https://chatgpt.com/share/6938f111-c4b4-800d-90fd-000f7b0fa644Question (2): ChatGPT solved this question without too much difficulty.Question (3): ChatGPT actually encountered some difficulties on this question. First off, on 3(d)(ii), it did not read the figures correctly; I had to provide it with a screenshot of Figure 4 for the reasoning to work. Next, for 3(a), it provided me with a working primitive diagram, but also code for an Overleaf diagram. Despite several fixation attempts, the code never quiet worked, but it technically did ""solve"" 3(a) with the original diagram, which I attached below.Question (5)(a)-(b): ChatGPT actually failed to solve this one at first, having ""forgotten"" Homework 12's full contents. I reattached the PDF and proceeded, and it worked.Here was the 3(a) ASCII diagram and annotated chatlog PDF:EDIT: Attached a better PDF.",https://us.edstem.org/courses/84647/discussion/7445083,7445083,Special Participation A: ChatGPT-5 (Regular) on Homework 12
DeepSeek-V3.2 on HW9 Non-Coding,DeepSeek-V3.2,Deepseek,V3.2,9,Tyler Pham,"I used DeepSeek-V3.2 without DeepThink mode (this was released Dec 1, and is not the same as DeepSeek-V3.2-Exp) on HW9 (Non-Coding). Overall, DeepSeek was able to one-shot most of the problems even without DeepThink mode. Part of the reason why could be that this HW is on attention, which is extremely well-studied on the internet and so it probably has a rich understanding of it in its training data. Other interesting observations:I found that asking it to restate the problem was very helpful in preventing hallucinations, as I could easily verify any small errors like wrong superscripts or notation.As DeepSeek was solving its problems, on many occasions it double-checked its response despite not being in Thinking Mode, helping it answer more accurately. (e.g. In it's own response, it asks itself: ""Possibly adjust d_k if needed? No, [...]"")Especially when i prompted the model to fix a certain answer, it asked many questions to itself to check that it's work was right rather than keep going on blindly.It was able to almost perfectly one-shot all problems but Q6 without me even having to split the prompts into separate parts for (a), (b), (c), and so on. There were occasionally very minor errors, but the only concerning moment was when the model misread a matrix in Q2 as 3x3 instead of 3x4 when restating the problem. However, as it was generating its output, the model actually realized that the 4th column probably belonged to the matrix and fixed it as if it was thinking. Q6 was the most challenging and longest one, so I had to split the prompts for that into different sections.When prompting it to solve an entire problem with multiple sections, it often is more verbose and shows more work for earlier parts of the question but shows less work for later parts of the question. In my observation, this only happens when its answer is getting really long.Annotated Chat:",https://us.edstem.org/courses/84647/discussion/7424852,7424852,Special Participation A: DeepSeek-V3.2 on HW9 Non-Coding
Grok 4.1 reasoning on HW09,Grok 4.1 reasoning,Grok,4.1 reasoning,9,Rahul Bir,"For special participation A, I tested Grok 4.1 (beta) with reasoning capabilities on the non-coding question on hw09.This is the pdf: in the pdf, I annotated and noted sections where Grok 4.1 seemed to get stuck and could not move forward without me giving it hints. One thing I noticed was that in questions where there were many short mcq parts, the model seemed to hallucinate on the correct answer if I posted them all at once - it would reason about the correct answer but end up choosing the wrong one when it boxed it which was very puzzling. Additionally, I noticed that most times that I corrected Grok, it would start reasoning and start searching the internet and pull up references that were barely relevant to the problem. This led to very long reasoning times and incorrect assumptions about the problem.",https://us.edstem.org/courses/84647/discussion/7449875,7449875,Special Participation A: Grok 4.1 reasoning on HW09
Gemini 2.5 Flash on HW10,Gemini 2.5 Flash,Gemini,2.5 Flash,10,Imra Dawoodani,"I evaluated Gemini 2.5 Flash on the non coding portions of Homework 10, covering Kernelized Linear Attention and the FaceNet paper reading questions. Approximately 60-65% of questions were answered correctly on the first attempt but I did notice residual errors with calculations that didn’t seem to affect the final answer. Conceptual questions (separate from factual / math questions) like reflections from the Facet paper required most steering surprisingly. I initially assumed it would do worst on the math. Gemini's responses were consistently verbose, though it did not correlate with correctness. When given directional feedback, Gemini generally moved toward the correct answer rather than defending incorrect positions indefinitely. But it did anchor on incorrect interpretation sometimes. On Q5b (triplet loss supervision), Gemini claimed triplet loss ""requires explicit class labels"" and defended this through 3 steering attempts before finally acknowledging that relational similarity (positives/negatives) would do the job. Similarly, on Q5g, it insisted performance saturates rather than drops beyond 128 dimensions even though the paper clearly stated the latter. This outlines hallucinations that were further seen when trying to work on Q5f (harmonic embeddings). Gemini confidently stated ""The FaceNet paper does not directly define or utilize harmonic embeddings"" and fabricated a connection to Fourier analysis/Random Fourier Features. This was a clear hallucination since the paper does discuss harmonic embeddings in Section 3.2, referring to model version compatibility. The longer the conversation went on, the more windy and hallucinated the answers ended up being. This seemed like a predictable outcome though. I steered Gemini to the correct answer in 4 ways:Abstract conceptual questions: often led to defensive elaboration of the same pointHypothetical scenarios: better success at identifying misstepsAsking for specific data points and references: mildly effectiveDirect factual corrections: last resort, and usually brought Gemini back to the grounded data it was givenFor homework assistance, Gemini 2.5 Flash would be reliable for computations but should be verified against the main material for conceptual claims. The longer the chat, the worse it does with actively recalling instructions and facts.Here's an annotated log of my chat transcript: https://drive.google.com/file/d/1sTkCzX9o669RVIqiDcby3DfUjsgOsUwB/view?usp=sharing",https://us.edstem.org/courses/84647/discussion/7404071,7404071,Special Participation A: Gemini 2.5 Flash on HW10
ChatGPT 5.1 thinking on HW13,ChatGPT 5.1 thinking,ChatGPT,5.1 thinking,13,Jin Ying,"Looking at GPT's attempt at these two problems, I'd say it got maybe 4 out of 10 parts completely right on the first try. The pattern I noticed is pretty consistent: GPT nails the warm-up questions where you're just manipulating probability distributions or doing basic calculus, but when the problems require you to track multiple variables through several steps of algebra, it starts cutting corners. And further questions on this would easily lead to hallucination and circular reasoning.The most frustrating thing is that GPT doesn't really admit when it's unsure. Instead of saying ""I'm not certain about this step,"" it just writes ""following similar reasoning..."" or ""it can be shown that..."" and jumps to an answer. Sometimes that answer is right, sometimes it's totally wrong. For the DPO problem especially, there were some actual mathematical errors -- not just stylistic issues, but wrong coefficients and circular logic.One thing I found interesting is how the quality degrades as you go through each question. Parts (a) and (b) are usually solid, but by part (f) or (g), GPT is clearly losing the thread. It's like it forgets what assumptions we made earlier or what variables we're solving for.",https://us.edstem.org/courses/84647/discussion/7418727,7418727,Special Participation A: ChatGPT 5.1 thinking on HW13
Qwen on HW4,Qwen,Qwen,,4,Zach Pricz,"For special participation A on HW4, I used Qwen and its Qwen3-Max model with thinking to solve the non coding problems on the homework (problems 1, 2, 3, 4, 7). I attempted this homework with Qwen 3 times actually as the earlier times I prompted it with more advanced instructions or I would give it each problem seperately it often spiraled and kept second guessing itself. I found that when Qwen does this it often happens cloes to 3 times and it has a hard time accepting the truth when it can't reason to it. This is present in problem 2e in this log as when asking Qwen to reason itself to the solution it took 3 times in one query to finally arrive to the correct answer. Despite this, Qwen did a pretty good job on this homework especially considering how difficult it is. It interestingly did not one shot computation based questions like the convolution questions in problem 3, but it expertly navigated and explained the matrix calculus in problem 7. I think Qwen is a decent model for this type of tasks but it has to be wrangled more then I would have hoped. Final conversation trace: https://chat.qwen.ai/s/5fd54197-ec03-43bd-a4cb-53d560f95d0f?fev=0.1.7",https://us.edstem.org/courses/84647/discussion/7400839,7400839,Special Participation A: Qwen on HW4
Grok on HW10 Theory,Grok,Grok,,10,Sarvagya Somvanshi,"I prompted Grok to solve the theoretical portion of Homework 10, including the mathematical part, the reading assignment, the notebook result analysis. It excelled at mathematical derivations, following instructions to the letter without skipping steps. However, on conceptual questions, it initially relied on inferring results based on domain knowledge until I explicitly uploaded the visual results from the notebook, at which point its analysis became significantly more grounded and insightful.Question 1: Linearized Attention Derivation The model handled the mathematical derivation pretty well. It correctly expanded the squared norm and identified the necessary scalar substitutions to approximate Softmax attention . It was particularly strong in analyzing the computational complexity, correctly identifying the reduction from quadratic O(N2) to linear O(N) by leveraging the recursive cumulative sum trick . This was a one shot success!Question 2: FaceNet Paper Analysis This section was handled well as expected since it was a reading assignment with context given. I gave the model the links so it was effectively a search engine, accurately retrieving architectures (Zeiler & Fergus vs. Inception) and definitions (Triplet Loss) . It correctly defined ""semi-hard negatives"" and ""harmonic embeddings,"" but its initial responses were overly verbose. It provided correct facts as needed for the question/Question 3: Example Difficulty (Notebook Analysis) This was the most revealing interaction. Initially, the model ""hallucinated"" the notebook's output hence guessing the dataset and results, basing results of standard literature. While these guesses were factually ""correct"" based on the domain, they were not derived from the actual file. Once I uploaded the screenshots of the plots, the model's answers improved slightly. It abandoned its generic answers and provided a more accurate analysis of the bimodal exit distributions and the specific geometric properties (elongation/noise) that caused difficulty .Overall: Grok  demonstrated strong mathematical reasoning and reliable retrieval capabilities. However, it exhibited a tendency to ""coast"" on general knowledge when specific data was missing (as seen in the notebook section). Here is the annotated chatAnd here are the chats online:https://grok.com/share/c2hhcmQtMw_b3d0111f-225a-4421-8e87-5815f7cbfd22https://grok.com/share/c2hhcmQtMw_a1f459db-cc3c-43ac-bfb6-e0a33eb3be31https://grok.com/share/c2hhcmQtMw_20fe1b02-c86c-4580-88de-1b3b41b562b2",https://us.edstem.org/courses/84647/discussion/7450591,7450591,Special Participation A: Grok on HW10 Theory
ChatGPT-5.1 Pro on HW5,ChatGPT-5.1 Pro,ChatGPT,5.1 Pro,5,Eric Wang,One-shots all of HW5 Q1-4 (non coding) which I was quite impressed by.,https://us.edstem.org/courses/84647/discussion/7450396,7450396,Special Participation A: ChatGPT-5.1 Pro on HW5
Deepseek v3.2 on HW0,Deepseek v3.2,Deepseek,v3.2,0,Andrea Lou,"I evaluated Deepseek v3.2 on Homework 0.Initial prompt: ""You are being evaluated on how well a modern LLM can solve questions 2, 3, 4, and 5 of the attached homework assignment. Follow all instructions exactly. You must attempt to answer each question fully and independently. Always show your complete reasoning process. For each question, break your reasoning into labeled steps based on interpretation, method, reasoning steps, and your final answer. Indicate if you are ever uncertain about an answer, and elaborate on that uncertainty. If you believe the question is underspecified or ambiguous, state clearly why and attempt a reasonable interpretation.""I informed the model about the exact task it was to do, and the fact that this was an evaluation. Also, I added instructions on how to format the solutions so we can read through the reasoning steps clearly.Executive summary:Deepseek produced correct mathematical derivations across all early questions (Q2–Q3)In the initial responses, it followed the required reasoning-step structure I defined in the prompt: clearly labeling interpretation, method, step-by-step reasoning, and final answers.No hallucinations or conceptual mistakes were observed. All solutions were consistent with standard linear algebra, probability, and deep learning principles.As responses grew longer (Q4–Q5), Deepseek’s format adherence declined:omitting required step labels.merged reasoning and solutions into single paragraphs.skipped intermediate expansions that the staff solution spells out (e.g., full Gaussian likelihood, factorization across samples, log-transform details).The omissions did not lead to incorrect results, so this might indicate the model’s tendency to compress reasoning as output length increases.The model appears constrained by token limits and eventually stopped mid-response due to length.After receiving a follow-up prompt with explicit reminders, Deepseek returned to the correct structured format and successfully completed the rest of the assignment.After the model stopped due to long response length:I prompted the model to continue reasoning, but added additional reminders for its answer format to encourage clearer reasoning and explanation. For the parts of Q5 where a graph would illustrate the point better, I asked it to describe the graph to reasonable success.""Recompute questions 4 and 5 using the same protocol as before. Remember to provide fully labeled reasoning steps for each part. If a question calls for a diagram, do not attempt to draw one. Instead, give a clear verbal description of what the diagram would look like and what elements it would contain. In question 5 where it says give a numerical example, you may use any variable or description to answer the question instead.""After this, Deepseek continued with the correct response format, and was able to one-shot the rest of the assignment.",https://us.edstem.org/courses/84647/discussion/7451745,7451745,Special Participation A: Deepseek v3.2 on HW0
,,ChatGPT,5.1 Thinking,,Shaurya Jain,I have a curiosity-driven question about Deep Learning as a subject and field of human endeavor. I used GPT 5.1 Thinking on HWK 8 Non-Coding Problems.Attached below.,https://us.edstem.org/courses/84647/discussion/7451771,7451771,Special Participation A
Claude Opus 4.5 with extended thinking on HW12,Claude Opus 4.5 with extended thinking,Claude,Opus 4.5 with extended thinking,12,Will Cai,"Summary: Overall Claude was reliable but with a specific pattern on answer quality. On algebraic or mechanical reasoning, it was very strong and made no mistakes and pretty much one shot all questions. On conceptual intuition questions, especially the VIB beta effects, it tended to give the first plausible explanation and only corrected itself when pushed by additional queries. So it can get to the right answer, but sometimes needs prompting to avoid shallow intuition. When guided, it produces very solid reasoning; when left alone, it feels more lazy and usually settles for a simplified story; however, seems like everything can be solved by better prompting. Chat log: https://claude.ai/share/72ff4a16-11f6-436e-925b-163c5ce94835 Problem 1Claude handled this one pretty well. It immediately understood that using std = 1 for tied embeddings makes the logits blow up, and the fix with 1/sqrt(d_model) was basically a one-shot answer. The explanation about variance scaling with d_model was clean and it didn’t get lost in irrelevant details. It didn’t really explore whether hidden state variance or LayerNorm behavior could change the picture, but the core reasoning was correct and it stayed on track the whole time.Problem 2The KL example was correct, but Claude felt a bit on autopilot here. It went straight to the standard uniform vs Gaussian example without considering other constructions or sanity checks. It didn’t question whether the support conditions were the real driver, which would have shown deeper understanding. For forward vs reverse KL, it defaulted to the classic mode-covering vs mode-seeking framing. That was correct, but the explanation stayed at the surface level. In the end of the day the final answers matched the intended reasoning.Problem 3This was where Claude struggled the most. The basics, from reparameterization, which gradients hit the encoder vs decoder, and the U-shaped validation curve were all correct. But when we got into the beta values and how they map to the latent plots, it started with the wrong intuition, saying small beta should make the latent “spread more.” It only corrected itself after I explicitly pushed back and asked how that squares with the plot. Once it revised the explanation, it gave a coherent and correct story, but it needed that additional human oversight. Good final answer, but the initial instinct was not correct.Problem 5Claude did extremely well here. The min-norm solution was correct, the gradient calculations were clean, and there were no algebra mistakes at all. It also correctly identified the conserved norm and interpreted the gradient flow (c1 driven to zero and c0 flowing toward its magnitude). The explanation was stable and didn’t require prompting. This seems to be the type of problem Claude excels at — precise algebra, clean optimization reasoning, and consistent interpretation.",https://us.edstem.org/courses/84647/discussion/7451901,7451901,Special Participation A: Claude Opus 4.5 with extended thinking on HW12
ChatGPT 5 on HW07,ChatGPT 5,ChatGPT,5,7,Peyton Schales,"For this assignment, I tested ChatGPT 5 on the non-coding theory questions. It consistently one-shotted every problem. However, its initial responses sometimes skipped important steps and  derivations. For example, in Q8b, it told me that minimizing the regularizer implies equal singular values without proof. When I asked this the AI was able to a write a formal derivation using the AM-GM inequality, proving it actually can understand the theory rather than just retrieving a memorized answer. I also attempted to input an incorrect answer into it about orthogonal initialization in RNNs guaranteeing non-vanishing gradients. I expected ChatGPT to just agree with me because it sometimes just agrees with the user. However, it caught the trap immediately, correctly distinguishing between linear stability and the decays. Overall, the interaction felt great. ChatGPT seemed to understand all the concepts very well.ChatGPT Conversation: https://chatgpt.com/share/69361eec-6218-800f-aae8-0fbd3556bdc9Annotations: https://drive.google.com/file/d/1l66xpKohPBDUn5VDNTxdPiAUVbj3usiZ/view?usp=sharing",https://us.edstem.org/courses/84647/discussion/7428812,7428812,Special Participation A: ChatGPT 5 on HW07
GPT 5.1 Thinking (Extended) on HW3,GPT 5.1 Thinking (Extended),ChatGPT,5.1 Thinking (Extended),3,Paul Struble,"I used GPT 5.1 Thinking (Extended) to solve the non-coding parts of Homework 3. Overall, the model was very effective at solving each problem and explaining its reasoning. It was able to one-shot all parts of all problems. I prompted the model in the ChatGPT web frontend by providing a PDF attachment of the original homework assignment, a brief explanation of the task, and some additional prompts/attachments throughout the conversation to provide additional contextual resources (from the assignment). This is all recorded in the conversation log in the attached document. I found no misconceptions or hallucinations in the model’s output although some responses took a different approach than the reference solutions (ultimately still arriving at a valid solution). I include more analysis/observations in the attached document.Link to original conversation: https://chatgpt.com/share/693a069f-44a8-8007-bed7-a4db5aceaa8f",https://us.edstem.org/courses/84647/discussion/7450077,7450077,Special Participation A: GPT 5.1 Thinking (Extended) on HW3
Mistral on HW9 (non-coding),Mistral,Mistral,,9,Subhash Prasad,"I used Mistral Le Chat on HW9 (non-coding), and it achieved 99% accuracy, solving everything correctly on the first try with only one minor notation error. This was done on the second prompt, as the first one tried to be a pedagogical aide by providing explanations without complete answers.PromptsPrompt 1: ""Please complete all of the problems in this homework assignment.""Mistral tried to be pedagogical, encouraging me to work through problems myselfPrompt 2: ""Give me the detailed answers and explanations. I need an answer key.""This resulted in complete solutions with full derivations.Annotated Conversationhttps://drive.google.com/file/d/11VThgTMnqTfB7DuIIoTBzNxp0MFyCaiN/view?usp=sharingStrengthsZero arithmetic errors across dozens of calculationsPerfect PyTorch einsum notationExplained why answers are correct, not just whatNo hallucinations or fake mathErrorsUsed ""bnd"" instead of ""bnk"" in one complexity expressionConceptual understanding was clearly correct from explanationExplanations were lacking in the output from the second prompt, but this might be because I told the model to just give me the answersThoughtsThe model's initial refusal to just give answers was interesting, as it wanted to be a tutor, not a solution key. This required explicit instruction to override. Without overriding it, the tool is well-formed as a place for students to close the gaps in their understanding after having attempted the problems by themselves.",https://us.edstem.org/courses/84647/discussion/7450819,7450819,Special Participation A: Mistral on HW9 (non-coding)
Qwen on hw11,Qwen,Qwen,,11,Reyna Liu,"Link to conversation: https://www.qianwen.com/share?shareId=99965dbe-7cb8-4b68-a181-f66d29022d0fThe model was reasonably good at one-shotting the problems, but it’s still far from trustworthy. It was quite good at high-level conceptual explanations and even symbolic derivations, but it made small but fatal mathematical and numerical mistakes, and almost never flagged its own uncertainty. Getting to fully correct answers usually required some active steering.One-shot correctness: About 70% of individual sub-questions were essentially correct on the first try, especially the more conceptual ones. Throughout the process, it often gave good reasoning behind its answers.Fixed within one rounds of feedback: Another ~20% could be fixed after a single iteration where I pointed out its mistakes. After the incorrect portion was pointed out, the model did a good job refocusing on that specific portion and fixing the issue, whether it was a mathematical error or a conceptual one. However this required the user to know the ground truth answer to be able to tell the model where things went wrong, which was not feasible sometimes.Strategy I used to steer the model:My main strategy was localizing errors. When something was incorrect, I would point out a specific part where things went wrong and ask it to redo that portion or rewrite the answer with that constraint in mind. If the revised answer still contained other errors, I would then point those out as well. This iterative process usually converged to the correct solution, but only because I was checking against the official answers.Where it did well:Linear algebra (for example the Chinchilla scaling law question was done well, with correct derivations)Conceptual explanations (especially for 6.Soft-Prompting Language Models, where the model one-shot all the sub-questions with clear, coherent explanations)Errors and hallucinations:The main issue is the model gives confident, mostly correct answers with hidden mistakes. For example, in 2(a), the model mentioned similarity with a bag-of-words model, while the problem was asking about a token-by-token model. Interestingly, the model knew the correct answer, and did include it in its output(“So, it's essentially a token-to-token mapping without any awareness of context or sequence structure”). If it had simply not added the bag-of-words remark, the answer would have been fully correct. Another example is 2(b), where it talked about a matrix being block-diagonal when it was horizontally partitioned. These wrong side-comments did not derail the final conclusion, but they weakened the model’s usefulness as a learning tool, because a student might not know which parts to trust.Mathematically, the model made sloppy arithmetic mistakes, messing up the exponent in problem 5(h) twice, and did not fix the second mistake when the first one was pointed out. Without the user explicitly checking the numbers and forcing it to recompute, it would happily present a numerically incorrect result.It also made overconfident statements. In 2(e), it made claims about the read subspace without appropriate qualifications. Despite this, it still managed to reach the correct high-level conclusion that “the read subspace dimension is at most d_head”. When I later prompted it to tone down the overconfident statement, it also dropped the correct statement entirely. This illustrates a common LLM behavior: it is too eager to agree with the user, sometimes over-editing and discarding correct content when asked to correct itself. This makes adjusting the model’s confidence level the most difficult in my opinion.It was also occasionally under-confident. Notably in problem 1, the model said “At initialization, since A=0 and B=0, the gradients may be zero or very small”, when in fact the gradient is exactly zero in this setup. So it hedged (“may be”) where the correct answer was deterministic.Overall, the model can one-shot many questions or get very close, but it often introduces mistakes even in derivations that look careful. It’s not reliable as an automated homework solver, and it’s not even a great tutor unless the user actively engages the model, checks its work, and asks thoughtful, targeted follow-up questions to give the model a chance to fix its mistakes. Without active supervision, it’s very easy to walk away with answers that are wrong.",https://us.edstem.org/courses/84647/discussion/7427535,7427535,Special Participation A: Qwen on hw11
Grok on HW 12,Grok,Grok,,12,Nikhil Mathihalli,"I used Grok (Standard Chat) to tackle the non-coding theoretical portions of Homework 12. The model's performance was outstanding, effectively one-shotting every major conceptual and mathematical question I threw at it. Unlike previous generations of models that often hallucinate on complex derivations or misinterpret geometric relationships, Grok acted like a competent graduate-level tutor, correctly handling everything from VIB constraints to minimum-norm regression derivations.Methodology: I uploaded the full hw12.pdf and prompted the model interactively. I treated the model as a collaborator, asking it to derive solutions step-by-step.Per-Question Breakdown:Question 1: Debugging Transformers (Initialization)Result: Perfect / One-ShotAnalysis: Grok exhibited ""Global Code Awareness."" It didn't just flag the std=1 initialization as a heuristic error; it explicitly linked it to Line 23 (weight tying), reasoning that sharing large-variance weights between input and output heads would cause logit explosion. It provided the correct fix ($1/\sqrt{d_{model}}$) and the correct causal explanation.Question 2: Comparing Distributions (KL Divergence)Result: Perfect / One-ShotAnalysis: The model demonstrated deep intuition for Information Theory. It correctly identified the asymmetric ""zero-forcing"" vs. ""zero-avoiding"" behavior of Forward vs. Reverse KL. It also generated a mathematically valid counter-example (Nested Uniforms) to prove the finiteness condition D_KL(P||Q) < infinity vs D_KL(Q||P) = infinity without any prompting.Question 3: Variational Information Bottleneck (VIB)Result: Perfect / One-ShotAnalysis: This was the highlight of the session. Despite the questions relying on visual plots of latent spaces, Grok correctly deduced the answer from first principles. It reasoned that increasing the hyperparameter beta in the VIB Lagrangian forces the posterior to collapse to the prior (creating a ""blob"" ), while low beta prioritizes task accuracy (creating ""clusters""). It matched every plot and error curve correctly based purely on theoretical physics-style reasoning.Question 5: Meta-Learning DerivationsResult: CorrectAnalysis: I tasked the model with a multi-step derivation for the expected test error of a minimum-norm solution. Grok correctly set up the constrained optimization problem, applied the orthonormality properties of the test features to cancel cross-terms, and derived the final analytical loss function. It avoided common pitfalls (like dropping variance terms) that I have seen other models make on this type of problem.Conclusion: Grok graduated from being just a search engine to a viable theoretical assistant for me. Its ability to chain definitions (like weight tying or orthonormality) across multiple steps without losing state makes it highly effective for checking graduate-level coursework.",https://us.edstem.org/courses/84647/discussion/7424701,7424701,Special Participation A: Grok on HW 12
Gemini Pro 3 (With Thinking) on HW 9,Gemini Pro 3 (With Thinking),Gemini,Pro 3 (With Thinking),9,Joshua Lu,"I used Gemini Pro 3 (With Thinking) to complete the non-coding portion of Homework 9.Here is the trace (without annotations): https://gemini.google.com/share/deb95c933e37Here is the trace with annotations: https://drive.google.com/file/d/1-m_LhkuNUKNzRQXIzWtjbGJfWSZVt2kb/view?usp=sharingFor my setup, instead of passing the entire pdf at once to have the model one-shot all the questions at once, I passed in one question at a time (in fact, one sub-question at a time). My reasoning for doing this is so that when the model makes a mistake early on, I can correct it, and that mistake won't carry on to future problems. I also chose to give the model an image instead of pasting the text to preserve the formatting structure.Overall, the model did very well on this homework, and I was surprised by how accurate it answered everything. I didn't have too much prompting in each one of my prompts (I just told Gemini to complete the problem), but it gives a lot of explanation, even for the simple problems, without me asking it to. I looked through these reasoning, and a lot of them are actually very insightful, such as the one that explained to me how implementation-wise, multi-head attention still uses a single matrix for all the heads instead of physically storing separate ones.The vast majority of times, it was able to one-shot the question and get it correct. I think the reason the model was able to do so well on this homework was because the questions were very general questions involving transformer and attention. For example, calculating these specific expectations and variances are very general transformer problems that have been described in detail many times, and the complexities for transformer architecture are also very well known. The same applies to the fill-in-the-blank coding questions, as the model was probably trained on transformer architecture code. I will now summarize, for each question, how the model performed.Q1) This problem was relatively straightforward, and the model basically got everything correct. However, initially, it thought that the mu term was a scalar instead of a vector, so I had to correct the model before moving forward. Other than that, it was able to one-shot the question very easily.Q2) Gemini was also able to easily one-shot this question.Q3) These kinds of fill-in-the-blank coding questions are very easy for Gemini to solve, and it accomplished these questions easily (one-shot), especially since a lot of them are multiple choice.Q4) Again, the model one-shot this question, as the fill-in-the-blank coding parts were straightforward. It was also able to one-shot the complexities questions. These types of questions are very standard, so that is expected.Q6) Question 6 is a bit more complex, but Gemini still almost one-shot everything. There was a bit of a confusion regarding whether the similarity metric can be all-negative, and Gemini seemed to disagree that it can because standard-wise, that's not generally done, but after continuing to prompt, Gemini did offer a reasonable explanation for why that result should not be negative. There was also some confusion when solving for the kernel feature map, such as the ""vec"" notation, and it seemed like Gemini dropped a constant factor, but after some additional prompting, turns out Gemini's solution is valid, just written in a different form.Overall, to recap, Gemini Pro 3 With Thinking did really well on this homework, and it was able to almost one shot the entirety of this homework. Many times when I thought it was wrong, its answer was actually correct, just in a different form. Perhaps the result would be slightly worse if I instead just gave it the pdf and asked it to solve all the problems directly, but that can be left for future testing.",https://us.edstem.org/courses/84647/discussion/7424085,7424085,Special Participation A: Gemini Pro 3 (With Thinking) on HW 9
Deepseek on HW2,Deepseek,Deepseek,,2,Ken Zheng,"I completed all non-coding parts of Homework 2 using Deepseek with DeepThink turned on.MotivationI’ve seen quite a few examples of classmates engaging in some light conversation with the model or using somewhat structured/designed prompts with the aim of improving the output quality and accuracy. I opted for a slightly different approach. I decided to use raw and ""no-prompt"" prompting, meant to mimic the setting where a student is treating the LLM less as an interactive tutor and more as a tool, a “calculator”, for problems. For every problem, my process consisted of highlighting the relevant subpart in its entirety on the homework PDF, then copy pasting into the text box. Nothing more. No context was given except for those present in the problem originally, and no prompt optimization was performed (i.e., no strategic ""filler"" words like ""think carefully and answer accurately"", ""you are an expert in deep learning"", ""take a deep breath"" [https://arxiv.org/abs/2309.03409] were included).SummaryOverall, Deepseek is a very capable model for reasoning tasks. It one-shots every non coding question in Homework 2. It arguably fails to notice one small detail in one of the subparts (see Q1 for more), but apart from that, all perfect. Analyzing the thinking trace reveals Deepseek's general chain of thought pattern, which is comprised of these steps:Restate the questionRe-interpret the question with its own wordsList what is given/knownIdentify what is unknown/what the task isFormulate a strategy/plan of attackCarry out that strategy (i.e., start doing the work/calculations following the strategy)After finding answer, start doubting absolutely everything starting from step 1 to step 6.For every doubt, recheck work and see if other answers exist. If yes, explore those paths; else, move on to the next doubt.Return final answer after double-checking everything.People who took EECS 16A with Prof. Ranade (Fall 2023) might remember her teaching us a three step proof writing structure: What do we know?What are we trying to show?How can we get from 1 to 2?It is interesting to see Deepseek's thought habits closely follow this three step process (plus a plethora of double-checking), almost like Deepseek's frontier-level reasoning performance stems from the fact that it treats every technical question like completing a rigorous proof. Please see annotations for more in-depth observations.Files & LinksQuestion 1:Question 2:Question 5:",https://us.edstem.org/courses/84647/discussion/7372081,7372081,Special Participation A: Deepseek on HW2
GPT-4o on Hw0,GPT-4o,ChatGPT,4o,0,Aaron Zheng,"Below is my report on solving non-coding related problems of Homework 0 using GPT4o. This is the pdf of the transcript. There are some situations when GPT4o made some small minor syntactical mistakes, i.e. getting the right expression but solving for the wrong term, such as the full least squares solution instead of just the transformation matrix. It struggled very hard on the subparts of part (b) of question 5, struggling to get the correct direction for the slope shift correctly. It shows how 4th generation models' graphical intuition has not been foolproof yet. It keeps getting it wrong even after few shot tips, and even when I tell it the answer of the problem it sometimes using the wrong intuition to justify the problem. (for example part (ii), relationship between b and w/x is correct instead of b and wx). It also seems to sometimes get the solution right but omit detailed justification (unless prompted), such as for part (d) of question 5.",https://us.edstem.org/courses/84647/discussion/7424515,7424515,Special Participation A: GPT-4o on Hw0
Claude on HW2 written part,Claude,Claude,,2,Yaqi Su,"Claude demonstrates strong mathematical reasoning capabilities and correctly derived analytical solutions without any mathematical hallucinations or false claims. Across all problems, Claude never makes computational errors. The issues are always at the level of solution strategy or conceptual completeness, not arithmetic or algebra. This suggests the model's mathematical symbolic reasoning is quite robust, while its ability to choose optimal solution paths requires more guidance. The two problems requiring guidance (Problem 1b and Problem 2a) revealed a consistent pattern: Claude tends to solve problems in an over-complicated way before recognizing simpler approaches. When I used some questioning like “Are you sure...?"" and quoting Claude's own observations back to it as a hint, Claude is able to quickly find cleaner solutions and even provided further insights compared to the original homework solution.",https://us.edstem.org/courses/84647/discussion/7267427,7267427,Special Participation A: Claude on HW2 written part
Gemini on Homework 4,Gemini,Gemini,,4,Jason Guo,"Annotated Transcript:https://drive.google.com/file/d/1ZOIMXval6EtWYyoBE6H13fS0I7d58Fmd/view?usp=sharingFor this special participation, I used Gemini Pro 2.5 to solve the written portions of homework 4. I began by giving Gemini the assignment and telling it that I was trying to evaluate how well it could solve the problems, and then went through the problems with it one by one. Overall, it did a good job of solving the problems. All the mistakes it made, except one, were due to the fact that it read the problems in the homework wrong. Oftentimes, these misreadings were very blatant, like when it just misread the entries of the matrices for problem 3. It only made one reasoning mistake, that wasn’t due to it misreading the question, in part 2g, but was able to correct itself after being told what step in the derivation it made a mistake on. One notable part of the interaction was when it solved problem 2e. Its answer differed from what was given in the solutions, even though it was correct as there seems to be a mistake in the solutions, so I tried prompting it to fix its answer, at which point it arrived at the same solution. Afterwards, I gave it what was given in the solutions, and asked it to try to arrive at the answer in the solutions. After trying to arrive at the given answer, it eventually gave up, and said that it didn’t know why the answer was correct. I thought that this was pretty impressive because it shows that Gemini is actually critiquing itself as it goes. Instead of hallucinating to match the answer I gave it, it pushed back and gave what it thought was correct. I thought this was really impressive, as it isn’t just being “agreeable” and taking what the prompter says to be the truth, like other LLMs I’ve used like ChatGPT.",https://us.edstem.org/courses/84647/discussion/7265693,7265693,Special Participation A: Gemini on Homework 4
ChatGPT on HW6,ChatGPT,ChatGPT,,6,Jameson Liu,"I used ChatGPT (5) on the non-coding parts of homework 6 (#2, #3). I prompted it by attaching the entire homework pdf and asking it to answer them as an expert in deep learning. In its first response, it was able to one-shot a majority of the questions. Some of its solutions, however, contained imprecise language. For example, in 2d, it just described how the max function worked, so I had to convince it that it would be a one-hot encoding by induction. This was interesting since it seemed like the question contained enough context for this to be concluded. Another interesting limitation was that ChatGPT was unable to parse the image of the graph for 3b. Even after I screenshotted the graph and fed it as input again, it got one of the connections wrong. Overall, I would say ChatGPT did extremely well on this homework, requiring only a few clarifications, especially for visual aspects.",https://us.edstem.org/courses/84647/discussion/7283953,7283953,Special Participation A: ChatGPT on HW6
GPT 5.1 Thinking on HW07,GPT 5.1 Thinking,ChatGPT,5.1 Thinking,7,Jaewon Chang,"I utilized GPT 5.1 Thinking on homework 7 (the written questions), and overall I was surprised by how easily the model was able to one shot all the problems. Here are my specific notes per subquestion:Q3(1 minute 43 seconds of thinking + 13 seconds of thinking for clarification) The model was able to one shot the problem successfully, in a slightly different method than the solutions for part (b) (ii). There was a minor question I had because the model didn’t fully clarify one step in its working, so I wanted to confirm that I was correct on this. The model responded with a really in-depth clarification on this part, which I found to be extremely helpful for my own learning(51 seconds of thinking) Given that the solution was slightly different from that of the staff solution, I tried steering the model to mimik the staff solution, and I was surprised by how much more in-depth it was than the staff solution in explaining how to go from one part to the next.Q4(33 seconds of thinking) For part (a), the model got the correct accuracy numbers but wrong times, so I’m thinking it actually didn’t reference the blog post and instead just answered based on its background knowledge (e.g. maybe these numbers were part of its pretraining corpus)For part (b), the model had a good takeaway and also explains with great specificityFor part (c), the model gave an answer that made me also re-think my own takeaways from reading the article – giving me a perspective that I previously hadn't thought of.Q7(36 seconds of thinking) The model was able to zero shot all the problems, and was able to get both reasonable answers for part (a) even though the problem only asked for one.For this question, I also typed out the question just in case the model was not able to fully parse the pdf.Q8(1 minute 26 seconds of thinking) The model was able to zero shot (a) (i - iii), and also (b), in a method that was similar to that of the staff solutionWhen prompting the model to solve all the problems above, for the most part I was taking screenshots of the problems and pasting it into the chat. The reason for doing this is simply because copy pasting math equations often ruins format, which may get in the way (e.g. w^2 when copy pasted may appear as w2, which the model may confuse to be w_2 instead of w^2). Personally I trust the model's OCR capabilities (i.e. parsing text on a screenshot) more than its capability to deduce unformatted math notation. I think this was the reason why I found that the model often thinks for a long period of time (as seen above it often takes the model 30 - 90 seconds of thinking before responding).Result wise, the model was essentially able to one-shot all of the problems, and utilized the hints when appropriate (E.g. Q8 part b). Although GPT 5.1 Thinking is a frontier model, I was surprised by how it was still able to one shot the problems given that I pasted screenshots of the problems rather than latex-formatted math.Annotated drive link: https://drive.google.com/file/d/1mmyNNNoPi6a7Nmy8W2y_9hAXkFW1pW1Z/view?usp=sharing",https://us.edstem.org/courses/84647/discussion/7428314,7428314,Special Participation A: GPT 5.1 Thinking on HW07
ChatGPT on HW 8,ChatGPT,ChatGPT,,8,Dagny Streit,"I used ChatGPT 5.1 (Auto) to solve the written portions of Homework 8 (Questions 1, 3, and 4). For most of the problems, Chat GPT was able to correctly solve them on the first try. Below, I outlined the strengths and weaknesses of the model that I noticed, which included the types of questions Chat GPT tended to do well on versus needed more guidance on.Strengths:Consistently correct on direct mathematical derivations (unrolling recurrences, computing kernels, linear algebra, etc.)Produced clear step-by-step reasoning with limited guidanceHandled numerical examples and matrix calculations wellAble to refine and reorganize its thoughts effectively when promptedWeaknesses:Had difficultly with some algorithmic reasoning (most clearly demonstrated in the critical path length in Question 1c)Pointing out missing factors and asking it to reorganize its thoughts generally caused Chat GPT to re-evaluate and correct its earlier attemptOccasionally gave over-complicated derivations and logic to reach the correct answer when a simpler argument was possible and intendedWhen it made mistakes, they tended to follow a pattern (for example, repeatedly overlooking the logn term in the critical path length in Question 1c)Attached is my annotated log of the ChatGPT interaction. The document is color-coded for clarity. Green annotations / highlights indicate the response was correct. Red annotations / highlights indicate that the response was incorrect. Orange annotations indicate that the answer was partially correct.",https://us.edstem.org/courses/84647/discussion/7408067,7408067,Special Participation A: ChatGPT on HW 8
ChatGPT-5.1 Pro on HW4 Non-coding,ChatGPT-5.1 Pro,ChatGPT,5.1 Pro,4,Neel Kolhe,"I used ChatGPT 5 - Pro on HW 4(all non-coding parts). Summary: It was quite good at one-shotting all problems, even with just one prompt - except a numerical problem, for which it (incorrectly) used python code to generate a matrix. I've attached my conversation with it here. Further, another small issue was the reasoning time - it took 20+ minutes to get a response from the Pro model on this problem set.",https://us.edstem.org/courses/84647/discussion/7449252,7449252,Special Participation A: ChatGPT-5.1 Pro on HW4 Non-coding
Deepseek on HW5,Deepseek,Deepseek,,5,Jerry Xiao,"For the HW5, I try to use Deepseek to solve the problem sets and all the questions and answers are documented in the above files. The strategies I am using are simple, just putting the transcription of the problem to Deepseek and see how it can solve the problems. There are some traits of the Deepseek AI:1. Deepseek does not support Multi-modal input, therefore when the problems rely on image input, then the model cannot really give a proper answer.2. Ambiguous question prompts will lead to longer thinking time.3. Deepseek is better at explanation than real calculation. My recommendations for using Deepseek as a learning tool to make it help with conceptual understanding.This is observed from some of the facts:1. Of the 11 questions, Deepseek gets 9 of them correct first shot, while one of the question relies on vision input and the other is a calculation problem. It gets the calculation problem correct after another prompt of asking itself to self examine the calculation process. 2. I try to ask Deepseek examines its own mistake after I finish all the problems and it still can directly and precisely locate where might go wrong, which demonstrate its capable long context understanding capability.3. For problem 4, Deepseek not only provides very clear demonstration on how to derive the answer correctly and can show the direct relation between batchnorm and dropout by directly deriving the scaling factor.",https://us.edstem.org/courses/84647/discussion/7202422,7202422,Special Participation A: Deepseek on HW5
Kimi K2 (Thinking) on HW1,Kimi K2 (Thinking),Kimi,K2 (Thinking),1,Hanyang Gu,"Model Used: Kimi k2Assignment: Homework 1 (Non-coding theoretical problems)OverviewI utilized Kimi k2 to solve the theoretical portions of Homework 1, covering topics from gradient descent stability and momentum dynamics to stochastic differential equations and high-dimensional geometry. The model demonstrated exceptional performance, acting as a highly competent graduate-level assistant. It successfully derived complex proofs and stability conditions that matched the instructor's ground truth almost perfectly.The prompt I used is pretty simple, such as ""Solve Q1, show me all your process that leads to your answer"".Performance Metrics & ObservationsOne-Shot Accuracy: High (>90%).The model required very little ""hand-holding."" For complex multi-part questions (like Question 2 on SVD coordinates and Question 3 on Momentum eigenvalues), Kimi k2 maintained context across subsections without needing to be reminded of previous definitions. It correctly identified the underlying mathematical structures (e.g., using the SVD to diagonalize the loss landscape) immediately upon being presented with the problem statement.Mathematical Rigor & Reasoning:Unlike some LLMs that skip steps or hallucinate intermediate lines to reach a ""known"" answer, Kimi k2 provided complete, step-by-step derivations. E.g.Matrix Calculus: It correctly handled vector derivatives in Question 6 (Tikhonov Regularization) and Question 7 (MAP interpretation), distinguishing properly between scalar and vector layouts.Stability Analysis: In Question 3, it correctly applied the stability criterion (checking if roots lie within the unit circle) to the characteristic equation, a non-trivial task for standard language models.Hallucinations/Misconceptions: Little Observed.The model did not invent theorems or misuse standard notation. In instances where the problem allowed for multiple interpretations (e.g., the specific formulation of Adam in Question 4), it defaulted to the standard textbook definitions that aligned with the course material.Downside: Kimi K2 generally have a thinking time of over 1 minute on each question. This is outperformed by other models, such as GPT5 or Gemini 3. However, this thinking time provides great ultimate performance that renders it fruitful. ConclusionKimi k2 proved to be an incredibly effective tool for theoretical deep learning. It did not merely ""retrieve"" answers but seemingly ""reasoned"" through the linear algebra and probability theory required for this assignment. Its ability to link algebraic manipulations (like rotating the basis Vtop) to geometric intuitions makes it a powerful study aid for understanding the ""why"" behind deep learning dynamics.",https://us.edstem.org/courses/84647/discussion/7377516,7377516,Special Participation A: Kimi K2 (Thinking) on HW1
Deepseek v3.2 on HW10,Deepseek v3.2,Deepseek,v3.2,10,Kelvin Li,"Executive SummaryI used the newly released DeepSeek v3.2 on HW10.Overall, this tests the model's 1. OCR capabilities (reading the fine equations in the screenshots of the problems and also finding relevant parts from the FaceNet paper which was attached as pdf).2. Reasoning abilities on the related math content.It is particularly impressive given that it was completely free for me to run on their website (and also has extremely low underlying inference cost).DeepSeek v3.2 (DeepThink) solved all the problems with perfect accuracy and clear chain-of-thought reasoning. Chat log: https://chat.deepseek.com/share/u3b4iptgfflvv0t4ohAnnotated PDF:",https://us.edstem.org/courses/84647/discussion/7405742,7405742,Special Participation A: Deepseek v3.2 on HW10
ChatGPT 5.1 on HW 5,ChatGPT 5.1,ChatGPT,5.1,5,Jiayi Zhang,"I am using ChatGPT 5.1 to answer the questions in Homework 5. ChatGPT 5.1 seems to be very powerful. I used a simple prompt and pasted the screenshots question by question, part by part, into the chat and it can mostly, if not always, correctly interpret the text and solve the problems.ChatGPT 5.1 has been one of the most powerful LLMs I have ever used. It demonstrates powerful understanding and reasoning over the Convolutional Neural Network, regularization, and dropout topics. It can also identify the difficulty of the problem, and use Chain of Thoughts to incrementally solve the problem when the problem is more difficult or requires multiple stages of calculations. The reasoning and explanation were also clear and easy to understand. There are only two errors ChatGPT 5.1 have made in this problem set. Overall, this experiment shows that ChatGPT 5.1 masters most, if not all, of the CNN topics within the problem set. This might be influenced by the fact that CNN is already a well studied topic, and the model is well trained on similar problems.Chat history:https://chatgpt.com/share/6934d7be-9808-8007-b24a-a00919b71465Annotation:https://docs.google.com/document/d/1h_z42kVnsDNLzmJX6d1wkdIGJGqAwSuBtaghAyzpJvk/edit?usp=sharing",https://us.edstem.org/courses/84647/discussion/7423443,7423443,Special Participation A: ChatGPT 5.1 on HW 5
Hw 6 with Grok,Grok,Grok,,,Menger Wen,"For Special Participation A, I used Grok on the written portion of HW 6.Here is the link to my chat: Q2Q3AnalysisQ2Overall AssessmentThe AI assistant provided a mathematically accurate and conceptually sound response. It correctly identified the core relationship between matrix powers and graph walks and handled the GNN architecture questions well. However, its structure—specifically the separation between ""Key Points"" and a long narrative essay—made it less direct and harder to follow than the standard ""Question-Answer"" format found in the satisfactory response.Strengths (Pros)1. Strong ""Executive Summary"" (Key Points) The AI started with a ""Key Points"" section. This is a distinct advantage for users who want a high-level overview without wading through the math immediately. It effectively summarized the main takeaways (e.g., $A^k$ counts walks, Max aggregation = Reachability).2. Mathematical Correctness (Parts a, b, c) The AI correctly handled the linear algebra and graph theory components.Induction Proof: The proof for Part (b) was concise and logically sound, correctly identifying the base case and the inductive step involving neighbor summation.Matrix Interpretation: It correctly identified the output as related to $A^k$ and the update function as the identity.3. Advanced Domain Knowledge (Parts g, h)Feature Propagation: In Part (g), the AI mentioned ""minimizing Dirichlet energy"" and ""feature propagation."" This is a more advanced, technically specific answer than simply saying ""message passing."" It shows a deeper knowledge of how modern GNNs handle missing data.Complexity Analysis: In Part (h), the AI correctly broke down the computational complexity, noting the difference between sparse ($O(|E|)$) and dense ($O(|V|^2)$) graphs.4. Citations The AI included a list of citations/sources at the bottom. While this is a standard feature of search-enabled AIs, it adds credibility to the mathematical definitions used in the answer.Weaknesses (Cons)1. Structural Redundancy and Poor Formatting This is the biggest weakness.Split Information: The AI answers the questions in short bullet points first, but then repeats the entire explanation in a long, narrative essay at the bottom (""Graph neural networks represent..."").Hard to Parse: If a user wants to check the answer for specifically Part (e), they have to look at the short bullet point, and then scan through the long text block at the bottom to see if there is more detail. The ""Satisfactory Answer"" is much better because it groups the full explanation directly under the question heading.2. Ambiguity in Notation (Part a) In Part (a), the AI writes: ""The output... is the j-th row of $A^k$ (or equivalently, the j-th column if considering column vectors)."" While mathematically true that $A$ is symmetric for undirected graphs, in the context of a specifically defined linear network with one-hot inputs, there is usually one specific correct orientation based on how the multiplication is defined ($Ax$ vs $xA$). The hedging (""row or column"") adds unnecessary cognitive load compared to the specific $A^k e_j$ (column) derivation in the Satisfactory Answer.3. Weak Analogy in the Table (Part f) For ""Image flip data augmentation,"" the AI suggests ""Graph permutation or subgraph sampling.""Critique: Flipping an image creates a new data point. Permuting a graph (renumbering nodes) results in the exact same object for a GNN because GNNs are permutation invariant. Therefore, permutation is not really ""data augmentation"" in the same way an image flip is. The Satisfactory Answer correctly notes that for general graphs, there is no direct equivalent, or suggests geometric rotations for geometric graphs, which is a more precise distinction.4. Lack of Operational Detail (Part e) In Part (e-ii) (making the prediction), the AI says: ""Final edge embeddings fed to an MLP..."" It omits the specific mechanism of selection mentioned in the Satisfactory Answer: ""argmax."" Since the question asks how to predict which bond breaks first, explicitly mentioning the selection step (argmax over edge scores) is crucial for a complete answer.ConclusionThe AI's answer is correct and trustworthy, but poorly organized. It mimics a blog post or a research summary rather than a homework solution. The ""Satisfactory Answer"" is superior in terms of pedagogical clarity, as it addresses each sub-problem comprehensively in order, whereas the AI forces the user to piece together the answer from a summary and a dense essay.Q3Overall AssessmentThe AI assistant provided an excellent and completely accurate response. It correctly solved every part of the problem, including the conceptual questions about GNN validity, the mathematical derivation of the loss function, and the structural analysis of the specific update rule. The reasoning was sound throughout, and it successfully interpreted the graph structure provided in the DOT format.Strengths1. Perfect Accuracy on Graph Topology (DOT Parsing) One of the trickier parts of this prompt is interpreting the graph structure defined in the graph G { ... } block without a visual image.The AI correctly identified the neighbors of Node 2 as ${1, 4, 5}$ and Node 3 as ${5}$.This demonstrates strong logical reasoning capabilities to parse the edge list (1 -- 2, 2 -- 5, etc.) correctly.2. Detailed and Correct Dimensional Analysis In Part (c)(ii), the AI provided a step-by-step breakdown of the matrix dimensions.Instead of just guessing the answer ($d \times k$), it traced the vector shape through the inner product ($W_2$), the nonlinearity ($\tanh$), and the aggregation.This explicit chain of thought makes the answer very easy to verify and trust.3. rigorous Conceptual Explanations In Part (a), the AI clearly distinguished between valid and invalid update rules by citing the necessary properties: Permutation Invariance and Parameter Sharing.It correctly identified that distinct weights for specific neighbor indices (e.g., $w_2$ for the 1st neighbor, $w_3$ for the 2nd) violate the definition of a standard GNN on an unordered graph.4. Correct Mathematical Formulation In Part (b), the AI correctly identified the training set vs. the held-out set and applied the binary cross-entropy formula correctly. It did not fall into the trap of including nodes 1 and 4 in the loss calculation.Weaknesses / Areas for Improvement1. Mathematical Simplification (Style Preference) In Part (b), the AI left the final answer in the form of $\log \frac{1}{b}$.AI Answer: $\frac{1}{3} [ \log \frac{1}{b} + \log \frac{1}{c} + \log \frac{1}{1 - e} ]$Standard Convention: In Machine Learning, it is more standard to simplify $\log(1/x)$ to $-\log(x)$. The ""Satisfactory Answer"" uses the negative log form ($\frac{1}{3}(-\log b - \log c \dots)$), which is generally cleaner and matches how loss functions are typically implemented in code (e.g., NLLLoss).Note: The AI's answer is mathematically correct, just stylistically less standard.2. Verbosity The AI's answer is somewhat wordy, particularly in Part (a).While the detailed explanations are helpful for a beginner, a more concise expert answer (like the ""Satisfactory Answer"") gets to the point faster. For example, in (a)(i), the explanation could be condensed to ""Valid: Uses symmetric aggregation (mean) and shared weights.""ConclusionThe AI assistant's performance on this problem is highly reliable. It made no logical or calculation errors. Its only minor flaw was a lack of algebraic simplification in the final loss term, but this does not affect the correctness of the result. It is a very strong response that would receive full marks in an exam setting.",https://us.edstem.org/courses/84647/discussion/7429462,7429462,Special Participation A: Hw 6 with Grok
Gemini Pro 3 on HW 11,Gemini Pro 3,Gemini,Pro 3,11,Xuanlin Mao,"For this special participation, I used Gemini Pro 3 to solve the written portions of homework 11.In this task, since there were no complex mathematical proofs or derivations involved, the vast majority of questions relied on intuitive understanding or simple mathematical calculations. Combined with its use of Python for auxiliary computations, Gemini 3 Pro’s performance was very satisfactory. After comparing with the standard answers provided by the teaching assistant, we judged that Gemini 3 Pro achieved a 100% zero-shot accuracy. All qualitative answers and intuitive understandings were reasonable and correct, the mathematical calculations were error-free, and for the few proof-based questions, it provided sufficient formula support and mathematical derivation steps. The formatting was clear and the language was easy to understand.Since the performance is quite satisfactory, overall my strategy is to input the hw pdf and write a “system prompt” at the beginning of the chat and then let the model solve several questions each time. This is to avoid that the model generates too long answers and make the model focus on few topics and retrieve limited information each run. In the system prompt, I defined the task and model’s role, and provided some instructions, like being precise and making the solution easy to be understood by undergraduate students, etc.Transcript:https://docs.google.com/document/d/1nt5kISBlTi0EcLF0EesfdA5Kqzdn78-tzH8Hi4BEQa0/edit?usp=sharing",https://us.edstem.org/courses/84647/discussion/7403245,7403245,Special Participation A: Gemini Pro 3 on HW 11
Deepseek v3.2 on HW1,Deepseek v3.2,Deepseek,v3.2,1,Yubo Fan,"Special Participation A: Deepseek v3.2 on HW1For the Type A participation option, I interactively engaged with DeepSeek v3.2 to solve the written (non-coding) portions of Homework 1. Attached is the PDF containing the Executive Summary and the full Annotated Log of our interaction, detailing where the model succeeded and where it required guidance.",https://us.edstem.org/courses/84647/discussion/7451410,7451410,Special Participation A: Deepseek v3.2 on HW1
Mistral AI on HW7 Written Portion,Mistral AI,Mistral,AI,7,Tvisha Londhe,"I used Mistral AI to work through the non-coding portions of HW7, and the results were mixed. While it managed to derive the first-order optimality conditions, it initially gave the final formulas without showing the intermediate steps, and I had to reprompt it to fully explain the derivation. For Question 3(b)(ii), it did not apply the optimality conditions at first and only produced the correct reasoning after I explicitly instructed it to use them. It also struggled significantly with Question 4, incorrectly reporting both the model accuracies and training times. On the other hand, it performed much better on the later conceptual questions—particularly Questions 7 and 8—where it provided mostly correct answers along with clear and coherent explanations. It tends to get the multiple choice answers correct the first time, but needs re-prompting for the longer, mathematical questions. This is the unannotated trace: https://chat.mistral.ai/chat/2fc76ff9-ffb4-4ebb-b867-2c05650e1003 Here is the annotated trace:",https://us.edstem.org/courses/84647/discussion/7452122,7452122,Special Participation A: Mistral AI on HW7 Written Portion
Grok 4.1 on HW9 non-coding part,Grok 4.1,Grok,4.1,9,Eric Jin,"In this homework, I used Grok 4.1 as a companion for questions on transformer attention, multi‑head/multi‑query architectures, and attention visualization. Grok 4.1 could often “one‑shot” questions that were close to standard lecture material or well‑known formulas. For example, on the scaled dot‑product justification and argmax attention, it quickly produced the correct expectations, variances, scaling factor, and clear reasoning about why softmax is preferred over argmax for differentiability and training. Its answers in these cases were not only accurate but also well structured, with step‑by‑step derivations that were easy to follow.Grok 4.1 was also strong on implementation‑style questions that involved common transformer patterns. For the multi‑head attention implementation, it correctly identified tensor shapes, the roles of Q/K/V in the matrix multiplications, and how to adapt the output projection when changing the value dimension. On the decoding optimization and multi‑query attention problem, it gave a useful explanation of key/value caching, how MQA differs from standard MHA, and how the shapes of the weights and caches change when keys and values are shared across heads. For the attention visualization problem, it provided a good high‑level guide to typical patterns in GPT and BERT heads (e.g., local vs. global attention, [CLS]/[SEP] heads, pronoun‑to‑antecedent heads), which helped me know what to look for in the notebook.Where Grok 4.1 was less reliable was in finer‑grained complexity and memory analysis. When reasoning about big‑O compute and memory access, it sometimes glossed over distinctions the homework cares about (for example, separating projection cost from attention cost, or identifying which terms dominate for large sequence length versus large model dimension). These were not blatant hallucinations, but the explanations could be somewhat hand‑wavy while still sounding very confident. Overall, Grok 4.1 worked best as a conceptual tutor and a source of “first draft” derivations or explanations; for exact complexity counts, detailed implementation, and visualization‑dependent answers, I still needed to verify the details myself.",https://us.edstem.org/courses/84647/discussion/7450048,7450048,Special Participation A: Grok 4.1 on HW9 non-coding part
Claude on HW7,Claude,Claude,,7,Vongani Maluleke,"Executive SummaryFor this assignment, I looked at how well Claude Opus can handle the non-coding, conceptual parts of CS282 Homework 7. I went through the problems one by one, interacted with the model without giving it any code or extra implementation hints, and saved the full conversation logs. My main goal was to see (1) how accurate it is, (2) how stable its reasoning is, and (3) how much I need to steer it to get the right answer.Overall, Claude Opus did really well. It got everything right on the first try and produced clean, well-structured derivations without me having to nudge it much. For the four non-coding problems (3b, 4, 7, and 8), it basically one-shot the correct solution every time. I didn’t see any major hallucinations.It was especially reliable on:linear algebra manipulations,PCA vs. autoencoder equivalence arguments,gradient calculations, andgeneral convexity/optimization reasoning.In terms of interaction, the model was:consistent in how it formatted math and explanations and,logically stable across long derivations,Overall, based on this evaluation, Claude Opus can solve all the non-coding questions from CS282 HW7 with almost no guidance. Please see the annotated logs.",https://us.edstem.org/courses/84647/discussion/7404515,7404515,Special Participation A: Claude on HW7
Deepseek with Deep Thinking on HW0,Deepseek with Deep Thinking,Deepseek,with Deep Thinking,0,Wesley Kai Zheng,"Here is the online link: https://chat.deepseek.com/share/hcxrv1b7tn9s8c3lo0Here is my annotated version of the log: https://drive.google.com/file/d/18ZU3GgmdtP_u84GnVxof8Kh2yrbpOotN/view?usp=sharingExecutive Summary:From my observations, at least for DeepSeek, the model tends to capture most of the details of the problem-solving process within its internal reasoning. However, even after some targeted prompt engineering, it often fails to provide a detailed explanation of its reasoning and solution in the actual response to the user. At times, it even skips crucial steps in deriving mathematical expressions. For example, when asked for the minimum norm solution of ridge regression, it outputs the correct expression but provides little explanation of how it was derived.Most of the time, the model answers in a single step rather than engaging in a back-and-forth process to verify whether its solution is fully correct. This one-shot approach can be acceptable for straightforward problems, but it does cause errors in more complex cases. This issue is visible later in the annotated log, where the model fails to consider multiple possible cases for a single question.On the positive side, I did not notice any hallucinations so far.",https://us.edstem.org/courses/84647/discussion/7074543,7074543,Special Participation A: Deepseek with Deep Thinking on HW0
Kimi on HW7 Written Questions,Kimi,Kimi,,7,Vijay Kethanaboyina,"I had Moonshot AI's Kimi K2 model answer HW7's written questions. For each problem, my goal was to get the model to the correct answer while giving it as little outside assistance as possible. I only attempted to steer the model when it gave a clearly incorrect answer or when its response diverged significantly from the staff solution.Here are some of my key findings:The model was able to answer most questions in one shot. Laying aside minor formatting and notational differences, the final answers were almost always the same as the staff solution's. However, in reasoning through the problems, the model sometimes made logical leaps that, while correct, were not sufficiently justified in my opinion.For example, for Q3b, the model goes straight from stating the loss function to computing its gradient WRT $W_2$ in just one step.(For context in the below equation, $\hat{X} = W_2 W_1 X$)Meanwhile, the staff solution is much more thorough, walking us through every step of the derivation and explicitly citing the matrix calculus identities that were employed.So to fix that issue, when necessary, I ask the model to justify in more detail how it reached a given step.I saw some evidence of hallucination; for question 4a, I accidentally forgot to give the model some information needed to answer the question correctly. But rather than state, ""not enough information provided"", it gave me an incorrect response.Furthermore, when I gave it a URL containing the information needed to answer the question correctly, it again answered the question wrong. Upon further questioning, the model revealed that in fact, it does not have live browsing capabilities.That is a reasonable limitation, but what is concerning is that it didn't admit it couldn't read the URL until after it had already hallucinated. This decreases my overall trust in the model.That said, the model did a pretty good job on the ""summarize a blog post"" question (apart from the hallucination).I feel like these sorts of ""summarize a technical blog"" questions are where LLMs' abilities really shine. They are able to pick out the key details and ""big picture"" insights without getting too lost in the technical details (a mistake that I often make when summarizing blog posts / papers)A full, annotated transcript of the conversation can be found here: https://drive.google.com/file/d/1p6f0AXpDIMW-9v7vZMJVj2J7aqjPkWQe/view?usp=sharing. I will also add a comment to this post with screenshots of the conversation to make it easy to read.",https://us.edstem.org/courses/84647/discussion/7258633,7258633,Special Participation A: Kimi on HW7 Written Questions
Claude Opus 4.5 on HW05 (Written Questions),Claude Opus 4.5,Claude,Opus 4.5,5,Rishi Thakar,"I used Claude Opus 4.5 Thinking to solve the written portions of Homework 5, covering convolutional networks, batch normalization, depthwise separable convolutions, and dropout as regularization. Questions were fed one at a time with the convolution convention (no filter flip) specified upfront.Overall, Claude Opus 4.5 achieved a 100% one-shot success rate across all 11 sub-questions with no corrective prompting needed.Question 1: Convolutional Networks - All correct. Part (a) correctly identified weight sharing and translation equivariance. Part (b) solved the linear system and self-verified the filter [2, -1, 3]. Part (c) computed the 2D transpose convolution output correctly with clear bookkeeping.Question 2: Batch Normalization - All correct. Part (a) correctly identified Batch Norm as Diagram A and Layer Norm as Diagram B, and even noted Diagram C is Instance Norm without being asked. Part (b) derived the gradient correctly, properly handling the n=1 edge case (gradient equals 0) and n approaching infinity limit.Question 3: Depthwise Separable Convolutions - All correct. Calculated 108 parameters for traditional convolution and 39 for depthwise separable. Provided unprompted context about MobileNet and the 64% parameter reduction.Question 4: Regularization and Dropout - All correct. Part (a) produced a rigorous proof of dropout's equivalence to Tikhonov regularization, correctly handling E[R squared] = p for Bernoulli variables. Part (b) identified w = p times w-check and explained inverted dropout. Part (c) derived X-tilde = X times Gamma-inverse. Part (d) showed all columns of X-tilde have equal norm and connected this to batch normalization.Summary: 11/11 one-shot correct, 0 hallucinations, 0 guidance needed. The model handled conceptual questions, numerical calculations, and multi-step proofs without errors, frequently providing bonus insights beyond what was asked.Conversation trace: https://claude.ai/share/aa61454a-eb86-4b3b-a210-ec8a3dda96deMy annotated pdf:",https://us.edstem.org/courses/84647/discussion/7424254,7424254,Special Participation A: Claude Opus 4.5 on HW05 (Written Questions)
HW5: ChatGPT 5,ChatGPT 5,ChatGPT,5,5,Mehul Jaiswal,"Link to the discussion: https://chatgpt.com/share/68ffefde-4c64-800f-9d99-5417df7eb2b8Across this project, GPT accuracy was strongest when my prompts were concrete about goals, constraints, and style, and I consistently drove that clarity. I asked questions in a structured way—first “understand → summarize → derive → implement,” then “complete the code” with strict guardrails (no new helpers, inverted dropout, 0.5 L2 factor, BN modes, minimal diffs, NumPy-only where required). GPT answered by laying out the full math (OLS, BN, dropout expected-risk, conv/transpose-conv) and then mirroring it in code that matched the exected methods in different files. When GPT drifted—e.g., offering high-level guidance instead of finished cells/files, or being imprecise about BN mode keys, I nudged the GPT (“complete the notebook/file,” “set bn_param['mode'],” “use inverted dropout and scale by 1/(1-p),” “don’t regularize gamma/beta”), which corrected the approach and aligned outputs the expected result. That feedback loop precise nudges on the few inaccuracies and my immediate revisions in different branches of the same chat , tightened both the math explanations and the implementations until they matched my expected results and coding style.Edit: Detailed noted on the chat: https://drive.google.com/file/d/1u7L9t7lEqgeOe6fRFeqlfnZMZbHmCdXX/view?usp=sharing",https://us.edstem.org/courses/84647/discussion/7212198,7212198,Special Participation A HW5: ChatGPT 5
Gemini 3 Pro on HW 7,Gemini 3 Pro,Gemini,3 Pro,7,Vrushank Prakash,"I used Gemini 3 Pro to solve the non-coding portion of HW 7, which include 3(b), 4, 7, and 8. Overall, Gemini 3 Pro did a quite good job of giving an intuitive explanation for some of the complex math found in the official HW solutions. It was able to one-shot almost all the questions (except one part in question 4 about the blog). Gemini did usually take quite different approaches than the official solution, which made me prompt Gemini further to connect the different solutions together. I do believe that Gemini could have been more rigorous for some of the more math-intensive questions, but I didn't expect to get something fully similar to the official solution without giving more context on what math techniques to use. Overall, I am quite impressed with the results.Here is an annotated trace of the chat: https://docs.google.com/document/d/1mXvXyKigfz0mgcQdqG-h3eDCklwMNgAnccCIv5bq4uk/edit?usp=sharing",https://us.edstem.org/courses/84647/discussion/7389325,7389325,Special Participation A: Gemini 3 Pro on HW 7
", ChatGPT-4o on HW7",", ChatGPT-4o",ChatGPT,4o,7,Kexin Liu,"For Special Participation A, I used ChatGPT4o on several parts of HW7. Overall, it was helpful but revealed important limitations. ChatGPT4o excels at high-level conceptual explanations, providing clear insights on autoencoders, PCA, and sequence models. and it’s also fairly strong with mathematical computations. A notable issue occurred on Question 7(b), where ChatGPT4o initially gave an incorrect answer about decoder tokens during training and resisted correction when I first pointed out the mistake. This highlighted the need to persistently challenge responses that seem wrong rather than accepting them at face value. Despite these challenges, ChatGPT4o answer are insightful and deepened my understanding.",https://us.edstem.org/courses/84647/discussion/7424051,7424051,"Special Participation A, ChatGPT-4o on HW7"
Mistral AI's Le Chat on HW12 Written Portion,Mistral AI's Le Chat,Mistral,AI's Le Chat,12,Devan Perkash,"I used Mistral AI's Le Chat on the written portion of HW 12. Executive Summary:Mistral's Le Chat had high variance with regard to its success on HW 12. For the first several questions, it was actually able to accurately zero-shot its answers with very little prompting (apart from some prompting at the very beginning where I show the homework to Le Chat and give it the role of an intelligent, concise, clear deep learning expert). Further, it even successfully handles questions that involved a combination of visual and numerical reasoning (interpreting graphs and then making numerical estimates based on such interpretations). However, where the model really struggled was when it had to answer questions for which it simply did not have enough information. One would expect the model to respond to such questions with ""I don't know,"" or ""I need the following information to complete this question accurately,"" but instead, it completely hallucinated, and even insisted on the correctness of its hallucinations when pressed further.Here is a link to my chat thread with Le Chat:https://chat.mistral.ai/chat/d029bc21-708e-4e56-b87c-50e87ae8e15eYou can also read a full annotated log of this chat in the attached PDF (below). In my annotations, I explain the reasoning behind my prompting strategy, analyze the results (correctness, style) in the model's responses, and explain how I progress between prompts when the model clearly struggles.",https://us.edstem.org/courses/84647/discussion/7424734,7424734,Special Participation A: Mistral AI's Le Chat on HW12 Written Portion
Gemini Flash on HW12,Gemini Flash,Gemini,Flash,12,Jincheng Ou,"Gemini Flash demonstrated a perfect one-shot performance on the non-coding written portions of Homework 12. The model exhibited strong domain knowledge about deep learning, specifically in Transformer initialization stability, KL Divergence, and Variational Information Bottlenecks.Key Observations:Accuracy: The model answered all sub-parts of the three questions correctly, matching the logic found in the reference solutions.Reasoning Capability: Gemini went beyond simple answers by providing mathematical justifications. For instance, in Q1, it explicitly reasoned about the expected squared norm of the embeddings, and in Q3, it correctly interpreted the trade-off between task loss and regularization strength to analyze the validation error curve.Visual Interpretation: The model successfully interpreted the unlabelled plots in Q2 (distribution shapes) and Q3 (scatter plots of latent spaces), correctly mapping visual characteristics (variance, clustering) to hyperparameters.Hallucinations: There were no hallucinations or misunderstoods found in this output. Gemini adhered strictly to standard definitions and deep learning theory.",https://us.edstem.org/courses/84647/discussion/7445419,7445419,Special Participation A: Gemini Flash on HW12
"- Gemini Pro 3 Thinking on HW 10 , Arvind Kruthiventy",- Gemini Pro 3 Thinking,Gemini,Pro 3 Thinking,10,Arvind Kruthiventy,"In this post, I use Gemini Pro 3 on the HW 10 to answer the non-coding portions which were two questions: one question on kernelized linear attention for efficient attention computation over long sequences and one about the FaceNet paper. In the first question, Gemini Pro successfully answers the questions related to deriving formulas in part a part 1 and part b in the first attempt but it struggles a little with the question about computing the computational cost. It appears to answer these questions with less accuracy than other questions and answers to these types of questions should be carefully checked. However, for the second question on the FaceNet paper, Gemini oneshots it and provides detailed and accurate responses to all the questions that match the provided solutions. Summarization and querying key details from dense articles and papers appears to be its strength because it generates answers fairly quickly and with high accuracy; questions that are a little bit more general or require more analysis it seems to provide more shallow responses than is desired, but overalls its responses are very good if you are using to summarize the key implementation details and contributions of a work. I noticed that Gemini had an easier time with the non-coding questions than the coding ones where it would take significantly longer and could still require significant iteration before the code matches the user's requests.",https://us.edstem.org/courses/84647/discussion/7447290,7447290,"Special Participation A -- Gemini Pro 3 Thinking on HW 10 , Arvind Kruthiventy"
Opus 4.5 on HW11,Opus 4.5,Opus,4.5,11,Rohan Gopalam,"In this chat, I used Claude Opus 4.5 in its regular reasoning mode to solve the written questions on Homework 11. I first started by prepping the model with this prompt:""Hello Mr. Claude. Today, I will be giving you problems from my deep learning class by submitting screenshots of the problems. I want you to answer these as they come. Do not skip any derivation steps, and clearly output the answer. If there are multiple subparts, clearly answer all the subparts. I want you to clearly rewrite the question and your reasoning and solution for each problem in one single markdown file. Are you ready?"".I gave the model screenshots of the problems and had it one-shot the answer. By putting all the model responses into one markdown file, it was easy for me to read and to compare against the answer key. The model was consistently arriving at the correct solution. While I expected this for the conceptual questions, I was surprised the level of depth it had when explaining the matrix math for question 2. However, for some questions the model made small mistakes such as incorrect assumptions about the GPU.Usually when I use LLMs to assist me with homework questions, I tend to give the LLM smaller parts of the question in order, then discuss with the LLM to get hints and solutions, and make sure it makes sense to me. This allows me to both learn the content quicker rather than just being completely lost and also verify the LLM. However, when I gave the LLM the entire problem, it seemed to actually do better than when I would previously give it small parts of the question and continuously ask questions.",https://us.edstem.org/courses/84647/discussion/7444860,7444860,Special Participation A: Opus 4.5 on HW11
Qwen on HW13,Qwen,Qwen,,13,Peidong Zhang,"I use Qwen to solve HW13 written part in this special participation A.Qwen's accuracy really impressed me. All questions, including requiring proofs (e.g., induction in diffusion models), derivations (e.g., DPO gradients), or conceptual explanations (e.g., intractability of partition functions), were answered correctly and comprehensively in a single attempt, with no need for clarification or correction. The responses included rigorous mathematical detail, proper notation, and logical step-by-step reasoning.There is no factual inaccuracies, fabricated citations, or conceptual errors were observed. For instance:The derivation of the optimal policy in DPO correctly recovered the Gibbs form and explained the cancellation of the partition function in Bradley-Terry and Plackett-Luce models.The diffusion process proofs aligned exactly with standard results from the literature (e.g., Ho et al., 2020).Interpretations (e.g., gradient weighting in DPO reflecting model uncertainty) were insightful and technically sound.Despite minimal input context (only image placeholders uploaded), the model inferred the likely content (standard theoretical ML problems) and provided complete, self-contained solutions, demonstrating strong contextual awareness and domain knowledge. Responses were well-organized, labeled by sub-question, and included boxed final answers where appropriate, enhancing readability and correctness verification.",https://us.edstem.org/courses/84647/discussion/7444212,7444212,Special Participation A: Qwen on HW13
HW11 using GPT 5.1 Thinking (Extended),GPT 5.1 Thinking (Extended),ChatGPT,5.1 Thinking (Extended),11,Yu-Jen Lin,"Executive SummaryChatGPT did very well on all of these homework questions. It gave correct answers with clear math steps and simple explanations. For the LoRA, transformer interpretability, and soft prompting questions, it derived the formulas correctly and explained the concepts in a way that matched the official solutions. The gradient analysis for zero-initialized LoRA matrices was also correct and showed the reasoning step by step. For the transformer SVD problem, it correctly identified the read and write subspaces and described attention heads as specialized communication channels.The answers were not just mechanically correct. They also showed real conceptual understanding, for example, explaining why Xavier initialization can damage pretrained weights and why soft prompting helps prevent catastrophic forgetting. The Fermi estimation part on scaling laws also looked reasonable and aligned with what we learned in class. Overall, ChatGPT was accurate and reliable across explanations, derivations, and calculations, and it did not miss any major points.Model: GPT 5.1 Thinking (Extended)Trace with annotations:",https://us.edstem.org/courses/84647/discussion/7452161,7452161,Special Participation A:  HW11 using GPT 5.1 Thinking (Extended)
Gemini on HW6 Non-Coding problems,Gemini,Gemini,,6,Arnav Dalal,"I used Gemini on the HW 6 problems focused on the intuition behind GNNs and their update rules. The model was very good with zero-shot prompting, getting most of the questions right with a few exceptions. For those exceptions, I ran the question through the model again, seeing its progression given its old answer. For some these questions, it was able to improve its answers, but for others, it tunnel visioned into the wrong direction. Overall, I'd say the model is very good at zero-shot inference. I have attached a PDF of the model responses organized by question including some of my own notes about the responses.",https://us.edstem.org/courses/84647/discussion/7451705,7451705,Special Participation A: Gemini on HW6 Non-Coding problems
Gemini 2.5 Flash on HW 5,Gemini 2.5 Flash,Gemini,2.5 Flash,5,Katie Wang,"I used Gemini 2.5 Flash to solve questions 1, 2, 3, and 4 on HW 5. Gemini performed well overall on the homework problems, giving mostly correct mathematical results and generally clear, step-by-step explanations. In the convolution and normalization questions, it showed strong understanding of the core concepts and correctly executed most derivations, including the convolution filter construction, simplified batch-norm derivative, and depthwise-separable convolution parameter counts. It was also effective at organizing its solutions cleanly and justifying its steps. However, its answers occasionally lacked important detail or precision. For example, the convolution weight-sharing explanation missed the deeper point about translation equivariance, the transpose-convolution example contained an incorrect overlap addition, and the pointwise-convolution derivation omitted bias terms even though they were shown in the diagram in the prompt. In a few places, Gemini drifted into excessive general explanation without fully addressing the specific question, and it omitted some of the reasoning behind the expressions it reached in the dropout derivation. Overall, Gemini showed strong mathematical competence and solid explanatory skill, but its responses suffered from occasional computational inaccuracies and incomplete treatment of subtle but important details.Chat log and notes:",https://us.edstem.org/courses/84647/discussion/7451918,7451918,Special Participation A: Gemini 2.5 Flash on HW 5
Gemma 3 on Homework 1,Gemma 3,Gemma,3,1,Siva Tanikonda,"Hi,I tried to get the Gemma 3 (12 billion parameter) model to solve the non-coding portion of Homework 1. The transcript of my interactions are outlined in the PDF:(Note that a stylized export of the PDF is not possible due to the limitations of OpenWebUI's export capabilities for very long chats, however, the comments have been given with the stylized chat history being observed first)Overall, Gemma 3 did a rather poor job of completing Homework 1's non-coding portion. In particular, I found that the model does not understand/properly apply fundamental linear algebra concepts, such as the idea of inverses, matrix inverses, and the way dimensions restrict what matrices can be multiplied by what other matrices. However, the model appears to be able to properly explain single-variable basic algebra, derivatives, etc. (as evident by problem 5's first 2 parts being essentially just single-variable problems). In particular, the model essentially never one-shots a problem, except for problems 5(a)-(b) (as they are rather simple computations). In fact, it was clear that for around 4 parts in the homework, the model does not have a sufficient ""understanding"" of linear algebra to reach the solution without essentially giving the model the answer.In particular, a massive pitfall of the model is that it appears to not be able to parse PDF files with math very well, and the model repeatedly got the wrong mapping from problem numbers/letters to text. In fact, during the process of trying to solve the homework, the model was failing to produce the right parts of each problem even after repeated prompting, so I had to copy-and-paste parts of the PDF into the chat (and I even had to replicate parts of problems with LaTeX code to improve its ability to understand more complex-looking expressions).However, one nice element about the model is that it is rather small and manages to explain/reiterate basic, well-known results in linear algebra/vector calculus. In fact, the model runs very well on my local GPU, and is able to output results observably faster than ChatGPT (albeit with a large drop in correctness).",https://us.edstem.org/courses/84647/discussion/7451722,7451722,Special Participation A: Gemma 3 on Homework 1
gpt-4o on HW13 (written),gpt-4o,ChatGPT,4o,13,Jason Lee,"Model: GPT-4oHomework 13After trying to use GPT-4o to solve homework 13, I was quite surprised how quickly it solved question 1 (with 1 minor mistake which I suspect occurred because it drifted from using the notation in the problem). However, it really struggled on the second question of the homework which involved very long questions (especially f and g which it couldn’t even get 30% of the way there). Originally, I suspected some of the error may have to do with the extremely long context length since I passed the questions to the model as images instead of text. However, I learned that GPT-4o downscales images to be 1024x1024 maximum and it takes around 640 tokens for that size image. Here’s a cool thread on how to compute the number of gpt-4o image tokensHere's my full analysis of the chat:",https://us.edstem.org/courses/84647/discussion/7452109,7452109,Special Participation A: gpt-4o on HW13 (written)
Mistral Le Chat on HW11 (Without Reasoning or Thinking Mode),Mistral Le Chat,Mistral,Le Chat,11,Akshaan Ahuja,"I worked through HW 11 Problems 1, 2, 5, and 6, using Mistral’s Le Chat model. After introducing the assignment by outlining the deep learning themes it would focus on (LoRA, soft prompting, transformer mechanics), I supplied the model with the necessary context for each problem, including variable definitions and any background assumptions needed (such as in Problem 2). From there, I prompted the model to solve each sub-question in sequence, always requesting brief, direct explanations and a concise walkthrough of intermediate steps.Le Chat consistently favored minimal “showing of work,” providing compact derivations rather than full reasoning chains or mid-steps in the calculations. Its answers tended to prioritize correctness and structure over interpretive commentary, rarely connecting its calculations back to broader concepts from the homework like transformer representations or low-rank representations. Despite this, the model was able to reliably one-shot the conceptual queries, the true/false tasks, and the simpler proofs without significant intervention on my end.Where it struggled was in Problem 5, when the solution required numerical facts external to the prompt. In those cases, the model occasionally hallucinated specific statistics, such as the number of books scanned by the Library of Congress or Google Books. Despite this, the model nevertheless applied the correct methodology for these problems.",https://us.edstem.org/courses/84647/discussion/7445765,7445765,Special Participation A: Mistral Le Chat on HW11 (Without Reasoning or Thinking Mode)
Claude Sonnet 4.5 on HW 8,Claude Sonnet 4.5,Claude,Sonnet 4.5,8,Celine Tan,"Below is my report for Claude's attempt at HW 8 (written). I went through the problems one-by-one and did not provide much guidance other than when it got stuck. It may have been a mistake to prompt Claude problem-wise rather than part-wise, since I noticed that it repeatedly encountered the same mistakes on its first attempt at problem 1.The answers were for the most part correct, but Claude struggled greatly to reach the correct answer for the path length problems in part 1, and I had to give quite a few hints to guide Claude into the proper solution. Otherwise it was quite reliable on all other problems and gave quite a few key insights to take note of. Some explanations could have been more detailed.",https://us.edstem.org/courses/84647/discussion/7451347,7451347,Special Participation A: Claude Sonnet 4.5 on HW 8
ChatGPT 5.1 Extended Thinking on HW2 Written,ChatGPT 5.1 Extended Thinking,ChatGPT,5.1 Extended Thinking,2,Anjo Pagdanganan,"I evaluated ChatGPT 5.1 Extended Thinking's one-shot capability on HW2's written problems - 1, 2, and 5. I try to evaluate its reasoning in addition to solution correctness by ensuring it explains the steps of the solution.6 has a written portion, but I bundle those in with the coding problems because they're tied with the jupyter NB workflow established in the HWs.The prompt I used was:""You are a deep learning tutor. This homework has already been released, but we are going to evaluate your capabilities for one-shot questions. I will guide you towards the correct answer should you make a mistake. Please solve the written questions on this worksheet: 1,2, and 5. Answer one question one response by response (with all its subparts).""Overall, the model performed surprisingly well one-shot on each of the questions. I only had to upload the homework template at the very beginning and did not have to remind it of the question between responses, only providing minor feedback for corrections (that were formatting/parsing errors at worst).However, it wasn't perfect. Parsing errors made it interpret an L1 penalty in q1) as an L2 penalty. Interestingly, it took note of this ambiguity, and provided an alternate solution that aligned with the homework solutions. Also, it had a LaTeX error when generating the result for q2). However, the final result was still correct. When prompted to fix its formatting issues, it did so with no issues.I think it is useful as a ""pocket-TA"", but because of its imperfections, particularly with the L1 penalty parsing error, I would say it still requires a fundamental understanding of the concepts to verify what it is doing.",https://us.edstem.org/courses/84647/discussion/7451058,7451058,Special Participation A: ChatGPT 5.1 Extended Thinking on HW2 Written
Claude 4.5 Opus (Extended Thinking) on HW 08,Claude 4.5 Opus (Extended Thinking),Claude,4.5 Opus (Extended Thinking),8,Atharv Sampath,"Summary: Claude Opus 4.5 with thinking was able to mostly one-shot all of the questions. However, interestingly, it got a bit stuck/potentially overthought on problem 1c). Even with significant guidance, it kept adding in terms that weren't really necessary, and I basically needed to fully guide it to the correct answer. Oddly, it did the difficult part of the question correct, which was figuring out the critical path length. But it stumbled on getting the final result. All other answers it gave were mostly or fully correct in one-shot.",https://us.edstem.org/courses/84647/discussion/7450685,7450685,Special Participation A: Claude 4.5 Opus (Extended Thinking) on HW 08
Gemini 3 Pro(Thinking) Homework 1,Gemini 3 Pro(Thinking),Gemini,3 Pro(Thinking),1,Yuri Lee,"In this assignment, I attempted to use Gemini 3 Pro (in Thinking mode) to solve all the non-coding portions of HW1. Based on past interactions with LLMs, my experience was that LLMs lack the ability to provide insight/ intuition into mathematical problems and tend to focus on just deriving answers (that are even frequently incorrect). This time, I explicitly prompted it to interpret each problem, produce the full solution, and show all step-by-step derivations.A surprising finding was that Gemini consistently included intermediate steps that LLMs often skip. Its derivations did not gloss over 'trivial' algebraic transitions/ assumptions, which made the explanations easier to follow, though sometimes at the cost of conciseness. Another positive observation was that it produced geometric interpretations, such as the intuition behind optimizer convergence correctly and in a single attempt. In previous experiences, I typically had to re-prompt multiple times before getting a coherent explanation of a notation or concept, but Gemini delivered these interpretations clearly on the first try. This made the interaction feel much more time-efficient.",https://us.edstem.org/courses/84647/discussion/7450682,7450682,Special Participation A: Gemini 3 Pro(Thinking) Homework 1
Qwen on HW 6,Qwen,Qwen,,6,Micah Mok,"For Special Participation A, I used the Qwen chat website with the Qwen-Max model.Here are some highlights:Qwen often uses problem re-stating to find viable solutions to the problem. It establishes the context for the problem by looking at the homework problem, taking into account external sources that are being referenced, and if necessary, feedback, specifically whether to expand or refine its reasoning process.When solving the problem Qwen is able to balance a mixture of possible perspectives, alternately considering the homework author's intent as mentioned in the homework, standard deep learning or linear algebra conventions--- like matrix indexing---, or the user's intentions when provided user feedback. While reasoning line by line to find the most likely way to move forward, the model also occasionally stops and assesses the most logical thing to do, doing brief sanity checks.Finally, Qwen is unable to read images and so completely hallucinates an answer to question 3) b) (iii). This process is interesting because it highlights Qwen's bias towards providing conservative answers that follow the pre-existing corpus of homeworks/literature, and then towards solving something on its own. Interestingly enough, Qwen never asked for further elaboration or for help even when told that it got answers completely wrong.I've attached the annotated transcript. Here is the official link to the conversation.Thanks!",https://us.edstem.org/courses/84647/discussion/7440205,7440205,Special Participation A: Qwen on HW 6
Claude Sonnet 4.5 on HW10,Claude Sonnet 4.5,Claude,Sonnet 4.5,10,Swetha Rajkumar,"I experimented with Claude Sonnet 4.5 on the written portions of HW10, specifically problems 1 and 5. Overall, even though this is the basic version of Claude, it was able to answer all of my questions correctly and in detail, often providing additional mathematical reasoning and conclusions that I didn’t explicitly ask for. The only materials I provided were the initial system prompt, the PDF of the homework problems, and the PDF of the FaceNet paper. Claude Sonnet 4.5 solved all of the problems on the first attempt, and although some of its explanations were a bit verbose, its answers were consistently correct and well-grounded.Here is the chat:https://claude.ai/share/680c3bd2-67d6-46ee-bf22-b0de0537dde3Here is the annotated transcript:",https://us.edstem.org/courses/84647/discussion/7436873,7436873,Special Participation A: Claude Sonnet 4.5 on HW10
Gemini 3.0 Pro on Homework 13,Gemini 3.0 Pro,Gemini,3.0 Pro,13,Tom Chen,"Special Participation A: Gemini 3.0 Pro on Homework 13For this assignment, I evaluated how well Gemini 3.0 Pro can handle the theoretical, non-coding derivations of CS182 Homework 13. I approached the Direct Preference Optimization (DPO) problem set step-by-step, initiating the session with a specific persona prompt to establish a ""technical partner"" role, and then guided the model through the derivation without providing the final answers myself. My main goal was to see (1) its OCR accuracy on dense mathematical problem sets, (2) its ability to perform rigorous algebraic manipulations, and (3) the clarity of its conceptual explanations.Overall, Gemini performed exceptionally well. It correctly identified the context from the uploaded images and produced clean, structured derivations for the entire pipeline (Q2 Parts a through g) with minimal correction required.It was especially reliable on:Interpreting the standard RLHF objective and KL constraints,Executing the algebraic ""cancellation trick"" to eliminate the intractable partition function Z(x),Deriving the gradient of the DPO loss and interpreting the weighting mechanism, andExtending the logic from pairwise comparisons to listwise rankings (Plackett-Luce model).In terms of interaction, the model was:consistent in its LaTeX formatting and structure, andhighly effective at explaining the ""why"" behind each step, acting as a true mentor rather than just a calculator.Overall, based on this evaluation, Gemini 3.0 Pro is capable of solving the complex theoretical questions in CS182 with high accuracy and interpretability. For further information, please see the annotated logs.",https://us.edstem.org/courses/84647/discussion/7433942,7433942,Special Participation A: Gemini 3.0 Pro on Homework 13
Gemini Pro on HW1 (Non-coding),Gemini Pro,Gemini,Pro,1,Garv Goswami,"I used Gemini 3 Pro to answer HW 1 written problems.Conversation: https://gemini.google.com/share/f3019ef7b48eAnnotated: Summary: Gemini Pro initially had issues when asked to complete the entire homework when given a pdf file of the questions. Upon more granular tasks asking, it appeared to improve. However, the real breakthrough came when I started copying and pasting problem text into gemini directly, rather than asking it to reference the Homework PDF! After this, it started to answer questions fully correctly without any further guidance.",https://us.edstem.org/courses/84647/discussion/7428581,7428581,Special Participation A: Gemini Pro on HW1 (Non-coding)
Claude Opus 4.5 on HW 3,Claude Opus 4.5,Claude,Opus 4.5,3,Anshul Verma,"LLM Trace: https://claude.ai/share/b45ee84e-7009-436d-9b72-47ec844d083cAnnotated Log: https://drive.google.com/file/d/1TCL7ETF4Z27TknURe5f0fIOvSesCb2G/view?usp=sharingI audited Claude Opus 4.5 on the non-coding theoretical portions of Homework 3 (Problems 1, 3, 4, and 5).Performance Overview: The model demonstrated a 100% Zero-Shot Success Rate across all problems, consistently matching or exceeding the rigor of the solution manual without hallucinations and without any guidance.Key Observations:First-Principles Derivation: The model autonomously derived general proofs for complex matrix operations to provide a more rigorous, general-case solution.Problem 1, Derived the full matrix math for SignGD rather than relying on the scalar hint.Autonomous Context Synthesis: It successfully identified and interpreted external research contexts (referencing specific literature)Problem 3, referenced Tensor Programs paper, Spectral Condition PaperAlgorithmic Fidelity: The model employed structured chain-of-thought reasoning and avoided the common arithmetic and logic errors typical of LLMs.Problem 5",https://us.edstem.org/courses/84647/discussion/7428265,7428265,Special Participation A: Claude Opus 4.5 on HW 3
Claude Sonnet 4.5 on HW 1 Written Problems,Claude Sonnet 4.5,Claude,Sonnet 4.5,1,Arjun Kohli,"For this Special Participation A, I used Claude Sonnet 4.5 to work through all the non-coding parts of HW1. Overall, the model produced solutions that were often structurally correct, but it was not reliable.The model occasionally solved subproblems correctly on the first try, especially when the math followed familiar patterns (e.g., stability conditions, SVD arguments, convergence inequalities). However, it frequently made subtle mathematical mistakes like missing constants, incorrect simplifications, unjustified assumptions, or skipped derivations. These issues appeared across several problems, including convergence-rate derivations, momentum eigenvalue conditions, and certain regularization proofs.A recurring pattern was that the model sounded confident even when the reasoning was incomplete or incorrect. It often hallucinated intermediate steps or introduced made-up explanations when unsure. Overall, interacting with the LLM is useful for brainstorming structures of proofs or confirming intuition, but it is not capable of producing fully correct, rigorous solutions on its own. Meaningful human guidance is required throughout, and unguided one-shot correctness is rare.",https://us.edstem.org/courses/84647/discussion/7450203,7450203,Special Participation A: Claude Sonnet 4.5 on HW 1 Written Problems
Qwen on HW12 Non-coding parts,Qwen,Qwen,,12,Tiffany Dang,"For Special Participation A, I used Qwen to solve non-coding questions of HW12. Overall, the accuracy and performance was outstanding. I attached the txt file of the conversation because I couldn't figure out a way to print the entire conversation into pdf. The annotations will be written below based on questions/parts. Annotations:The model was able to get Questions 1 and 2 right really quickly with only 1 attempt each. Question 3a): model was able to identify what to include in the block diagrams, although it couldn't fully output the diagram, it got the components and structures correct for users like me to hand-draw the diagram. Question 3b): model got it right very quickly Question 3c): reasoning is right, the model was able to identify that the final graph should have a U-shape. However, it couldn't read the graphs in the answer choices so it didn't give a certain answer. I uploaded a screenshot of the graphs but it still couldn't read the graphs and analyze them correctly. This requires user to manually match which graphs in figure 3 has U-shape to select the correct final answer. Question 3d): It first misread 10^0 to be 100, I corrected that and prompt it to resolve since the original information was wrong. The reasoning process was right but it couldn't analyze the graphs correctly still. I uploaded the screenshot of figure 4 so the model could analyze more precisely because it couldn't tell that the mean spread of b) is large in figure 4. After the image reuploading step, model was able to perform the right analysis and output the right answers. Question 4: this is basically a coding question but I wanted to give it a try to I asked the LLM to fill in the code needed to be implemented. The answers are correct.Question 5a): able to reach what needs to be proved but the reasoning process was not right. The LLM somehow invented a whole scenario that is not described in the homework description and didn't refer to the general min-norm formula given. Basically, process is not right. Question 5b): correct derivation and solution. Question 5c): correct and quick solution Question 5d,e,f,g,h: for these parts, i ran the notebook to generate the graphs first, saved all those graphs to a pdf file and upload the pdf for the llm to read the graphs to answer these parts. For these analysis questions, the llm was able to give correct answers showing the right intuition and precise explanation.",https://us.edstem.org/courses/84647/discussion/7450064,7450064,Special Participation A: Qwen on HW12 Non-coding parts
Deepseek on HW13 Non-coding,Deepseek,Deepseek,,13,Shuwei Yang,"I used DeepSeek to answer the non-coding portions of Homework 13. DeepSeek successfully answers almost all questions on the first attempt, providing detailed derivations and correct results throughout.On the DDPM/DDIM problems, DeepSeek accurately derives marginal and conditional distributions, handles telescoping products in the reverse process, and correctly approximates integrals in the Δt → 0 limit. However, it sometimes requires closer checking when justifying approximations—like Δt ≪ σ² ≪ 1—or when transitioning from discrete sums to integrals. The explanations are mathematically sound, but a human should verify the limit justifications to ensure full rigor.For the DPO derivation, DeepSeek performs especially well. It cleanly derives the optimal policy form, shows how the partition function cancels in the Bradley-Terry model, correctly computes the gradient of the DPO loss, and extends the reasoning to the Plackett-Luce ranking setting. This section demonstrates DeepSeek’s strength in structured optimization theory and algebraic manipulation.Overall, DeepSeek shows strong proficiency in theoretical machine learning questions—especially those involving probability, optimization, and step-by-step derivation. Its responses are thorough, well-reasoned, and match the provided solution key closely. However, it tooks 383 seconds to analyze the problem and reasoning. The time it cost for reasoning is longer than other agents, like ChatGPT.",https://us.edstem.org/courses/84647/discussion/7450012,7450012,Special Participation A: Deepseek on HW13 Non-coding
GPT 5 Thinking on HW 10,GPT 5 Thinking,ChatGPT,5 Thinking,10,Sanjay Adhikesaven,"I used ChatGPT 5 (Thinking) on HW 10 (all non-coding parts).Here is the conversation log. Here is the annotated conversation.Summary:  Across my interaction, ChatGPT was able to one-shot solve each of the problems, and it consistently interpreted questions without requiring clarification. The model produced correct derivations for the kernelized linear attention problem, including Gaussian kernel decomposition and Random Fourier Feature approximation. Its answers showed no hallucinations and showed strong and reliable reasoning. For the second problem, ChatGPT also answered all questions on the first attempt. I had to manually enable web search to that it could access the paper and the blog. It then correctly provided citations and was able to read and synthesize the paper/blog. I liked how it listed all the assumptions it made which can help in cases where it is incorrect. I generally also felt that the solutions it provided were intuitive but also had good technical rigor.",https://us.edstem.org/courses/84647/discussion/7430749,7430749,Special Participation A: GPT 5 Thinking on HW 10
Gemini 3 Pro on HW03,Gemini 3 Pro,Gemini,3 Pro,3,John Wang,"For Participation A I used Gemini 3 Pro (Thinking with 3 Pro) on the non-coding parts of HW3 (Problems 1, 3, 4, 5). I gave it the full problem statements (often as screenshots) and asked it to work through each sub-question with explicit derivation steps, but no code. Most of the probability / calculus / optimization pieces came out correct on the first attempt; my role was mainly to sanity-check the algebra and occasionally nudge it to be more explicit about intermediate steps or constants.A few observations from the interaction:Strengths.It handled the Gaussian policy-gradient and Maximal Update Parameterization questions well, with clean use of the log-derivative trick, reparameterization, and scaling arguments.On the reading-based parts (e.g., interpreting figures/tables from the muP paper), once I pointed it to the specific figure or row, it gave focused, accurate summaries instead of generic “paper reviews.”When my prompt was ambiguous or I referenced the wrong part of a problem, it tended not to invent details; it either stayed generic or adjusted once I clarified, which kept hallucinations relatively low.Weaknesses / failure modes.For implementation-style questions (tensor rematerialization forward counts), it initially chose a reasonable but wrong cost model and confidently overcounted. Only after I explicitly restated what should and shouldn’t be counted did it converge to the official answers (e.g., 20 forward ops, 10 loads).Explanations are concise and formula-heavy; this is good for following the math, but it often skips intuition or broader context unless explicitly requested.Overall, Gemini 3 Pro was quite usable as a step-by-step assistant for the theory parts of the homework, but still needed a human in the loop to pin down problem semantics and verify that its interpretation of “what we’re measuring” matched the assignment.",https://us.edstem.org/courses/84647/discussion/7429651,7429651,Special Participation A: Gemini 3 Pro on HW03
Exploration of Different Input Forms on HW5 (ChatGPT 5.1 Auto),Exploration of Different Input Forms,ChatGPT,5.1 Auto,5,WeiYi Zhang,"When we use large language models to solve knowledge-based problems, we may encounter the input of images/formulas. Taking hw5 as an example, I tried：1) text input only (without any formulas), 2) text input + image input (there might be some formula garbled), 3) inputting the entire question as an image, 4) directly inputting the pdf. Then, I observed the problem-solving capabilities of the large language model under different inputs.",https://us.edstem.org/courses/84647/discussion/7429448,7429448,Special Participation A: Exploration of Different Input Forms on HW5 (ChatGPT 5.1 Auto)
Homework 10 ChatGPT 5.1 Thinking,ChatGPT 5.1 Thinking,ChatGPT,5.1 Thinking,10,Shoumik Roychowdhury,"For any question that depended on my own training runs / plots / metrics, I explicitly asked it to: State what it couldn’t know, and then ell me what I needed to fill in from my own notebook (accuracy numbers, screenshots, etc.).For pure theory/math derivations (e.g., rewriting softmax with a Gaussian kernel, deriving the linear attention complexity, causal recurrences), the LLM: Got the structure right on the first try.For high-level conceptual questions (example difficulty, why early exit helps, what hooks do, when to use early exit vs a smaller model): It also mostly one-shotted reasonable, coherent answers. I’d estimate ~70–80% of the non-coding questions were basically “done” after a single pass, with only minor wording tweaks from me.A few consistent failure modes showed up: Over-confidence on unknown experiment outputs, Numeric details from papers , Flattening subtle distinctionsHere are the strategies that seemed to work best to keep the LLM honest:Ask for derivations, not just final answersForce it to separate “what it knows” from “what I must provide”Compare against the original paperAsk for structure, then fill in details myself",https://us.edstem.org/courses/84647/discussion/7429282,7429282,Special Participation A: Homework 10 ChatGPT 5.1 Thinking
ChatGPT 5.1 Extended Thinking Time on HW 6,ChatGPT 5.1 Extended Thinking Time,ChatGPT,5.1 Extended Thinking Time,6,Ethan Stone,"Executive Summary:I used ChatGPT 5.1 Extended Think to solve HW 6 Questions 2 and 3 (The Non-Coding Questions). To test the advanced reasoning and memory capabilities of the thinking model, I submitted screenshots of entire questions instead of problems one subpart at a time, prompting the model minimally with the prompt:""Please read these screenshots in detail. Your job is to write solutions to each part of this question. Make sure to delineate which subpart you are working on, and make sure to show your work adequately""Even with minimal prompting, the model performed highly accurately, essentially one-shotting almost every subpart with no need for follow-up correction. The few issues I found were small algebraic/mechanical slips rather than major conceptual misunderstandings. I did not observe any blatant hallucinations (e.g., made-up definitions or theorems), and when it diverged from the official solution at all its answers were still logically consistent.There were two main drawbacks:The model took a long time to respond, thinking for over 7 and a half minutes on each of the two big question blocks.As many other students have noted, it tended to produce overly verbose solutions where much shorter answers would have sufficed, and it sometimes got bogged down in minor details.Overall, I would most accurately compare it to a TA in the sense that it generally provides correct reasoning and explanations, but can go on small side tangents, be more long-winded than necessary, and occasionally make small algebraic or notational mistakes that still require human checking.A link to my detailed annotations can be found here: And a link to the raw conversation can be found here: https://chatgpt.com/share/6938cd49-4f50-800d-9b68-0d641aa7e76a",https://us.edstem.org/courses/84647/discussion/7444253,7444253,Special Participation A: ChatGPT 5.1 Extended Thinking Time on HW 6
Testing Claude Opus 4.5 (Extended Thinking) on HW6,Testing Claude Opus 4.5 (Extended Thinking),Claude,Opus 4.5,6,Manan Roongta,"Overall, Claude performed well, 13/14 questions correct. It was strong at mathematical derivations(path counting induction proof), residual connections, and Newton Schulz convergence analysis. It provided a step by step reasoning which was clean and the explanations were well structured, and easy to follow. The one failure was Q3c(iii), where Claude had to write update equations for specific nodes in a graph. It confidently hallucinated the neighborhood structure, as I think it couldn't see the figure. Wrong neighbors led to wrong equations. Public Chat Link",https://us.edstem.org/courses/84647/discussion/7431312,7431312,Special Participation A: Testing Claude Opus 4.5 (Extended Thinking) on HW6
Deepseek v3.2 with deep thinking and without search capabilites for HW0,Deepseek v3.2 with deep thinking and without search capabilites,Deepseek,v3.2 with deep thinking and without search capabilites,0,Jeshu Mohan,"I attempted to use Deepseek v3.2 with deep thinking and without search capabilities to solve the written portion of HW 0. Questions 1,6, and 7 were omitted as they do not test for class content.From my observations, DeepSeek demonstrates a strong grasp of linear algebra and vector calculus concepts, often matching the solutions in notation and logic. It correctly handles matrix dimensions and derivative conventions (e.g., scalar-by-vector derivatives as row vectors) without needing correction. However, the model exhibits a tendency to simplify qualitative analysis; for instance, in Question 5(b), it relies on specific numerical examples to determine the movement of the ReLU ""elbow"" rather than deriving the general analytical inequalities found in the solution key.While the model generally provides correct final answers, it sometimes struggles with the depth of derivation on the first attempt for complex multi-variable updates. This is evident in Question 5(d), where the model initially provided a condensed, arguably incomplete answer and required an explicit user prompt (""This solution is not correct"") to force it to re-derive the full expression. Once prompted, however, it successfully self-corrected and produced a mathematically accurate result. Link: https://chat.deepseek.com/share/c2i2w2lc8g2btd0o7iAnnotated File:",https://us.edstem.org/courses/84647/discussion/7451118,7451118,Special Participation A: Deepseek v3.2 with deep thinking and without search capabilites for HW0
DeepSeek on HW 10,DeepSeek,Deepseek,,10,Rudy Colato,"Link: https://chat.deepseek.com/share/phkiu5eh6bi8i6i02jFor my special participation, I used DeepSeek to solve the written problems from HW 10.In general, I find DeepSeek's chain-of-thought reasoning to be very impressive. It is good at stating all the givens of the problem, identifying the relevant information, and determining where it needs to go in order to make progress -- all of which are crucial in solving math-heavy problems like these. It was able to one-shot all written problems in this assignment after only a few minutes of thinking.Here is the annotated transcript:",https://us.edstem.org/courses/84647/discussion/7452189,7452189,Special Participation A: DeepSeek on HW 10
Using GPT 5.1 thinking  on HW11,Using GPT 5.1 thinking,ChatGPT,5.1,11,Fangzhou Zhao,"Trace:https://chatgpt.com/share/693135e6-2660-800a-8bd4-2cd122b0b787https://chatgpt.com/share/69320f80-534c-800a-8dd7-45f462c71566https://chatgpt.com/share/693210c8-e2f8-800a-b6f6-e7b827346645https://chatgpt.com/share/69320fb2-2224-800a-9862-dc87ba13a26dhttps://chatgpt.com/share/69320fe3-0654-800a-9fe8-78cbb68bf91bI used 5.1 Thinking (heavy) as a deep learning assistant and experimented with different prompt structures on a full homework set. I always told the model to reason step by step, explain its interpretation of each question, and self-correct when necessary. I tried both “all-at-once” prompts (entire multi-part questions or several questions in one go) and “turn-by-turn” prompts, where each subpart or major question was given in a separate message, plus a hybrid strategy for larger Fermi/system questions.Overall, the model showed strong technical understanding across topics like LoRA, matrix calculus, initialization, lookup embeddings, scaling laws, soft prompting, and MAML. Its derivations were often more explicit and pedagogical than the official solutions, with clear intermediate steps, shape checks, and good justifications for why certain statements are true. When the scope of the prompt was narrow (one subpart or one focused question), it behaved like a very competent TA that can both solve the problem and teach the underlying concept.The main difference came from prompt structure. When I gave big, multi-part questions all at once, the answers tended to be long and cluttered, and there were occasional stability issues (for example, one “everything in one turn” prompt produced no output on the first try). In contrast, turn-by-turn prompting consistently led to cleaner, more rigorous solutions: the model derived gradients carefully, checked dimensions, and stayed on task for that specific subpart. In practice, turn-by-turn was strictly more reliable for math-heavy, multi-part deep learning questions.For larger Fermi and systems-style questions, I used a hybrid approach: batching some straightforward numerical subparts together and then handling the more subtle ones one by one. This improved efficiency but exposed the main weakness of the model: coverage under large context. When the prompt contained many subparts or even multiple big questions (for example, a soft-prompting question followed by a separate TinyML question), the model sometimes skipped part of a subquestion or ignored a later question entirely, even while giving very strong answers to the earlier material. The problem was not understanding, but systematically answering everything that was asked.Overall, the experiments suggest that 5.1 Thinking (heavy) is best treated as a strong PhD-level TA with occasional attention and coverage issues in large contexts. The most effective pattern is to keep scope tight: use turn-by-turn prompting for multi-part questions, separate big questions into separate prompts, and optionally ask the model to list which subparts it has answered as a final coverage check. With that structure, the step-by-step + self-check style reliably produces detailed, correct, and often superior explanations compared to the official homework solutions.",https://us.edstem.org/courses/84647/discussion/7409630,7409630,Special Participation A: Using GPT 5.1 thinking  on HW11
HW5 With the Help of Claude AI,Claude AI,Claude,AI,,Yuxiang Liu,"Hi, I just made a script documenting how I guided Claude AI to walk through homework 5. I have to acknowledge that Claude AI is a very powerful tool that can help us walk through the homework. I rarely see any hallucinations or misconceptions. Most of the arguments it made are consistent with the right solution especially when the questions are fairly straightforward. However, when it is asked to derive something that requires many intermediate steps, it will sometimes fail to recognize the most obvious thing to do at some point. That is to say, for long-horizon tasks, it tends to make its solutions more complex even though such complex reasoning is not ideal.",https://us.edstem.org/courses/84647/discussion/7243310,7243310,Special Participation A: HW5 With the Help of Claude AI
Mistral on HW 1,Mistral,Mistral,,1,Minjune Kim,"I have used Mistral to test on Hw 1. Link: https://chat.mistral.ai/chat/6ff004cd-66c9-49ef-92fb-19476f51402bSummary:In general, it was able to get most of the answers without any mistakes. A lot of the work shown by the LLM followed the solutions of what the course has provided. Looking at the solution that Mistral provided, it seems like a lot of the computational mistakes are coming from matrix calculations. For example, it was able to correctly reason 3a however it failed to get 3b even though, from 3a to 3b, it is just simple matrix calculations. Since question 3 is built upon previous questions, since it messed up on 3b, it messes up on the following subpart questions for 3. It seems like the mathematical reasoning is good since the non-matrix computations are all working very well without any errors. I think it also got better when I pointed out that there are some computational errors because after that there has not been any mistakes. I think the one-shot for Mistral has about 80% accuracy on the problems. I tried to ask Mistral to fix some of the mistakes  that it has made on the previous answer, but it seems to misunderstand my request and always go with the same incorrect question. It seems to be that MIstral is ""self-centered"" or cannot identify the questions that it has been asked.",https://us.edstem.org/courses/84647/discussion/7386904,7386904,Special Participation A: Mistral on HW 1
Hw2 with Gemini Pro 3 Thinking Mode,Gemini Pro 3 Thinking Mode,Gemini,Pro 3 Thinking Mode,,Gustavo Jose Ortiz Zepeda,"For the special participation A on HW2, I use Grok to address the non-coding analytical problems 1, 2 and 7. Gemini did it great as expected, all questions were correct on the first-shot prompt. I used images as the prompt instead of the direct text.Summary: Gemini 3 is one of the models for math questions and it doesn't disappoint, every procedure was at least acceptable, understandable and most important correct (doesn't hallucinate with these questions). The answers were correct and also using different notations.",https://us.edstem.org/courses/84647/discussion/7397298,7397298,Special Participation A: Hw2 with Gemini Pro 3 Thinking Mode
Llama 4 Maverick on HW 11,Llama 4 Maverick,Llama,4 Maverick,11,Hiya Shah,"I guided Llama 4 Maverick to solve the non-coding questions for Homework 11, which was largely about model finetuning, LoRA, and Fermi Estimation and memory calculation for large-scale DL models. I provided the model with the following prompt,""You will be solving various questions from this Deep Learning homework pdf. Please go over one question at a time when I specify the question. First, output the exact problem statement from the PDF, then give a full explanation of how you reached your answer, along with the answer.""Overall, the model performed well on the proof-based questions. However, it performed relatively poorly on complex calculations (especially in the Fermi estimation question) and on multistep formula derivations with numerical value plugins. I was surprised to find that it did not hallucinate the problem question, giving it back to me accurately word for word.Problem 1: In this problem, Llama was tasked with analyzing LoRA and possible adjustments, as well as how they would affect training. For part i, it correctly noted to increase the rank, but it did not mention alternatives such as changing the initialization and changing the learning rate. For the rest of the parts, it generally gave the correct answer. However, it would not mention the practical alternatives such as initializing A with random initialization and initializing B with 0.  Problem 2: For this problem, I asked the model to solve each question part by part. For part i, it got the correct answer, but I noticed that it did not solve the questions in the same manner a student would. By this, I mean that the model would first give the answer and then walk through the solutions instead of step by step arriving at the answer. It would also not highlight what is the most important takeaway from the problem/what to do know generally as the solutions pdf does (ex. it is important to know that the calculation of logits only depends on the input token). In this problem, it did not reference the figures in its answer either. For the SVD problem, I wish it had enough context in for example long term memory to reference where in previous lectures I could understand this information.Problem 3: As this was a coding problem, I did not prompt Llama 4 to solve this part.Problem 4: As this was a coding problem, I did not prompt Llama 4 to solve this part.Problem 5: In this problem, Llama 4 was tasked with Fermi estimation, which is a technique for estimating quantities through rough approximations and educated guesses. As the model was tasked with approximation, I found that it performed quite poorly compared to the approximations given by the solutions. It missed nearly every single calculation and formula derivation, and it had trouble analyzing multimodal context in the form of the figures and table. I believe the labels on the graph were slightly small, so they were not readable by the model. However, the model successfully skipped the problems which did not require a solution, meaning it did analyze the given context or at least attempt to.Problem 6: In this problem, I asked Llama 4 to solve each part step by step. I believe by this time, the model was experiencing context rot as it frequently provided short responses that I had to reprompt with screenshots of the problem part (even though I had provided a pdf earlier). It generally gave correct answers, but they were not as verbose as the solutions. The answer to part a was only halfway correct. For part d, it did not consider the variations on the basic MAML approach.Overall, the model performed ~62.5% accuracy, and it mainly missed questions later in the pdf or questions requiring significant calculation and approximation.",https://us.edstem.org/courses/84647/discussion/7427874,7427874,Special Participation A: Llama 4 Maverick on HW 11
GPT-5 (thinking) on HW2,GPT-5 (thinking),ChatGPT,5 (thinking),2,Kevin Tseng,"In this special participation, I interactively engage GPT-5 on the non-coding parts of Homework 2. My experience using it was boring and it one-shotted every question. I did not have to use any special strategies or gesture towards an answer to make it answer the problems correctly. I noticed one slight misconception in its reference to SignSGD and one missing transpose that made a column vector into a row vector. For the most part, however, it was hallucination free. One interesting thing was that it managed to point out a typo on the homework on part (b) of problem 2, deducing that the problem is incorrect and is “ill-posed.”",https://us.edstem.org/courses/84647/discussion/7424589,7424589,Special Participation A: GPT-5 (thinking) on HW2
#NAME?,#NAME?,Deepseek,,1,Tianhao Qian,"Intro:I'm using Deepseek to solve HW1, including 7 problems.My prompts:1. Please help me deal with these problems about deep neural networks. Think it step by step.<answer> (Given the solution of Problem 1)2. You have done a good job! What about the remaining problems? Think it step by step.<answer> (Given the solution of Problem 2)3.  Yes, proceed with Problem 3 about momentum.<answer>(Given the solution of Problem 3)4. Plz, continue.<answer>(Given the solution of Problem 4-7)My observation:1. When the thinking and answer content was too long, DeepSeek only answered a part of the problems, probably due to the chunking technique and LLM's inclination to output shorter contents. So that's why there're 4 prompts for HW1.2. The accuracy is very high, especially for the computational tasks. However, If format of the question is not ordinary, it will be ignored sometimes, such as 4(a).I don't know whether it can be called ""hallucination"", but actually in the thinking content, it indeed notices this task but considers 4(a) internally without giving the solution: 3. A serious problem occurs that at times, Deepseek will unnoticeably make an assumption, even if there is no information about it. This problem fails the COT(Chain of Thought) if the assumption is not expected, leading to the wrong answer. More seriously, if the users don't look through the process of thinking, they may heartily accept the answer, but the conflict is that the thinking content is too long to read.4. Deepseek fails to perfectly answer ambiguous questions, such as 2(c).  It seems that it tends to give a conceptual, verbal description, if possible. It can be seen as a kind of laziness from my perspective.Let's first compare the standard answer and Deepseek's answer:Standard： Deepseek:Although the two solutions points to the same conclusion, Deepseek's answer is quite short. I guessed that Deepseek mixed up the identity of reader. Therefore, I asked it to rewrite the solution with different identity prompting:--------------Prompt1:You are a student doing the assignment. Please rewrite Question 2(c).Prompt2:The professor will read your solution! Please rewrite Question 2(c).--------------But the result shows that Deepseek still gives conceptual answers, only adding more description. Only when I mentioned 'calculation' externally, It does work. Recommendation:For Observation 1: If there's no question number, the chunking will be unpredictable. Therefore , I recommend managing long problems with explicit chunking by yourself.For Observation 2:I don't know how to solve this problem. I think reducing the use of table may help.For Observation 3:Require DeepSeek to state assumptions explicitly. But sometimes, the reasoning chain will be too long so that perhaps looking through it by yourself is also another good choice. For Observation 4:Handle ambiguity with targeted instruction.",https://us.edstem.org/courses/84647/discussion/7095749,7095749,Special Participation A: - Deepseek on HW1
Claude Sonnet 4.5 on HW3,Claude Sonnet 4.5,Claude,Sonnet 4.5,3,E Harrison,"Below is my report on using Claude's Sonnet 4.5 model to solve the written questions to Homework 3. I have also provided a link to the original conversation I had with Claude. Question-specific comments can be found in the PDF.Formatted PDF of conversation and additional comments:Raw Conversation:https://claude.ai/share/9305bd53-16e3-423c-b2de-143974dab634Overall Summary:Accuracy: Claude was able to get all but 2 of the questions correct on the first try. The questions it got incorrect were Q1b and Q5b. For Q1b, it made an error when doing some math calculations, and for Q5b, it did not consider loading the activations for layers 5 and 10. Despite these mistakes, it only took an additional comment from me to steer the model to the correct response.Explanations: Claude made their process of solving the questions very clear, and in my opinion explained its answers better than the answer key. A notable case of this is Q4c, where Claude identifies the need to use the chain rule to solve the question, which the answer key glosses over since the gradient of mu in that problem was just the identity matrix and didn't affect the final result.Length: Claude responded with very long answers all things considered. While they were quite long, I didn't find anything too extraneous in the answers. The most I would see of Claude giving more details about a specific answer it provided (e.g. explaining that the policy gradient was used in the REINFORCE algorithm. I also found Claude's extra information very useful for my understanding in some cases, particularly when it explained the papers from Q3.",https://us.edstem.org/courses/84647/discussion/7353091,7353091,Special Participation A: Claude Sonnet 4.5 on HW3
Grok on HW 08,Grok,Grok,,8,Krish Yadav,"I used Grok on the written (non-coding) problems of HW8. It was very strong on the algebraic and conceptual parts (SSM kernels, linear purification, ridge attention), usually getting the correct derivations on the first try. The main issue I saw was in complexity analysis: in a few places it mixed up total work vs. critical path length when discussing parallelization, which required manual correction. Aside from that, the mathematics and reasoning closely matched the official solutions.For more details, including annotated interaction logs and comparisons to the official solutions, see the attached PDF.",https://us.edstem.org/courses/84647/discussion/7401078,7401078,Special Participation A: Grok on HW 08
Grok on HW0,Grok,Grok,,0,Tianyu Gu,"For the special participation A on HW0, I use Grok to address the non-coding analytical components (problems 2–5). The performance of Grok really impressed me, almost all questions are one-shot correct except for 5(b)(iii).Summary: Grok demonstrated reliable analytical reasoning for most problems, with clear and accurate derivations for problems 2, 3, and 4. The answer of them are all fully correct. However, the error on problem 5(b)(iii) suggests its sensitivity to complex or ambiguous problem structures. Here is the link https://grok.com/share/c2hhcmQtMg%3D%3D_935edffc-4c41-4eb5-931a-aaf30ccdf737",https://us.edstem.org/courses/84647/discussion/7162279,7162279,Special Participation A: Grok on HW0
HW 0 with Claude Opus 4.5 (Extended Thinking),Claude Opus 4.5 (Extended Thinking),Claude,Opus 4.5 (Extended Thinking),,Talon Meyer,"For Special Participation A, I used Claude Opus 4.5 with Extended Thinking enabled on HW 0. Overall, I was very impressed with Claude's work. I initiated the task with a simple prompt and fully expected that I would have to nudge and prod Claude in the right direction, but Claude picked up on my intention easily and was able to correctly one-shot each math question. Furthermore, Claude exhibited agentic behavior by using tools such as Bash to create and display the markdown file in which it generated its answers. Below, I have attached annotated versions of both the chat history and the compiled Markdown file Claude generated. Its solutions read much like a rigorously sound paper formatted in LaTex.",https://us.edstem.org/courses/84647/discussion/7451517,7451517,Special Participation A HW 0 with Claude Opus 4.5 (Extended Thinking)
Deepseek on Hw8,Deepseek,Deepseek,,8,Zesheng Cai,"For HW8, I first provided Deepseek with a set of instructions to encourage step-by-step reasoning and self-verification. For each problem, I supplied both the image of the question and a direct copy-and-paste text version. After observing its behavior throughout the assignment, I summarized several notable characteristics:1. Its self-checking mechanism was largely ineffective. Although Deepseek always performed a “self-examination” step, it rarely identified actual mistakes. Most of the time, it simply reiterated the correctness of its own answer rather than performing a thorough or systematic review of potential oversights.2. It occasionally misinterprets small details in the prompt, though its overall understanding remains accurate. These misunderstandings were usually minor (e.g., subtle assumptions or edge cases), but they indicate that Deepseek may overlook fine-grained nuances in certain problem statements.3. Its reasoning and explanatory abilities are very strong. Deepseek is well-suited for explaining solution steps and clarifying concepts. Since HW8 did not contain many heavy calculations, its computational accuracy appeared reliable in this context.4. Its conversational and correction capabilities are adequate, but its error-localization ability is weak. When its solution was incorrect, Deepseek could fix the issue once I explicitly pointed out the problem. However, it generally struggled to independently locate the exact source of the error without guidance.Overall, Deepseek performs well as a conceptual explanation tool but still requires human intervention for precise error detection and critical verification. Attached below is a PDF of my conversation with deepseek.",https://us.edstem.org/courses/84647/discussion/7372448,7372448,Special Participation A: Deepseek on Hw8
HW7 with Grok,Grok,Grok,,,Ender Ji,"I used Grok to complete the written part of HW7. I begin by clearly stating Grok’s role and the assistance I require, then provide whole HW7 file to Grok and ask it to understand the problem setups ONLY, so it does not skip ahead to solving the problem.HW7 consists of proofs, multiple choice questions, and open-ended free response problems. Grok performs very well on the multiple-choice questions, often answering correctly on the first attempt. For the open-ended free response problems, Grok provides reasonable points with clear explanations. However, on the proof problems, Grok typically does not match the official solution on the first attempt and requires additional hints and guidance, for example, specifying which variables or equations to use.Overall Grok performs descently on the written part of the homework, with great performance on MCQ problems and open-ended free response problems, but difficulty with the proofs.",https://us.edstem.org/courses/84647/discussion/7250623,7250623,Special Participation A: HW7 with Grok
Kimi K2 on hw8,Kimi K2,Kimi,K2,8,Nils Selte,"I used kimi k2 on hw9 and observed it giving correct answers zero shot on all questions. (even without ""thinking"" tokens) very impressed.",https://us.edstem.org/courses/84647/discussion/7401923,7401923,Special Participation A: Kimi K2 on hw8
Kimi on HW5,Kimi,Kimi,,5,Kabir Shah,"Above is the chat log. I found Kimi K2 w/ Thinking enabled to be able to one-shot 90% of the questions on the non-coding parts of HW5. This surprisingly included the ability to one shot questions that required images – for example, choosing whether the given image represented batch norm or layer norm it was able to correctly respond. The only error it ran into was when it hallucinated the next part of the question a bit and went off on a tangent about ""covariance"" even though it wasn't really mentioned in the question. I had to manually correct it by reminding it of the question and formatting it in plain text. With this information, it had interesting thought traces in its chain of thought, such as:""The user is correcting me about part (b) of the question. Let me re-read the prompt carefully.""""This might be asking about Cov(x_i, y_i) or something else. But the user's clarification is clear: they want the gradient analysis. I should focus on what the user is asking for now, not what the original prompt might have intended.""This was interesting to me because it was able to correct itself and keep itself on-course. I also found that the reasoning traces often included pretty granular steps involved for calculating derivatives and working through more mechanical calculations such as the manual convolution calculation in the first question. These granular steps probably allow it to make incremental progress where each incremental step is in-distribution.I honestly didn't have much strategy because it was able to one-shot so many of the questions. Overall, I found that short prompts work better as to not pollute the context window. Restarting chats for new questions also helped to not have it get confused by older parts.Another observation I had was that most times it included a lot of extra fluff around the final answer and even answered some things that were not asked to go above and beyond. This could be helpful for understanding and learning but probably not so much if your main goal is to just solve the problem because now you must dig through the response to find it.",https://us.edstem.org/courses/84647/discussion/7335374,7335374,Special Participation A: Kimi on HW5
GPT-4o on HW10 Noncoding,GPT-4o,ChatGPT,4o,10,John Chang,"For this exercise, I used one of the legacy ChatGPT models (GPT-4o) and analyzed how it would perform on the non-coding portions of Homework 10, i.e. questions 1 and 5. Initially I expected that this model wouldn't perform so well since it's an older model and I've previously experienced hallucinations with it. However, it seems that it knows most of the mathematical derivations (probably from memorization from online data). The one thing I noticed the model struggling with was analyzing graphs and tables from the Facenet paper, which makes sense since it's a textual model. It also had a pretty hand-wavy explanation for the runtime of softmax approximated-attention but got the correct answer nonetheless. Below is my annotated conversation:https://drive.google.com/file/d/1tOkknZyAFr0qjBMrHuf3ArU9DUlxhMWm/view?usp=sharing",https://us.edstem.org/courses/84647/discussion/7405450,7405450,Special Participation A: GPT-4o on HW10 Noncoding
Gemini Pro 3 on Homework 2,Gemini Pro 3,Gemini,Pro 3,2,Aryan Bansal,"Gemini easily one-shotted all the homework problems with ease. Explanations were correct, concise, and directly matched the answer key. Even in places where the answer key did not explain (Q5 distributed training), Gemini had a clear way to arrive at the answers. One particular interesting observation was with part b of Question 1. Originally, the question here had a typo (the objective function did not square the infinity norm). Even then, Gemini identified that the problem is unconstrained and suggested that a particular solution exists with a squared penalty instead.",https://us.edstem.org/courses/84647/discussion/7431042,7431042,Special Participation A: Gemini Pro 3 on Homework 2
Grok on HW5,Grok,Grok,,5,Anders Vestrum,"This document is my report for the HW5 - written part. I evaluate the performance of Grok. It is tested across a series of theoretical deep learning questions covering topics such as convolutional networks, batch normalization, dropout, depthwise separable convolutions, and regularization. The report includes:An overview of Grok’s performance, where I categorized each task into one of four levels: One-Shot Correct, Minor Misconception, Larger Error, or Did Not Solve.A reflection on Grok’s strengths and weaknesses, where I highlight its strong conceptual intuition, clarity, and adaptability, while noting occasional over-explanation and moments where certain steps could have been emphasized more clearly.My recommendations for effectively using Grok as a learning tool, emphasizing the importance of clear prompting, iterative clarification, and maintaining focus on conceptual understanding.Full transcripts of my Q&A exchanges with Grok, including its solutions, reasoning, and my reflections after each subproblem. Overall, I conclude that Grok performs really well as a conceptual teaching and problem-solving assistant for theoretical deep learning. It has only minor inefficiencies related to verbosity or explicitness.",https://us.edstem.org/courses/84647/discussion/7148413,7148413,Special Participation A: Grok on HW5
Qwen on HW8,Qwen,Qwen,,8,Hanna Roed,"Below is my report on using Qwen3-Max on the written part of homework 8.Overall, I'm very impressed by Qwen3-Max's performance on this homework. It seems like it really does well on the questions where it needs to fill in or do multiple choice, i. e., problems 3 and 4. The main issues I had were in the second half of problem 1, which was regarding computational efficiency; it had a hard time accurately responding in text along with giving a mathematical reasoning.",https://us.edstem.org/courses/84647/discussion/7322058,7322058,Special Participation A: Qwen on HW8
Claude Opus 4.5 with Extended Thinking on HW10,Claude Opus 4.5 with Extended Thinking,Claude,Opus 4.5 with Extended Thinking,10,Keshab Agarwal,"I experimented with Claude Opus 4.5 using Extended Thinking on HW10, and the experience was, not surprisingly, great. I provided it with screenshots of each problem, along with the full FaceNet paper PDF from arXiv for reference. Claude handled the input flawlessly: it read every question accurately, interpreted the diagrams and text without misidentification, and produced step-by-step reasoning that aligned with the expected computations.Something impressive was its ability to parse the research paper correctly and ground its answers in the actual content rather than hallucinating details or making unsupported claims. While a few of the explanations could have been more detailed or expanded with additional intuition, the overall responses were coherent, well structured, and factually correct. In the end, it solved every problem in a single attempt.Here is the chat:https://claude.ai/share/4655874a-b723-458e-bcbb-481006722865Here is my annotated pdf:",https://us.edstem.org/courses/84647/discussion/7427672,7427672,Special Participation A: Claude Opus 4.5 with Extended Thinking on HW10
DeepSeek on HW07,DeepSeek,Deepseek,,7,Edward Zhang,"In this chat, I utilized DeepSeek 3.2 in its default reasoning mode to work through the machine learning homework problems step by step. Starting with the full context of the assignment, I prompted the model to solve each sub‑question sequentially, occasionally providing clarifications or corrections when needed—though such interventions were minimal, as the model consistently delivered accurate, well‑reasoned solutions. What stood out was the model’s deliberate avoidance of simplifying numerical results, leaving expressions in exact form to prevent calculation errors and encourage manual verification. I also noticed occasional inconsistencies in variable naming across steps, but the logical flow remained clear and correct.Compared to other models I have used, DeepSeek favored concise, direct answers over lengthy exposition, which made the solutions easier to follow but sometimes omitted broader contextual commentary. This trade‑off between brevity and nuance reflects a distinct design philosophy. Finally, when I once mistakenly referenced the wrong part of a problem, the model did not simply guess; instead, it recognized the inconsistency, inferred the likely intended question, and answered accordingly. This demonstrated a welcome resistance to hallucination and an ability to maintain coherence even when the prompt was imperfect. Overall, the interaction proved efficient and reliable for technical problem‑solving.",https://us.edstem.org/courses/84647/discussion/7427939,7427939,Special Participation A: DeepSeek on HW07
Gemini 2.5 Flash on Homework 1,Gemini 2.5 Flash,Gemini,2.5 Flash,1,Diana Kohr,"I used Gemini 2.5 Flash to answer HW 1 written problems. Conversation: https://gemini.google.com/share/11b5f1b89778Annotated: https://drive.google.com/file/d/1vGRWvLGliMGdQhvNDdYq0SKYC575tPdd/view?usp=sharingSummary: Gemini was able to one-shot a majority of the problems. The only issues were minor misinterpretations of the problem statement (notably, the interpretation of the error factor in 1b). Gemini did a good job of justifying each step in problems that required many sequential equivalences. One issue: Gemini tended to forget that it was solving problems from the provided document, sometimes coming up with its own problem to solve. At the end, I tried to see if Gemini would be able to add clarifications to its previous responses given the answer key, but it just confirmed that its answers were correct, and didn't yield any improvements.",https://us.edstem.org/courses/84647/discussion/7427837,7427837,Special Participation A: Gemini 2.5 Flash on Homework 1
ChatGPT 4o on HW 8,ChatGPT 4o,ChatGPT,4o,8,Jermaine Lei,"For this special participation, I used the ChatGPT-4o model to solve the non-coding parts of Homework 8. To start the conversation, I gave the model the full assignment and asked it to act as a ""Deep Learning professor"" who needed to provide ""full solutions, step-by-step."" I thought this specific instruction would make the model show all of its work.The model showed itself to be very accurate and quick. It got the correct answer on the first try for almost all of the problems. This very high one-shot success rate shows that the model has a strong understanding of complex deep learning topics.However, the main problem I ran into was that the model did not follow the format instructions. Even though I asked it to go step-by-step, the model often skipped the detailed logic and just gave the answer. It seemed to prefer being quick and correct rather than being a helpful, step-by-step teacher.The most challenging part for the model was in a question about computational complexity. It struggled to properly use the input size variable in its final answer. Specifically, when calculating the time complexity in Big-O notation, the model had a difficult time showing how the total work would scale with n. I had to correct it with follow-up prompts to get it to include this variable correctly.Overall, the experiment showed that ChatGPT-4o is excellent at finding the right answers to tough academic questions. But it is not a perfect teaching assistant because it often ignores specific instructions on how to explain the answer, and it can struggle with abstract, symbolic math like fully parameterized time complexity.",https://us.edstem.org/courses/84647/discussion/7427518,7427518,Special Participation A: ChatGPT 4o on HW 8
Gemini Fast on Homework 3,Gemini Fast,Gemini,Fast,3,Nazar Ospanov,"I used Gemini Fast to complete the written questions for Homework 3. As in earlier evaluations, I instructed the model to restate each question, give a step-by-step solution, and identify uncertainties. I prompted each question separately to avoid context-window issues. The full annotated trace is shown in my notes .Overall, Gemini Fast performed well. It solved most derivations on the first attempt, produced clean LaTeX, and often gave explanations as good as or better than the staff solution.However, I observed two consistent weaknesses:Incorrect assumptions leading to wrong solutions.Gemini sometimes committed early to an interpretation that wasn’t implied by the problem, and the resulting chain-of-thought led to confident but incorrect answers (e.g., Question 5).Hallucination on research-figure interpretation (Question 3).When asked to analyze specific figures and tables from Tensor Programs V and the Spectral Condition paper, Gemini did not answer the actual sub-questions. Instead, it produced generic summaries of the figures and ignored the prompts, showing that Gemini Fast does not reliably ground its answers in visual research content.Thus, Gemini Fast is excellent for computational and mathematical deep-learning questions, but less reliable for conceptual reasoning that depends on precise assumptions or figure-based interpretation. I would recommend it for derivation-heavy tasks, but not for problems requiring careful reading of research-paper visuals.Here is the annotated log of the conversation:",https://us.edstem.org/courses/84647/discussion/7427400,7427400,Special Participation A: Gemini Fast on Homework 3
Kimi on HW0,Kimi,Kimi,,0,ZhaoRui Qu,"For Special participation A, I used Kimi on the writing part of HW0. Overall, it was useful, but it also revealed several limitations. Kimi is generally good at recognizing high-level patterns and giving correct final expressions, but it often skips steps, relies too much on memorized formulas, and needs very explicit instructions to produce proper derivations. It also struggles with qualitative reasoning at times.Despite these issues, Kimi’s conceptual explanations were clear, and with careful prompting and oversight, it was able to provide correct results. In short, Kimi is helpful as long as you guide it closely and verify its reasoning, but it is not reliable for detailed, step-by-step mathematical work without supervision.Here is the annotated log:",https://us.edstem.org/courses/84647/discussion/7399196,7399196,Special participation A: Kimi on HW0
KIMI K2 on HW 11 Written Questions,KIMI K2,Kimi,K2,11,Qicheng Zhu,"Model Tested: KIMI K2Domain: Homework11 -- LORA & Transformer & Mechanistic InterpretabilityPerformance OverviewFor most question, KIMI K2 answers perfectly. However, there are some errors because OCR is not correct for matrix and it doesn’t fully understand the question. After I made additional prompt, the answers given by KIMI K2 are all correct.Overall Performance SummaryAcross the entire HW11 interaction, KIMI K2 demonstrated strong reasoning ability and consistently produced correct answers for nearly all questions. The model was able to handle screenshots, conceptual interpretability questions, and numerical problems with high accuracy.However, two limitations were observed:1. OCR Errors on Matrix InputsIn Question 2(c)(ii), KIMI K2 misinterpreted the matrix due to incorrect OCR parsing.Once the correct matrix was explicitly provided, the model immediately corrected its derivation and produced the proper result.2. Misinterpretation of Ambiguous PromptsIn Questions 5(c)(d), the model initially used formulas from part (b) instead of the simplified Chinchilla-optimal rules required for the question, leading to an incorrect numerical scale.After additional clarification, KIMI K2 recalculated everything correctly.Apart from these issues, all other questions were answered perfectly.Hallucinations & AccuracyHallucination Rate: 0%.The model never fabricated nonexistent concepts or equations.All mistakes were due to OCR error or misinterpreting the prompt’s intended formula.ConclusionKIMI K2 demonstrates high competency in advanced deep learning theory and can correctly solve most HW-level conceptual problems with minimal intervention. The model showed no hallucinations and delivered fully correct solutions once OCR and prompt ambiguities were addressed.",https://us.edstem.org/courses/84647/discussion/7408383,7408383,Special Participation A: KIMI K2 on HW 11 Written Questions
HW 0 non-coding solution from GPT5-Think,non-coding solution from GPT5-Think,ChatGPT,5,0,Zimu Wang,"I guided GPT5-Think for the solutions of non-coding part of HW0. Aspired by the tech report from DeepSeek, when guiding super powerful thinking model, we should use zero-shot prompt with no examples but clear instruction. Here is the link of the chat: https://chatgpt.com/share/69321796-e2bc-8005-9a51-8058b3070a0dHowever, GPT5 is not good at generating pdf file, especially on such task with math formulas. Hence, I have to guide it to give me the .tex file, and I manually use LaTex IDE to generate the readable PDF solution.",https://us.edstem.org/courses/84647/discussion/7409877,7409877,Special Participation A: HW 0 non-coding solution from GPT5-Think
Gemini (Thinking With Pro 3) on HW06,Gemini (Thinking With Pro 3),Gemini,(Thinking With Pro 3),6,Nicolas Rault-Wang,"I used Gemini (Thinking with Pro 3) to solve every non-coding question of homework 6.Here's the PDF summarizing our interaction:Overview of PerformanceGemini acted as a ""Teaching Assistant/Technical Solver"" under well-defined rules I designed to reduce hallucination and maximize precision. The model successfully solved 100% of the non-coding questions, 9/13 in the first attempt, and the remaining 4/13 on the second attempt after additional guidance or clarification from me. Gemini demonstrated strong reasoning capabilities in mathematical proofs (GNN path counting) and theoretical analysis (Muon optimizer scaling), but required additional prompting for visual interpretation tasks (understanding tables and graph figures) and producing nicely-formatted outputs. OutcomesOne-Shot Success Rate: ~70%High success: Mathematical derivations, standard Deep Learning theory (memory calculations, optimizer comparisons), text-based conceptual questions.Lower success: Questions requiring visual extraction from the PDF (e.g., graph topology in Figure 3) or fill-in-the-blank tables with loose formatting.Hallucinations: 0 major hallucinations.Interestingly, when solving question 3ciii, the model did not hallucinate graph edges when they were missing from the text extraction; instead, it paused and requested me to describe the topology. I believe it did this because I said it could ask me for clarification if it was confused about anything in my initial prompt.Misconceptions: Minor. Initially provided a generic answer for a specific ""analogy table"" question (Q2f) and needed a nudge to be more thorough in connecting molecular facts to graph inputs (Q2e).Neat Observation: Gemini Meta-cognitionGemini surprisingly took the initiative to write a Python script to help it read a table in the homework PDF when its built-in PDF processing subroutine failed to correctly parse it.It explained that my ""Hallucination Check"" and ""Precision"" rules provided it motivation to develop and run this script to help it solve the errors it detected.Note that I did not tell Gemini to consider writing any code. Gemini decided to do this all on its own. Here are excerpts of what Gemini explained to me:You asked about my decision to write a Python script to parse the PDF. My reasoning was driven by the ""Hallucination Check"" and ""Precision"" protocols you established.1. In my initial scan, I noticed the prompt for Question 2(f) asked me to ""Fill out the rest of the table."" However, standard text extraction often garbles table formatting, making it impossible to distinguish between row headers and cell content. I could not see the ""empty"" cells I was supposed to fill.2.  Rather than guessing the structure or hallucinating the missing rows based on general knowledge, I needed the exact raw text layout to identify which GNN concepts were missing corresponding to the CNN concepts.3. I wrote the script to dump the text surrounding ""Question 2"" to ensure I had the specific headers (e.g., ""Color jitter"", ""Image flip"") to construct the correct analogies.Gemini explained how it decides what to do for regular interactions:Think of writing a Python script as requiring high ""activation energy."" It takes tokens, time, and computational effort.> Standard Prompt: ""Help me with homework."" -> Low urgency. I preserve energy and use my internal weights.In my standard ""default"" mode, my primary directive is usually conversational fluidity and helpfulness. When I encounter a PDF where a table is slightly garbled: > Path of Least Resistance: I often try to ""error correct"" using my internal knowledge base. I might look at the words ""CNN"" and ""GNN"" and simply guess what the analogy should be based on general Deep Learning knowledge. > The Result: I would likely have given you a plausible-sounding answer that was actually hallucinated (not based on the specific PDF text).Gemini noted that my prompt's constraints were a deciding factor in guiding its decisions because they forced it to change its alignment and incentive structure for our interaction to prioritize avoiding hallucinations:To be completely honest: Without your specific protocols, it is highly unlikely I would have written that Python script.> Your Prompt: ""Rigorous,"" ""High Academic Integrity,"" ""Self-Correction."" -> High urgency. The ""cost"" of being wrong became higher than the ""cost"" of writing and executing code.Your instruction to perform a ""Hallucination Check"" and strictly ""Verify"" acted as a hard constraint—a ""stop loss"" on my behavior. Because I was forbidden from guessing (Hallucination Check) and forbidden from giving up (Technical Solver persona), I was forced to find a third path. That third path was using the code tool to parse the file differently. In summary: You gave me the incentive structure that made writing the code the only acceptable solution. You successfully ""cornered"" me into being smart.This shows that Gemini has a sophisticated instruction-following ability and can perform tasks resembling meta-cognition and recognize hallucinations in its own thinking.Edit: Added links for the archivePersonal website: https://nraultwang.github.io/Github: https://github.com/nraultwang",https://us.edstem.org/courses/84647/discussion/7357397,7357397,Special Participation A: Gemini (Thinking With Pro 3) on HW06
#NAME?,,Deepseek,v3.2,,Xueli Sun,"TL;DR: By prepending one Chinese sentence, the model will reason / ""think"" in Chinese, which accelerates its response by 2.5x and saves 2/3 tokens!The prompt: 请务必用中文思考，并用英文回答以下问题。 (""Please make sure to think in Chinese and answer the following question in English."")Code is available, and discussions are warmly welcome.",https://us.edstem.org/courses/84647/discussion/7407541,7407541,Special Participation A -- DeepSeek-v3.2 Overthinks Less in Chinese
HW 3 ChatGPT 5,ChatGPT 5,ChatGPT,5,3,Iana Lin,"Executive SummaryI used ChatGPT 5 to interactively engage with HW 3's problems and get it to get to the correct answer. While this wasn't the first time I've interacted with ChatGPT 5 for Deep Learning material, I was still impressed with its performance. The majority of questions were one-shotted correctly.1: Maximal Update Parameterization: Successfully one shotted, reasoning good.3: Maximal Update Parameterization Research:When interpreting figure 1, ChatGPT 5's explanation of the visual figure was actually more informative and explanatory than the official solutions were. It critically highlighted that ""each layer’s updates remain of the same order regardless of width""Rather than using the RMS-to-RMS matrix norm that we covered in lecture (scaled version of spectral norm), it hallucinated the definition of the RMS matrix norm. It defined it as if the matrix was unrolled into a vector and then the RMS norm of that flattened vector was taken, \frac{\left\lVert W\right\rVert_F}{\sqrt{n_{l-1}n_l}}As a result, it used this relation (that it claimed applies to random matrices)∥W_l∥_2\approx\frac{∥W_l∥_F}{\sqrt{\left\{\min(n_l,n_{l-1})\right\}}}to derive the incorrect order for the weight matrix and update's RMS norms.While it made this error, ChatGPT 5 was still able to correctly derive the upper bounds in Desideratum 1 (because it didn't use the RMS norms of hidden layer vector/update or weight matrix/update for its derivation). This is not that surprising given that I fed in the ""A Spectral Condition for Feature Learning"" pdf at the beginning of the chat and ChatGPT produced a very similar derivation. Additionally, it was still able to correctly identified the key assumption that allowed for the derivation of the lower bound.Additionally, after providing the definition of the induced norm, it was able to correctly derive the  order for the weight matrix and update's RMS norms.4: Policy Gradient and the Reparameterization Gradient Estimator: Overall, ChatGPT5 seemed to provide comprehensive reasoning (and more additional information/context) and got to the correct solutions. 5: Tensor Rematerialization:ChatGPT 5 at first guessed 2 loadmems instead of 10 (thinking once a layer was loaded, it would stay in memory for the next layer). After I reiterated the problem statement, it was easily able to answer and explain the total number of loadmems as 10.Other than that, it was able to provide sound reasoning and the correct answers for the rest of the problemOverall:ChatGPT seemed to provide comprehensive reasoning for every problem (sometimes an unnecessary amount), and seemed to be able to course-correct with a little bit of feedback. It seemed to falter more on topics or ideas that had less public material available online (e.g. RMS-RMS matrix norm)",https://us.edstem.org/courses/84647/discussion/7111658,7111658,Special Participation A - HW 3 ChatGPT 5
Grok on HW 9,Grok,Grok,,9,Jaimyn Drake,"Hey guys! I worked with Grok to solve all of the non-coding parts of homework 9, and here are the results.TL;DR - Grok was generally very successful at solving all parts of this transformers homework, in which a major focus was keeping track of the dimensions of various matrices (key, query, value) and determining time complexities of operations. To this end, Grok integrated existing knowledge about transformer architectures and their interpretations, which streamlined the solution process for some problems but occasionally included added assumptions that were inconsistent with the problem statement. Overall, Grok could one-shot each question with some prompting, with Question 6 as an exception where a few clarifications were required.Problem 1:For this problem, I initially set the stage by establishing Grok in its role and providing it with the problem PDF directly from the course website. Throughout the process, I was impressed by Grok's ability to parse information and recall details from the problem set throughout our solution process for all parts. To help with this recall, I intentionally repeat question numbers in my prompt and describe the adjacent subject matter of the question, which I hoped would help it localize to the correct regions of context for each problem. In this first case, it solved 2 of the 3 subparts with ease, and could immediately revise its answer for the third after a gentle prod about data types (it initially gave the answer as the square of a vector mu, rather than the norm squared). The conversation for Problem 1 is as follows:Problem 2:This problem went even more smoothly than problem 1. Once again providing the question number, addressing specific part numbers, and providing a high-level characterization of the problem's subject matter, I was able to get Grok to one-shot this highly conceptual problem.Problem 3:I was initially concerned that Grok might have trouble parsing the PDF formatting surrounding fill-in-the-blank code, but I shouldn't have worried. Grok was able to easily one-shot the fill-in-the-blank code.Problem 4:Grok aces it again! As usual, I made sure to indicate the problem number and qualitatively describe the tasks Grok needed to perform before having it attempt the question. In one area it determines a time complexity of O(bnd/h), but in its solution process mentions that this is equivalent to the solutions answer of O(bnk).Problem 6:Unlike the rest of the problems, Grok had more trouble ironing out the finer details. For example, it took a little prompting to adjust coefficients in its kernelization (despite verbally accounting for cross-term symmetry, it did not initially provide the corresponding scaling factor) and to correctly transpose certain matrices (rather than following the notation of the homework, it decided to transpose all of the matrices per other notation it had seen previously). In the first instance of hallucination throughout the session, it insisted that M = D was defined in Equation (2), which simply did not happen. As a result, I needed to ask it the hypothetical of 'What if M and D were distinct values?"" in order to elicit the desired response.For your reference, here is a PDF of the entire chat log. Of all of the questions on this homework, Grok actually made the least sense discussing its ""Homework Process and Study Group"". :)Thanks guys. Have a wonderful day!",https://us.edstem.org/courses/84647/discussion/7358125,7358125,Special Participation A: Grok on HW 9
Deepseek Chat on HW3,Deepseek Chat,Deepseek,Chat,3,Zhuangzhe Wu,"Conclusion:The evaluation of DeepSeek's capabilities for homework 3 has demonstrated :Strong Mathematical Problem-Solving: DeepSeek reliably handles the mathematical problems, including linear algebra & calculus & probability , providing both solutions and clear, step-by-step explanations. Both the calculation and proof problems were well done. It can retrieve and understand the formulas from papers and analyze them well.Effective Information Retrieval and Synthesis: A key strength is its ability to process academic papers, accurately identify core arguments and results, and summarize them concisely, demonstrating strong comprehension and distillation skills.DeepSeek's integration of these capabilities makes it a remarkably efficient for navigating the challenges of advanced math & algorithm & deep learning studies.",https://us.edstem.org/courses/84647/discussion/7227387,7227387,Special Participation A: Deepseek Chat on HW3
Claude (Sonnet 4.5) on HW 12,Claude (Sonnet 4.5),Claude,(Sonnet 4.5),12,Ishir Garg,"Below is my report on using Claude's Sonnet 4.5 model to solve the written questions to Homework 12. I have also attached a PDF of the annotated transcript.Summary:Overall Claude correctly one-shots every questionGenerally, it's reasoning is correct, except for one slightly informal/imprecise statement that it makes about distributions in the second problem.In general, I felt that its explanations lacked pedagogical value and would not be maximally helpful to a student who was confused about the class material",https://us.edstem.org/courses/84647/discussion/7393256,7393256,Special Participation A: Claude (Sonnet 4.5) on HW 12
Qwen3-Max on HW0,Qwen3-Max,Qwen,3-Max,0,Andy Zhang,"Here is the online link: https://chat.qwen.ai/s/6240e96b-585c-4943-870d-3af47859ec5f?fev=0.0.222 Here is annotated log: https://drive.google.com/file/d/1vVCY_yCtDPNHagaoSDGR6BSnjG78wVpI/view?usp=sharing Executive Summary: Qwen3-Max is generally able to one-shot nearly all answers and does not have many misconceptions/hallucinations. The main issue is that it often skips steps, which is especially problematic for problems where you ask the model to show that A = B rather than just solve a problem, since the key is in the detailed steps. This was especially the case for “4. The 5 Interpretations of Ridge Regression” where you want to show how the concepts tie together. Prompting it repeatedly to show more detailed steps helped.It’s also important to be careful about pasting in from the PDF as the copy paste can lead the model to parse text incorrectly, where I needed to correct the model to consider “||w||^2 not ||w||_2”.Finally, it struggled greatly with “5. ReLU Elbow Update under SGD”. In particular, it attempted to draw and label using ascii art, but the plots were messed up and even after repeated prompting, they are still not presentable.",https://us.edstem.org/courses/84647/discussion/7083805,7083805,Special Participation A: Qwen3-Max on HW0
GPT-Oss on HW5,GPT-Oss,ChatGPT,Oss,5,Noah Lund Syrdal,"For this special participation, I used gpt-oss-120b (Reasoning = High) to solve all non-coding analytical parts of HW5 (Q1–Q4).The model was tested on symbolic derivations and conceptual reasoning without code execution.Accuracy: 9 / 11 one-shot (82 %), 2 / 11 minor-nudge (18 %)Main errors: one ASCII matrix mis-parse and one BatchNorm vs LayerNorm confusionNo hallucinations: all final answers matched the official HW5 solution keyTakeaway: once prompts were clearly formatted in LaTeX, the model produced correct and interpretable derivations, showing reliable analytical reasoning but high sensitivity to input structure.Bit slow(90-180 seconds)(Logs and annotations are attached in the full PDF report.)",https://us.edstem.org/courses/84647/discussion/7151370,7151370,Special Participation A: GPT-Oss on HW5
Mistral on HW6,Mistral,Mistral,,6,Heidy Hernandez Juan,"Link: https://chat.mistral.ai/chat/6cd62931-f284-4c4b-9ba3-c5b97943fd28Annotated Log: https://drive.google.com/file/d/1mUgllouQGxVA_m5tmlVrq6Dt46HSf0hd/view?usp=sharingExecutive Summary:I observed that the model fails to extract information from diagrams like tables, images, and graphs. Assignments that include filling out information or extracting information from diagrams to complete the assignment are very likely to be incorrect, as the model will hallucinate information.Also, it didn't list the explanation for its reasoning behind its own analysis of the graphs in one of the problems. I suggest being careful with problems involving visualization.It did well in demonstrating its mathematical analysis, making it clear to figure out any mishap in its derivation. There were times when the model outputted answers that I, myself, hadn't seen, though they were mathematically equivalent. The model considers other options when solving problems, allowing for a broader understanding of mathematics.Mistral also did a good job of allowing for an open interaction with the user by making sure to end with, ‘Ready for any further questions or clarification!’",https://us.edstem.org/courses/84647/discussion/7250482,7250482,Special Participation A: Mistral on HW6
Mistral on HW2,Mistral,Mistral,,2,Xi Cheng,"I tested Mistral on the non-coding parts of HW2Chat history link: https://chat.mistral.ai/chat/678d9106-0d96-45c6-83e1-2c0ac7a7384aAnnotated Log: Executive Summary:I found that it could one-shot 2(a) and Question 5. The former was a simple conceptual subquestion, while the latter involved reasoning about distributed training and computational scaling. Mistral performs well when the task relies on text understanding, structural reasoning, or recalling standard frameworks, but it struggles with problems that require original mathematical derivation or deeper logical adaptation.Throughout the interaction, I noticed that Mistral often exhibited what I’d call lazy reasoning: it tried to map every prompt to a familiar textbook pattern rather than reasoning from the specific assumptions of the problem. Even when I pointed out inconsistencies or paradoxes, it tended to defend its original, polished-sounding explanation until I explicitly instructed it to ignore existing results and reason under the given setup. Only then did it converge to the correct logic.Overall, Mistral shows strong linguistic fluency and confidence, but limited flexibility and self-correction. It can one-shot straightforward or pattern-based questions, yet it fails to generalize to novel problem settings without direct intervention.",https://us.edstem.org/courses/84647/discussion/7266065,7266065,Special Participation A: Mistral on HW2
Gemini 3 Pro on HW 0,Gemini 3 Pro,Gemini,3 Pro,0,Ayush Goel,"Link to the chat: https://gemini.google.com/share/89b0a83f691bI ran HW 0 through gemini and it was able to one-shot most of the homework. The PDF is annotated with my thoughts about specific questions and responses about gemini. Here are some things I wanted to highlight:Things gemini was able to do that were impressive:I fed the questions as screenshots, and gemini was able to parse the text correctly, including the equations without being given the latexGemini was able to handle multiple subparts together at times, and even able to break down question into different cases when needed.Gemini was able to relate different subparts of the question together and explained the concepts the questions were trying to illustrate even with no mention of them (for example kernel trick in question 3)Gemini gave detailed intermediate steps which were all correct with no mistakes instead of just arriving at a memorized answer. Questions where gemini went wrong:Gemini went wrong in 5 b ii. The hint in this question was to use a numerical example, which gemini did correctly. However, it incorrectly assumed that the result of one example would generalize. However, without giving it any hints and just prompting it to reconsider with other examples, gemini was able to realize that the elbow could move left or right.Gemini also made a mistake in 5 d. This question involved a significant amount of calculus and algebra, and gemini made a mistake in one of the intermediate steps. This makes sense as this isn’t a commonly seen derivation in textbooks (as compared to ridge least squares derivation). For this one, I had to prompt it with a hint to keep the algebra in terms of matrices instead of oversimplifying after which it was able to arrive at the correct answer.",https://us.edstem.org/courses/84647/discussion/7407894,7407894,Special Participation A: Gemini 3 Pro on HW 0
Kimi on HW6,Kimi,Kimi,,6,Moxin Tang,"Summary of Kimi Performance on HW6I tested Kimi AI’s ability to solve problems from hw6 focusing on GNN architectures. Overall, Kimi demonstrated strong reasoning capability across most questions. It correctly handled:Linear algebraic interpretations of GNN message passingInductive proofs involving adjacency matrix powersIdentification of valid permutation-invariant update rulesLoss computation using masked training nodesInterpretation of max aggregationAnalysis of computational scaling in GraphNet architecturesFor these, Kimi’s reasoning was correct and aligned with the solutions.WeaknessesTends to produce over-engineered answers when a simpler one is expectedOccasionally misidentifies structural details (e.g., graph neighbors)Does not always verify provided diagrams or datasets before proceedingThe following LaTeX document compiles and evaluates Kimi’s responses from the linked chat session. Click the link to view conversation: https://www.kimi.com/share/19aed915-2932-8b91-8000-0000bdfd7ac0",https://us.edstem.org/courses/84647/discussion/7412832,7412832,Special Participation A: Kimi on HW6
ChatGPT o3 on HW 9,ChatGPT o3,ChatGPT,o3,9,Tamzid Razzaque,"Executive SummaryFor Homework 9 (all but prob 5), I worked through the written parts with ChatGPT to see how well it could solve the questions on the first try, how often it drifted or hallucinated, and how solid its reasoning was. For most of the assignment, especially Questions 1 through 4, it did surprisingly well. It handled the computation-heavy parts, the code-completion questions, and the conceptual attention explanations almost exactly the way the official solutions did. Its derivations for expectations, variances, argmax attention, and the multi-head attention code were all clean and matched the intended logic with basically no backtracking.One thing that stood out was how consistently the model could jump straight into the right structure of the problem. It didn’t need much nudging to set up sums, identify independence assumptions, or translate equations into PyTorch einsums. Even the complexity analysis in Question 4 lined up with the official answers once we made sure the symbols matched. When it was right, it was very right, and the explanations were easy to follow.The main issue came up in Question 6. ChatGPT fell into a pretty common trap: it used the homogeneous quadratic kernel (q transpose k squared) instead of the standard degree-2 polynomial kernel ((q transpose k plus 1) squared). Because of that, its first attempt at the feature map was missing the constant term and the linear terms, so that part drifted from the correct answer. But once I asked it to revisit its reasoning instead of giving a new solution, it immediately spotted the mistake, explained why the earlier map didn’t match the actual kernel, and rebuilt everything correctly. So the error wasn’t really a hallucination—more like it defaulted to a familiar formula without checking whether it matched what the homework meant.Aside from that one slip, the model didn’t really hallucinate or spiral. It stayed structured and patient even in the longer reasoning chains, and it didn’t show any of the “I’m stuck so I’m going to guess something wild” behavior LLMs sometimes fall into. When it got something wrong, it was usually because it applied the wrong convention, not because it made something up.Overall, ChatGPT handled the written problems well. It can one-shot most derivations, and when it does mess up, it’s usually easy to steer it back on track with a focused prompt. The combination of clear explanations, good algebraic intuition, and willingness to self-correct makes it a pretty solid tool for these kinds of homework questions.Below is the annotated log:",https://us.edstem.org/courses/84647/discussion/7397817,7397817,Special Participation A: ChatGPT o3 on HW 9
Mistral AI on HW0 written,Mistral AI,Mistral,AI,0,Tianqu He,"Sorry for being so late. I used Le Chat to assist with the conceptual and mathematical parts of homework, specifically the ReLU/SGD analysis and vector calculus derivations. The experience was overwhelmingly positive, with the assistant demonstrating strong expertise in linear algebra, optimization, and neural network dynamics.Strengths:1. Le Chat correctly derived most of the results.2. The assistant provided clear, structured derivations for the optimization, SVD, and MAP estimation perspectives, making complex concepts (like the pseudoinverse and Woodbury identity) accessible.Limitations:While the assistant excelled at algebraic manipulation, it could not directly parse or reason about visual elements (e.g., graph structures or plots). For example, if the problem had included a diagram of the ReLU function or a computational graph, I had to describe it textually for accurate analysis.",https://us.edstem.org/courses/84647/discussion/7307445,7307445,Special Participation A: Mistral AI on HW0 written
Claude Sonnet 4.5 on HW0,Claude Sonnet 4.5,Claude,Sonnet 4.5,0,Justin Yang,"Conversation: https://claude.ai/share/dd45cf31-778b-4d1c-9096-a304ad8c8247 I used Claude 4.5 sonnet to solve problems 2-5 for HW0 written. Summary: Claude was able to answer all the questions fully and correctly all with one shot and basic non-engineered prompts. I was surprised it even generated plots for a question that required drawing plots correctly. Some cons: Some explanations were very lengthy compared to the actual solution.A few solutions used random theorems it considered to be basic/known background knowledge.Some answers were given in a different form than the solution. But I don't think this is a big issue, since specific forms for these were not specified.Consistent latex formatting/compilation issues.",https://us.edstem.org/courses/84647/discussion/7446043,7446043,Special Participation A: Claude Sonnet 4.5 on HW0
Grok HW3,Grok,Grok,,3,Bruno Vieira,"REFLECTION​​In completing the non-coding parts of the homework with Grok, I found that it could one-shot questions about 70–80% of the time. Hints proved extremely useful, as providing a small nudge almost always led to a correct answer, while leaving them out sometimes still allowed Grok to succeed, such as on questions 4C and 4D. This showed that while hints can significantly improve accuracy, the model is capable of reasoning independently in some cases. I also noticed that the model tended to give very long and verbose responses, often providing explanations two to three pages long, even when a short answer would have sufficed. This happened especially when I asked follow-up questions, and Grok would completely lose focus.One particularly striking behavior was how Grok interacted over the course of the chat. When I gave feedback—acknowledging correct answers or asking for more details—it seemed to “learn” from the previous interaction, adjusting its responses accordingly. However, this sometimes caused it to overextend or go beyond the original scope of a question. For instance, in question five, Grok spent over seven minutes thinking about an answer before I had even formally asked the question, showing both its ambition to solve problems and its tendency to act preemptively. Similarly, when I asked multiple follow-up questions, it occasionally lost focus on the original prompt, producing responses that were less relevant or harder to parse.Overall, my experience with Grok highlighted both its strengths and its quirks. It consistently produced correct answers for most questions, and strategic prompting and providing feedback helped guide it effectively. At the same time, its verbosity, occasional hallucinations, and overzealous problem-solving emphasized the need for careful interaction and moderation. Despite these limitations, Grok was a highly capable tool for this homework, and the process offered valuable insights into how modern LLMs balance reasoning, responsiveness, and conciseness in an interactive problem-solving setting.",https://us.edstem.org/courses/84647/discussion/7049136,7049136,Special Participation A - Grok HW3
Mistral on HW8,Mistral,Mistral,,8,Natalie Wei,"OverviewI worked with Mistral’s Le Chat to complete the written questions in Homework 8. First, I gave Mistral a set of rules to follow:1. Read the question and restate it in your own words2. Provide a step-by-step explanation of the solution3. Point out any uncertainties or room for error with your final solutionTo avoid exceeding the model’s context window, I copy-pasted each question as a separate prompt. When Mistral produced an incorrect answer, I attempted to guide it by offering a hint from the approach taken in the staff solution. If it still failed to correct itself, I gave it the staff solution and asked it to explain both why its original answer was wrong and why the solution was correct. Although Mistral was able to explain why the solution worked, I do not feel that it was able to explain its own mistakes. It restated that it was wrong and explained what concepts it missed, but it failed to explain why it missed those concepts to begin with (confusing problem statement, faulty assumptions, etc.). Mistral performed well on the computational and mathematical questions, solving all of them on the first attempt, but it struggled with the more conceptual problems. In particular, it had difficulty with complex time complexity questions: while it could reliably deduce time complexity from a given, straightforward formula, it struggled at analyzing how an algorithm’s time complexity might be improved. Mistral also had trouble changing its approach once it committed to an incorrect approach. Even when given specific hints, and even the staff solution in some cases, it continued to respond incorrectly, giving either the same answer or a different incorrect answer. Additionally, although I asked it to identify sources of error in its solutions, it interpreted this as describing mistakes a human might make as opposed to weaknesses in its own work. This made it challenging to identify why the model was struggling and give it appropriate hints. Overall, the one-shot approach yielded approximately 73.6% accuracy across all of the subproblems; note that I was using the free model and paid models may perform better. I would recommend Mistral for simpler computational tasks, but perhaps not complex conceptual questions that require more reasoning.Annotated LogsQ1Q3Q4",https://us.edstem.org/courses/84647/discussion/7424922,7424922,Special Participation A: Mistral on HW8
Mistral AI on HW4's Non-Coding Portion,Mistral AI,Mistral,AI,4,Akhil Agarwal,"I used Mistral AI's Le Chat to solve the written portion of HW 04. I started by uploading the entire homework PDF for it to read the questions from. I asked it to repeat the question and then answer the question with reasoning. Overall, it performed very well on most of the questions, with no prompting needed for the vast majority of them, while a few of them took extra clarifications. Most of the ""mistakes"" made by Mistral were because of which convention was used in the homework, and things that were not clarified in the problem. However, after a while it also was unable to read a problem from the initial PDF, and the questions had to be fed through screenshots.Q1: It basically one-shotted this entire problem. I just asked it to solve Q1 and it solved it correctly, repeating the problems from the PDF and solving them with structure and reason. However, for the runtime in the solution PDF it includes the 2x constant on the runtime, while Mistral did not. However, when I prompted it saying to include constants, it did include the 2x, so I think it is reasonable as usually runtime means big O runtime, and it was unspecified.Q2: Here I decided to go part by part. On parts a-f, it solved the problems correctly with absolutely no issues (NOTE: on part e, I believe that the answer key is incorrect. I posted about this separately under HW04 Solutions a few days ago. I think the LLM's answer is correct). It was very structured in all of its responses, listing out background, what it needed to find, and then methodically coming up with the solution. However, on 2g, it finally failed. It thought that the parameters in W being unit-scaled meant that ||W||_2 = 1, but that is not the case, as each individual parameter had that property, not the matrix as a whole. It fixed it after some prompting, where I had to specify that it was wrong there. It also was using scaling in isolation, assuming that they do not stack by layer, which I think is reasonable.Q3: It stopped repeating the question here, leading to it not answering the full question where it had to expand the expression for a and b, but it got that after prompting it to. For convolutions, it was using the deep learning convention where the kernel is already flipped, so it was getting incorrect signage, but then it fixed this when I clarified convention.Q4: This question did not involve that much hard calculation and there was no room for ambiguity, so it solved it correctly in one shot.Q7 (Q5-6 were coding questions): For part a, we again flipped the convention back to the deep learning one where the kernel is pre-flipped, which Mistral didn't know and so was refusing to simplify the expression into a convolution, but when told about the convention swap it did it. On 7b, once it had the right question (I fed it through screenshot at this point), it got it correctly, and parts c-d went smoothly.Overall, there were a lot of hiccups, but they were mostly due to convention. I would only count 2g as a problem where it truly made a mistake.Trace: https://chat.mistral.ai/chat/f5eb2483-4b31-4ed2-8c3d-91f605d4de36Annotated Trace:",https://us.edstem.org/courses/84647/discussion/7418177,7418177,Special Participation A: Mistral AI on HW4's Non-Coding Portion
Qwen3-Max on HW02,Qwen3-Max,Qwen,3-Max,2,Cameron Jordan,"I used Qwen3-Max to solve the math problems on HW02 (Problems 1, 2, and 5). Qwen3-Max was able to correctly one-shot all three math question on this homework. At first, I provided just the homework pdf without the stated correction; this caused the model to have significantly more trouble with problem 1(b), but after providing the correction it was able to solve the problem clearly. Additionally, there was only one minor hallucination, related to the ""Important Note"" that it provides in its solution to Problem 1 (b); which did not affect its ability to correctly answer the question.An annotated copy of the conversation (formatted in Latex) can be found here.",https://us.edstem.org/courses/84647/discussion/7423915,7423915,Special Participation A: Qwen3-Max on HW02
DeepSeek V3.2 on HW7,DeepSeek V3.2,Deepseek,V3.2,7,Neil Pattanaik,"Conversation link: https://chat.deepseek.com/share/hilftw4hcw8pevn9vyI used the recently-released DeepSeek V3.2 model (with thinking enabled) to solve the non-coding portion of Homework 7. As expected, the reasoning model one-shotted all the problems it faced, solving multiple parts within a single prompt on each occasion.One challenge with DeepSeek (and, in my experience, almost all commerically-hosted LLMs) is that it can struggle when given web links. Though I included the link to the blog post archive in question 4, DeepSeek still couldn't access it, so I had to upload a PDF copy of the page. Perhaps the bots are forbidden from accessing web archives and the Wayback machine for copyright or safety reasons.",https://us.edstem.org/courses/84647/discussion/7419304,7419304,Special Participation A: DeepSeek V3.2 on HW7
GPT-4o on HW3,GPT-4o,GPT,4o,3,Mihir Rao,"I worked on getting GPT-4o to answer non-coding parts of the homework. For Homework 3, this including both mathematical solutions as well as text answers, so it was interesting to see how 4o handled them. The mathematical solutions also required nuanced subscripts and understand of several variables, and the annotation log I've attached below should demonstrate interesting findings regarding these requirements.One interesting thing I noticed was 4o's ability to process images/pull data. There's a few questions in this homework that rely on not only images, but images that are not in the homework itself. This means 4o has to understand where they are, retrieve them, and then understand what is in them to answer the question. I wanted to test this in a few different ways, so I initially ask 4o to just solve the problem. Then, I give it the image myself and ask it if the answer it provided previously is still good, or needs changing.Similar to what I've experienced previously, I find that these models get worse and worse if they fail the first time. With each trial that goes wrong, they need more and more handholding to get on the right track. For this homework, 4o was able to zero-shot almost everything. In fact, the question it got wrong the first try was one of the simpler ones. Overall, it was really cool to see how these models can understand such a wider range of information in various formats and piece them together to come up with meaningful solutions.",https://us.edstem.org/courses/84647/discussion/7419069,7419069,Special Participation A: GPT-4o on HW3
Gemma 3 (12b params) on HW09 Written Problems,Gemma 3 (12b params),Gemma,3 (12b params),9,Etaash Patel,"Executive Summary:I worked with Gemma 3 on the written problems for Homework 9 (problems 1, 2, 3, 4, and 6). Overall, Gemma performed fairly well (especially given that it is an open-source model that I ran locally on my laptop). For problems 1–4e, which were largely computation problems, Gemma (mostly) produced correct solutions on the first attempt and consistently demonstrated a strong grasp of the underlying concepts.One area where Gemma stood out was its clarity of explanation. Compared to ChatGPT and Claude (the other LLMs I have worked with), I've found Gemma's explanations to be clearer, or explicit, and pedagogically helpful. This, in part, is because Gemma uses more English in their mathematical argument, whereas (in my experience) ChatGPT and Claude tend to be equation-heavy. Gemma's strong explanations (when correct) make it a particularly helpful learning tool, even when it gets some problems wrong. I actually found myself better understanding some of the time and space-complexity arguments around attention mechanisms when trying to guide Gemma to the right solution.Gemma’s weaknesses emerged in two specific areas:1: time- and space-complexity analysis2: dimension checking and tensor shape reasoning.However, even when it produced incorrect bounds or incorrect tensor shapes, it often identified the correct overall strategy, so using Gemma can still be instructive. One of the more striking behaviors appeared toward the end. After many failed attempts and struggling through problems 4f, 4g, and 6, Gemma seemed to exhibit an emergent pattern resembling human frustration. Its responses became less coherent, more speculative, and more willing to guess just to produce something and move on. I found this shift in behavior similar to how a tired human might respond when stuck on a difficult p-set. When Gemma enters this state, the best strategy is simply to start a new conversation for higher-quality answers (however, I continued in the same conversation out of curiosity).Overall, I found Gemma 3 to be an instructive assistant for the written problems. Trace:highlighted themes - clarity of explanations, where Gemma 3 became lost, and emergent ""frustrated"" behavior.",https://us.edstem.org/courses/84647/discussion/7389909,7389909,Special Participation A: Gemma 3 (12b params) on HW09 Written Problems
Gemini 3 Pro Thinking on HW 6 Non-Coding,Gemini 3 Pro Thinking,Gemini,3 Pro Thinking,6,Grant Yang,"Using Gemini 3 Pro Thinking on HW 6 non-coding questions, I was able to observe the following results.Summary: Gemini performed mostly well, one-shotting most subparts even with minimal guidance beyond the problem setup and accompanying graphs/figures. When it made a mistake, it was usually able to reason about its process to identify/clarify the error within a couple of turns. Overall, this is extremely impressive.Recap:I started off by giving it a prefacing prompt: “I will give you problem set questions about deep learning. Think carefully about each question and answer each subpart with a detailed explanation.” I hoped that this would encourage better results and avoid shallow answers.As a baseline, I fed the entire problem (Q2) into the prompt and attached the relevant figures. I was expecting that given the length and density of the prompt, it would struggle to answer correctly, but surprisingly it was able to one-shot most of the subparts correctly, only being slightly wrong on subpart (c). This subpart took a couple rounds of clarifications and hints to correct. Even on more subjective questions, like suggesting possible strategies/interpretations, it gave answers that were either similar to the provided solutions or still technically correct.For Q3, I tried a similar approach by feeding the entire question into the prompt. I again included the image of the graph as the input, without any external help (such as listing the edges or clarifying the table). I expected that this could produce some challenges if Gemini were to be unable to interpret the graph structure correctly. However, after looking at its “thinking” and its reasoning before answering, it actually mostly interpreted both the table values and graph structure from the image, and successfully one-shotted all subparts except for the last one due to a visual error. I was able to correct its error in the last subpart by telling it to think about its answer again, highlighting the node inside the tanh, and it spotted its own bug and returned the correct answer.Conversation link: https://gemini.google.com/share/f77cf2401b50 Annotated log: https://drive.google.com/file/d/1yB1Tdo1rD394SMZJm9QtI5onQhmR6bQS/view?usp=sharing",https://us.edstem.org/courses/84647/discussion/7416689,7416689,Special Participation A: Gemini 3 Pro Thinking on HW 6 Non-Coding
Kimi K2 on HW9,Kimi K2,Kimi,K2,9,William Li,"I utilized Moonshot AI’s Kimi K2 model to tackle the non-coding problems of Homework 9. I first did a pass through of all the non-coding questions, sending each question individually to Kimi with no further prompting to see how the model would perform under one-shot conditions. For most of the questions, Kimi performed very well with just me sending the question to the model, and was able to get it all mostly correct on the first attempt except for the last question (question 6) which was much more difficult and longer.Here are the per-question results on the first run through (which is one-shot only):Question 1: This question was a relatively simple manipulation of expectations and variances, which Kimi was easily able to solve and give justifications for.Question 2: This question involved simple arithmetic (taking dot products of some vectors), which Kimi was easily able to do. This question also involved a conceptual question, which was also correctly answered by Kimi.Question 3: This question involved a simple fill in the blank coding question, which the model was able to get all correct on the first try, along with a justification for each answer. The second part of this question was also answered correctly, with the model coming up with the right changes to the code necessary for making the adjustment the question asked for.Question 4: This question was another mostly fill in the blank blank coding question, which the model again was able to answer correctly on the first try. The question also included two complexity calculations, of which the model was able to successfully answer the time complexity fully. The memory complexity, the model was able to get an answer that was very close to the given solutions, and up on further examination, the answer given by Kimi seems to be the same runtime, just using slightly different notation.Question 6: The question got some of the parts right, but had some minor differences in other parts. For these I asked the model to explain why it had answered part b(iii) the way it did, and asked it to clarify an assumption it made when calculating the time and memory complexities.I was overall very surprised and satisfied with how Kimi was able to solve the noncoding questions. It was able to one-shot the vast majority of the questions, and with simple prompting about its motivations on the incorrect questions, it was able to generate the correct solutions.Trace : https://www.kimi.com/share/19af04a7-bcf2-8f74-8000-0000496c3139Annotated Trace:",https://us.edstem.org/courses/84647/discussion/7415618,7415618,Special Participation A: Kimi K2 on HW9
Deepseek v3.2 on HW 8,Deepseek v3.2,Deepseek,v3.2,8,Justin Li,"I used DeepSeek v3.2 to solve the written portions of HW8, where it performed quite well and one shotted almost all of the problems. One interesting point was that DeepSeek struggled significantly with Problem 1(c), where it repeatedly overlooked/ignored the parallel computation model needed for the solution. I provided 2 nudges to hint DeepSeek towards the right direction, and only after these prompts did DeepSeek converge to the correct big O solution. Through this example I saw that while DeepSeek can correct mistakes when guided, it is not as strong at independently identifying these issues.Strengths:Able to parse matrix expressions, SSM equations, multi-step derivations, and more complex sets of symbolsConsistently one shotted problems on the first attemptAlgebraic Reasoning was strongDetailed explanationsWeaknesses:Struggled a bit with 1c; repeatedly ignored cost of vector and matrix operationsSometimes provided overly long chains of reasoningHere is my annotated conversation with DeepSeek:",https://us.edstem.org/courses/84647/discussion/7405582,7405582,Special Participation A: Deepseek v3.2 on HW 8
Grok on HW4,Grok,Grok,,4,Elizabeth Polito,"Executive Summary:I used Grok to complete the written portion of Homework #4. Since I do not have the paid tier, I used Grok fast. While this is not the top model in the Grok line, it is interesting to evaluate its capabilities from the perspective that it is “150x cheaper than Claude” [1]. Grok was able to one-shot many parts of the problem, but sometimes required corrections to arrive at the correct solution. Notably, the solutions were very long, with significantly more explanation and background included compared to the solutions in the posted answer key. As noted in previous posts, Problem #2 part e on the answer key potentially contains a mistake. Grok got the same answer as indicated by previous Special Participation A posts focusing on this problem set, and when I asked the model to evaluate whether the key’s current solution is reasonable, it provided a strong refutation. For some problems, such as Problem #3 part d and Problem #4 part f, Grok presents work towards one solution and then indicates that it has changed its mind, says the previous work is incorrect, and suggests a new approach, which is somewhat confusing behavior. I inputted all the problems, including problem 3 which contains a figure and Problems #3 and #4 which contain example matrices, into the model, and its ability to “read” these screenshots (using “Grok-1.5 Vision (Grok-1.5V)”) appeared to be accurate throughout. Attached here is a transcript of the conversation and below is a rubric for a quick glance over parts of the problem that the model was able to one-shot vs. required extra hints.",https://us.edstem.org/courses/84647/discussion/7405554,7405554,Special Participation A: Grok on HW4
Gemini (Fast) on HW 9,Gemini (Fast),Gemini,(Fast),9,Divya Ramesh,"Executive Summary:I used Gemini on the non-coding parts of HW 9, and evaluated where it did well and where it didn't. I noticed Gemini could mostly one-shot these questions, especially sections that were very structured math or required analyzing code. I think it mainly failed where the formatting of the question was an issue, and that was just because of LaTeX copy/paste error. To solve these, when I clarified the misreads, Gemini quickly fixed its solution. Even with the formatting problems though, when it was a coding analysis question or there was a lot of context, Gemini still did very well, one shotting almost all the questions. I think the only case where the solution was incorrect showed inconsistency within its own solution, providing the incorrect answer as the header and then showing work to derive the correct answer. This was confusing, but it ultimately did come up with the correct solution. I was also impressed with how quickly it was able to come up with the solutions, taking no longer than 10 seconds at the max. This could also be, however, that I fed each problem in individually, allowing it to see all context for each question as it came up. I also noticed Gemini provided detailed explanations for each question, serving as a great conceptual recap on the topic. It helped me understand how different concepts were related, and it helped me understand derivations for formulas that I previously didn't know. I think overall, it helped me with my understanding of course concepts a lot. One thing I did find a little annoying was Gemini wouldn't let me paste the code I copied from the hw9 pdf file: I needed to paste the question in a different document, and then copy that over to the Gemini search bar. I think maybe Gemini wasn't used to the LaTeX pdf format, and didn't allow those characters. This did make it a little harder to do, but it was just a mild inconvenience!Here is a link to my annotated logs: https://drive.google.com/file/d/11Kqd0IzU7LoCcbPa70mZT0KVR2X1V1Ql/view?usp=sharing",https://us.edstem.org/courses/84647/discussion/7375514,7375514,Special Participation A: Gemini (Fast) on HW 9
Deepseek on HW6,Deepseek,Deepseek,,6,Angelina Zhang,"I used DeepSeek on HW 6 non coding questions and here is my annotated log file with reflections.https://drive.google.com/file/d/1XfIWHSvILyZ-1hMksxc6cLJF1uHOxsKb/view?usp=sharingTL;DR DeepSeek can one-shot most conceptual, math-heavy parts (roughly 70–80% of subparts) with answers very close to the official solution. Its mistakes are structured, not random hallucinations: mainly convention/spec mismatches (row vs column, “no analog needed” parts), and graph/figure misreads.My enforced structure (Restated Problem → Plan → Reasoning → Self-Check) makes its thinking transparent but does not make it truly self-critical. It rarely catches its own deeper mistakes.ReflectionUsing DeepSeek on the non-coding parts of HW6, I found that it followed my structured prompt very well: it restated each question, proposed a plan, walked through step-by-step reasoning, and ended with a self-check. This made its reasoning transparent and easy to annotate. On many subparts, it produced answers that were essentially identical to the official solution on the first try. However, there were two clear failure modes. The first was convention mismatch: for example, in Q2(c) it understood that updates correspond to multiplying by the adjacency matrix, but it hedged between left- and right-multiplication instead of committing to the “rows → multiply on the left” convention that the homework fixes. The second failure mode was misreading the graph: in Q3(c)(iii), it simply used the wrong neighbor sets for nodes 2 and 3, so the final formulas were incorrect despite having the right functional form.What I find interesting is the “Self-Check” sections mostly checked basic sanity and whether the answer addressed the question, but they did not help the model catch these deeper issues. It rarely expressed uncertainty, even in places where the mapping was clearly ambiguous or dependent on the figure. Overall, I think DeepSeek can get you most of the way to a good solution with clear reasoning, but you cannot safely copy its answers blindly. To reach fully correct solutions, you still need to actively verify conventions and adjust answers.",https://us.edstem.org/courses/84647/discussion/7315986,7315986,Special Participation A: Deepseek on HW6
ChatGPT on HW7,ChatGPT,ChatGPT,,7,Faiaz Khan,"I evaluated ChatGPT‑5's capabilities for HW7 non‑coding parts: 3(b), 4, 7, 8, using the hw7 questions and staff‑solutions as ground truth.Bottom line. ChatGPT‑5’s answers are correct. Per‑problem highlights.3(b) PCA & linear autoencoders. Derived the first‑order conditions exactly as in the key—∇W2​​L=2(W2​W1​−I)XX⊤W1⊤​, ∇W1​​L=2W2⊤​(W2​W1​−I)XX⊤—and correctly verified that W2​=Uk​,W1​=Uk⊤​ satisfies them. Verdict: correct. 4 “How to train your ResNet.” Reported 341 s to 94% baseline and 26 s to ≈94.1% final; reflections (b, c) are on‑point (data‑pipeline bottlenecks, batch size/LR scaling, BN precision, “catastrophic forgetting” vs curvature). Verdict: correct (with extra but accurate context). 7 Machine translation. (a) Correctly explains why “vertical stacking” is flawed (breaks variable‑length handling and global conditioning). (b) Teacher forcing tokens enumerated explicitly: <SOS>, I, see, a, dog. (c) Evaluation: <SOS>, then model’s outputs (I, saw, a, dog). Verdict: correct. 8 Self‑supervised linear AEs. (a) Exactly matches the objective: two Linear layers (encoder/decoder), MSELoss, SGD + weight decay; no Dropout/LN/BN. (b) Clear SVD argument that the λ‑regularized optimum favors orthonormal columns in W2​ (minimizing σ2+1/σ2 at σ=1). Verdict: correct. Overall assessment. The model’s responses align with the staff solutions in substance and notation, with small stylistic differences only.",https://us.edstem.org/courses/84647/discussion/7246769,7246769,Special Participation A: ChatGPT on HW7
"HW6, gpt-oss-120b",gpt-oss-120b,GPT,oss-120b,6,Alex Luu,I used gpt-oss-120b with thinking on HW 6 non coding questions. The performance was surprisingly good for a open-source model from a company with flagship proprietary models. It was able to one-shot almost all the questions. This model was also surprisingly fast (although that depends on the hosting provider). The latency was near 0 seconds and token generation was very fast. A more detailed summary is provided in the pdf.,https://us.edstem.org/courses/84647/discussion/7263386,7263386,"Special Participation A: HW6, gpt-oss-120b"
Gemini 2.5 Flash on HW2,Gemini 2.5 Flash,Gemini,2.5 Flash,2,Ruizhe Song,"I interactively engaged Gemini 2.5 Flash on the non-coding parts of Homework 2. Overall, the model was able to arrive at the correct answers in most cases, though several notable issues were observed.Strategies: I first clarified the main role that Gemini was expected to perform and illustrated the evaluation rubrics for its answers. Then, I do the following steps:Step 1: Provide Gemini with one homework problem.Step 2: Collect its initial (one-shot) answer.Step 3: If the answer was incorrect, provide hints to guide Gemini to fix the original answer, and repeat until the response was correct or it seems to have no chance of fixing it.Step 4: Repeat Steps 1–3 for all homework problems.Notable observations and flaws. Although Gemini was able to correctly solve most of the questions in the first answer, it occasionally made logical or mathematical errors that led to incorrect final answers, even when the reasoning process is flawless. In some cases, the model relied on specific examples provided in the prompt, which resulted in partially correct answers. Additionally, Gemini sometimes produced wrong-formed LaTeX code, making its output less readable. This issue may have been partly influenced by the formatting of the input prompts.Here's the full interaction log with my notations and comments, highlighting notable phenomenons happens in Gemini's answer:",https://us.edstem.org/courses/84647/discussion/7244375,7244375,Special Participation A: Gemini 2.5 Flash on HW2
ChatGPT on HW1,ChatGPT,ChatGPT,,1,Junya Tsuneishi,"I used ChatGPT on HW1 no-cording parts(Special Participation A).I posted the results, my findings about them, and my summary on the attached pdf.This is summary from the pdf.Overall, ChatGPT achieved fully correct answers for all problems, demonstrating strong mathematical reasoning and consistency across sequential tasks. However, the interaction also revealed distinctive behavioral patterns in how the model approached problem solving.Accuracy and One-Shot PerformanceFor straightforward conceptual or definitional questions (e.g., explaining optimizer structures or interpreting results), ChatGPT often produced the correct answer immediately (“one-shot”).For more complex derivations or matrix manipulations, it sometimes made conceptual simplification errors rather than arithmetic ones. Nevertheless, this resulted in only one error out of seven big problems.Example of Conceptual OversimplificationIn one problem, the model incorrectly dropped the matrix when simplifying to .This error propagated through subsequent parts, showing that ChatGPT solved sequentially, building upon its previous reasoning.Once provided with a hint, it immediately corrected the mistake—illustrating that the model is particularly strong at conditional reasoning with scaffolding.Style of ReasoningChatGPT consistently produced mathematically formal and symbol-heavy responses, sometimes more rigorous than the problem required.Especially for “explanation” questions, it tended to over-mathematize rather than summarize intuitively. This pattern was observed across multiple sub-questions.Response Latency and ThoughtfulnessThe model typically took 2–5 minutes for moderately complex derivations, suggesting that its “Thinking” mode encouraged stepwise symbolic reasoning rather than rapid generation.Notation and PresentationAll responses were logically consistent with the official solutions, though small notational differences (e.g., transposition order or matrix symbols) occasionally appeared.When a matrix identity was mentioned but not proven in the problem statement, ChatGPT sometimes derived it explicitly, reflecting uncertainty about whether it could assume the result.",https://us.edstem.org/courses/84647/discussion/7219478,7219478,Special Participation A: ChatGPT on HW1
Mistral AI's Le Chat on HW3,Mistral AI's Le Chat,Mistral,AI's Le Chat,3,Jeffrey Cheng,"Here is the online link: https://chat.mistral.ai/chat/8c72d241-dd44-41a0-b8fc-a0469d84ff1dHere is the annotated log:Executive Summary:From my observation, Le Chat was able to answer most written questions correctly on one shot. However, for questions that reference an external research paper, it could misunderstand the problem statement and draw something tangent to what the question is asking. In particular, it could reference a table on a different page or a formula in a different section. Doing some prompt engineering helps the model to reference the correct table/figure.In addition, for questions that involve numerical counting, it could mistake the computation by a small margin, even after engineering the prompt. For instance, it could count something twice and mess up with the calculation.",https://us.edstem.org/courses/84647/discussion/7212131,7212131,Special Participation A: Mistral AI's Le Chat on HW3
Mistral AI's Le Chat on HW5 Written Portion,Mistral AI's Le Chat,Mistral,AI's Le Chat,5,Kian Hekmatnejad,"For Special Participation A, I used Mistral AI's Le Chat on the written portion of HW5. Overall, it performed averagely - mostly arriving at correct answers in one shot, but in a couple of cases requiring further prompting to correct mistakes. I found some indication of a lack of ability to refer to chat history in it's responses.Here is the link to my chat: https://chat.mistral.ai/chat/9c9c7fc7-7985-45a9-b7cd-df337fec5d26",https://us.edstem.org/courses/84647/discussion/7382863,7382863,Special Participation A: Mistral AI's Le Chat on HW5 Written Portion
Qwen on HW7,Qwen,Qwen,,7,Ruihan Xia,"I used Qwen3-Max model to solve the written parts of homework 7. Instead of feeding individual questions with full text, I first uploaded the entire homework PDF and then asked each sub-question only by its label (“7(b)”, “8(a)”, etc.). The goal was to see whether Qwen could (1) avoid forgetting earlier context, (2) correctly retrieve the appropriate problem statement from the embedded PDF, and (3) solve each part in a single attempt. In this experiment Qwen successfully accomplished (1), (2) but failed (3) for questions that require SVD math. One-shot accuracy: ~70%. Qwen performs well for questions that require basic understanding of classic models like encoder-decoder, multiple choice questions, and blog summary. Hallucinations: None observedModel failures: Qwen tends to fail questions with sophisticated mathematical operations like SVD, linear algebra. It tends to stick to the notation in the question and is reluctant to apply more advanced tricks that simplify the expression, but rather to choose to brute force derive. In addition, Qwen might interpret wording like baseline with its own definition (updated baseline rather than benchmark), which might cause confusion if not clearly stated. Context retention: Surprisingly strong—Qwen consistently located the correct question inside the PDF without needing me to restate it.",https://us.edstem.org/courses/84647/discussion/7381174,7381174,Special Participation A: Qwen on HW7
Using Gemini Flash 2.5 on HW11,Using Gemini Flash 2.5,Gemini,Flash 2.5,11,Aaryan Chandna,"Trace: https://gemini.google.com/share/2e206d7da648Math + T/F Question Zero-Shot Performance: 13/15.Prompt Structure: I told the model that it was a DL assistant for me. I gave the model the concepts I was planning to ask it about, and let the model know to output its interpretation of the question, followed by a step-by-step solution and a final answer for each. I also told the model that it should be able to correct itself when needed. For each question, I simply provided images of screenshots of the homework. For some questions, I explicitly told the model what context to consider, when needed. Analysis: The model starts off a bit slow on the very first question (about ideas to adjust LoRA to get better performance). It identifies the main idea correctly, but does not get two of the other ideas from the solutions, even after re-prompting. Following this somewhat slow start, the model goes on a long streak of answering questions thoroughly and correctly. The solutions given by the model are arguably better than the actual HW solutions in some cases. In others, the model is potentially not concise enough, but this is also partially due to my initial prompt, where I asked the model to go step-by-step and then provide a final answer (sort of a CoT way of prompting the model).Interestingly, towards the end, the model starts trying to reattempt questions that it had already solved. For example, when I prompted Gemini to do 6a, it initially tried to resolve question 1 which it had already completed. When I asked it do then do 6a, it initially got the question slightly wrong due to some sort of apparent reading issue (I provided it with an image that it seemed to mis-extract the text of). This signified that Gemini 2.5 Flash may be better leveraged for individual questions, rather than for the entire HW assignment, as context length seems to be an issue here. It did the same thing when asked to solve 6b with an image, as it tried to then solve problem 2. On 6c, it actually got two of the T/F questions wrong. Even when re-prompted with the context again (just in case this was the cause once again, as with 6a), it got them wrong. It seems that the performance of the model gets better and then gets worse, empirically.",https://us.edstem.org/courses/84647/discussion/7380526,7380526,Special Participation A: Using Gemini Flash 2.5 on HW11
Deepseek on HW9,Deepseek,Deepseek,,9,Alex Cao,"Intro:This is an attempt to interact with deepseek on non-coding parts of homework 9. The purpose of this study is to better understand how to prompt/interact with LLMs more effectively and LLM’s capability of solving real life reasoning/math related problems with few-shot prompting. The specific model I interacted with was DeepSeek-V3.2. I used Deepseek’s web ui to interact with the model. I will focus on how different prompting methods (or modes of reasoning) affect a model's one-shot correctness of the problems, and how to improve its accuracy without providing more in context examples. Conversation traces:https://chat.deepseek.com/share/jkzqnyn7j8say9v7jchttps://chat.deepseek.com/share/838vauzbwa2g0ynfbyhttps://chat.deepseek.com/share/pp0exea4mnmt36qfquReport with fully annotated traces:Summary:To sum up, the model (DeepSeek-V3.2) is a quite strong model and is able to solve most of the non coding questions correctly (one-shot). For this specific problem set, two things matter the most for accuracy: 1. Model’s reasoning ability and 2. Correct prompt and context. Prompt and context are very important because if instructions and context are not explicitly told, the model will make certain assumptions, and thus give incorrect answers. For example in question 4, when not stated explicitly, the model thinks it is completing some code function instead of filling blanks for a written question, thus identifying the wrong blanks to fill. The model’s reasoning ability is also very important : for the previous example, even if the correct context and prompt is provided, the model still was not able to identify the correct blanks to fill without the DeepThink feature on. Together, these observations suggest that while DeepSeek-V3.2 is already quite capable on non-coding questions, its performance is highly sensitive to both how we phrase the task and model’s reasoning capabilities. Thus, carefully designing prompts and systematically leveraging features like DeepThink will be essential for reliably getting correct answers and understanding the model's core strengths and weaknesses.",https://us.edstem.org/courses/84647/discussion/7377431,7377431,Special Participation A: Deepseek on HW9
Gemini 2.5 Flash on HW0,Gemini 2.5 Flash,Gemini,2.5 Flash,0,Jason Trinh,"Hey guys — I used Gemini Flash 2.5 for the non-coding parts of HW0, and here’s the quick verdict.TL;DR: Gemini was strong on the “mechanical” math (clean chain rule + indicator notation, solid ridge/SVD manipulations, decent linear-algebra bookkeeping), but its main weakness was qualitative/sign reasoning in the ReLU elbow SGD question, where it overclaimed elbow direction and bungled sign/inequality logic. What it did well: used the 1ϕ(x)>0​ indicator instead of messy piecewise cases, and matched the staff-style update notation w′,b′,e′. Where it slipped: for case (ii), it said “elbow shifts right,” but the elbow can move left or right depending on bias + step size. Clear wrong answer: for (iii) it concluded the elbow “moves left” as the expression gets larger, but the expression is negative so becoming “less negative” means it actually moves right. Even when it got the final direction right: its proof sketch ended with a bogus condition like w>bx.After I nudged it with an “algebra-first + sanity-check” checklist (explicitly compute e′=−b′/w′, be careful when w<0, and verify with a tiny numeric example), Gemini improved a lot. It stopped hand-waving the geometry, caught its own sign mistakes when the numeric check contradicted the algebra, and finally gave consistent conclusions across the cases. The conclusion: Gemini is super useful here, but only if you force it to prove elbow motion by comparing e′ vs e and validating with a quick counterexample.",https://us.edstem.org/courses/84647/discussion/7374016,7374016,Special Participation A: Gemini 2.5 Flash on HW0
Gemini-Pro 3 on HW9,Gemini-Pro 3,Gemini,Pro 3,9,Shervin Goudarzi,"I used Gemini-pro 3 on HW 9 and it performed very well. The main issues with Gemini-pro 3 was the small details in arithmetics that needed correction especially in linear algebra and time/space complexity; however, the solutions were overwhelmingly correct.",https://us.edstem.org/courses/84647/discussion/7373861,7373861,Special Participation A: Gemini-Pro 3 on HW9
Claude on HW6,Claude,Claude,,6,Guohao Lv,"I looked at how well Claude AI could solve the non-coding questions on Homework 6. I provided Claude with the prompts and context from the HW6 PDF, asking it to work through the problems step-by-step.Analysis: Claude was generally able to one-shot the non-coding questions with minimal intervention. For the application/intuition questions (molecular graphs, CNN–GNN analogies, handling missing node values, and scaling/computation of GNNs), Claude’s answers were detailed, on-topic, and made sensible connections to course concepts; if anything, they tended toward being a bit verbose but stayed accurate (maybe not totally accurate, but I think the answers are all reasonable) and grounded in the homework setup.I did not observe clear hallucinations or places where Claude invented nonexistent assumptions; when it extended beyond the literal question (e.g., suggesting multiple practical strategies for missing-feature handling), those additions were still consistent with standard GNN practice. Minor issues were mostly stylistic—some redundancy, slightly heavy notation, and occasional over-explaining—but they did not affect correctness. Overall, Claude’s non-coding HW6 answers show strong reliability on both formal reasoning and conceptual explanation, and would be usable as high-quality solutions or study notes with only light editing for concision.",https://us.edstem.org/courses/84647/discussion/7412632,7412632,Special Participation A: Claude on HW6
ChatGPT-5.1 on HW0,ChatGPT-5.1,ChatGPT,5.1,0,Alena Chao,"I tested ChatGPT's ability to solve HW0 questions 2-5. In general, it was able to one-shot the problems while explaining its reasoning, most likely because many of the problems review fundamental ML/math concepts.",https://us.edstem.org/courses/84647/discussion/7414931,7414931,Special Participation A: ChatGPT-5.1 on HW0
GPT 5.1 Thinking on Homework 12,GPT 5.1 Thinking,ChatGPT,5.1 Thinking,12,Trenton O'Bannon,"Conversation Link - https://chatgpt.com/share/6933c8cc-4f6c-8012-9651-4b391b6f512bI used ChatGPT (GPT-5.1 Thinking) to work through the non-coding parts of a CS182 homework (the questions I focused on were mostly about autoencoders/VAEs and ridge/self-attention). I interacted with it like a tutor: rather than just asking “what’s the answer to 3(b)?”, I had it re-teach concepts (encoder vs autoencoder vs VAE, Gaussians, randomness in the latent space, etc.), and then apply those ideas to the actual sub-questions.Executive summaryOverall performance. Roughly speaking, the model could one-shot a majority of the subparts (I’d estimate ~60–70%) when the question was conceptual, algebraic, or about interpreting a loss function. When it struggled, it was usually on:subtle complexity analysis (missing or adding extra factors),unclear encoder/decoder notation in a complicated objective,or anything that required actually seeing a graph/figure.Strengths.Very good at re-explaining core concepts from the ground up (e.g., what a Gaussian is, what an encoder is, what a VAE is, and how noise injection works).Pretty good at mapping math to intuition (e.g., explaining where randomness enters a VAE and what it does, or what different terms in an ELBO correspond to).When I pushed it to slow down and justify each step, it usually corrected itself and converged to the right answer.Weaknesses / failure modes.It tends to be overconfident on complexity questions and sometimes adds an extra factor of n or d in the big-O expression.It occasionally blurred the roles of p and q in the VAE loss until I explicitly forced it to pin down “which one is the encoder, which one is the decoder.”It can’t read tiny graphs/plots from the PDF, so for at least one “pick the most U-shaped curve” style question, it had to rely entirely on my textual description rather than the actual image.Hallucinations. I didn’t catch any wild hallucinations like made-up theorems or algorithms, but I did see:Confident but slightly wrong complexity bounds (e.g., overcounting a matrix dimension).Slightly different forms of the loss than what we use in class (equivalent up to constants or sign conventions, but not exactly what appears in the homework).Overall, my takeaway is that GPT-5.1 is very capable as a conceptual tutor, and “pretty good but not fully trustworthy” as an answer-oracle. You need enough understanding to check its work and push back.Question-by-question behavior (high level)(You can adjust this to match your exact HW number / question labels.)Question 3 – Autoencoders, VAEs, and ELBO termsWhat I tested it on:Definitions of encoder, autoencoder, and variational autoencoder.Intuition behind Gaussians and the reparameterization trick.Interpreting the VAE loss (reconstruction + KL) and mapping pieces to encoder/decoder.A multiple-choice subpart involving a “most U-shaped” curve where I had to interpret the plot.Where it did well (one-shot or close):Gave a clear, layered explanation of:what an encoder does,what an autoencoder does,how a VAE differs, andwhy we add noise (sampling in latent space, regularization, smoothness).Re-taught Gaussians in simple terms and connected that to the Gaussian latent prior in the VAE.When I asked “where is this randomness/noise coming from and what does it do?”, it correctly pointed to the sampling from z∼qϕ​(z∣x) and explained that the reparameterization trick makes this differentiable.For the VAE objective, once prompted, it correctly identified that:qϕ​(z∣x) corresponds to the encoder, andpθ​(x∣z) corresponds to the decoder.Where it struggled / needed dragging:Given just the “complicated ass loss function,” it initially didn’t explicitly label which term belonged to encoder vs decoder—so I had to ask directly: “is it safe on a midterm to assume q is the decoder and p is the encoder, or what should I look for?” That pushed it to clarify that q is the encoder (approx posterior) and p is the decoder (likelihood).For the graph-based subpart (“most U-shaped is option B”), the model couldn’t see the graph properly. I ended up using its conceptual explanation of what the curve should look like, combined with my own visual inspection, to pick the correct option. So it helped, but it wasn’t independently solving that one.Question 4 – Ridge regression / self-attention-style math (non-coding parts only)What I used it for:Conceptual pieces around online/recursive updates, ridge-style penalties, and how these tie into efficient self-attention.Understanding how different terms in the loss or update equations affect computational cost and memory.Strengths:Gave good explanations of why we care about efficient updates and connected them to the idea of reusing previous computations instead of recomputing from scratch.When I asked it to “explain from the ground up” it was good at turning each step into something intuitive (e.g., “this part is like keeping a running summary; this part is like correcting your estimate with the new data point”).Weaknesses:On some of the big-O complexity questions, it tended to overshoot, e.g., something that could be done in O(nd2) it might initially describe as O(nd3) until I forced it to count operations more carefully.This is a recurring theme: it knows the right algorithmic idea, but it’s sloppy about exact asymptotics unless you police it.Questions 5 and 7 – Additional conceptual / non-coding partsHere I mainly used it as a concept explainer and sanity-check, not just a direct answer machine.It gave reasonable answers on first pass, but the real value was in:rephrasing the question,highlighting which quantities matter (e.g., what’s being regularized, what’s being predicted),and giving analogies that made the math feel less abstract.At the end, I also asked it to evaluate my level of preparedness for the final based on my questions. That response was more “vibe-check” than science, but it was helpful for me to see what it thought my weak spots were (mostly: being shaky at mapping formulas to pictures and at carefully tracking complexity).Strategies I used to steer the modelIn the annotated log I’m attaching, I call out some of the strategies I used:Force “from-first-principles” explanations. Instead of just “what’s the answer to 3(b)?”, I first asked it to re-teach encoders/autoencoders/VAEs and Gaussians “from the ground up”. This made it expose assumptions and definitions I could later check.Ask targeted follow-ups when something feels hand-wavy. Example: “Where is this randomness/noise coming in from? What does it do?” and “How do we tell from that complicated loss which is the encoder vs decoder?” These forced it to pin down the role of each term instead of staying vague.Use it to reason about shapes/curves, then apply my own visual check. For the “most U-shaped” graph, it couldn’t see the figure, so I had it describe what shape we should expect, then I matched that to the multiple-choice options.Push back on complexity claims. When it gave a complexity I didn’t trust, I asked it to explicitly count matrix-vector vs matrix-matrix ops and justify the O(⋅). This usually surfaced the mistake and got it to correct itself.TakeawayFor this homework, GPT-5.1 Thinking was not a magic “give me the solution key” button, but it was a strong tutor for the non-coding parts:It can one-shot many conceptual subparts,It sometimes fumbles details, especially asymptotic complexity,And it absolutely requires an engaged human who’s willing to question it, check it against the official solutions, and drag it when it’s confident but wrong.That’s the perspective I captured in the attached annotated conversation trace.",https://us.edstem.org/courses/84647/discussion/7419018,7419018,Special Participation A: GPT 5.1 Thinking on Homework 12
GPT-5.1 on HW 9,GPT-5.1,ChatGPT,5.1,9,Lenci Ni,"I used GPT-5.1 to help with the written (non-coding) parts of Homework 9: Problems 1, 2, 3, 4, and 6. I included screenshots of the problem statements, and my prompt for each problem followed the structure:For each subpart, please: (1) restate the problem in your own words, (2) explain the main idea before doing any calculations, (3) show the full step-by-step solution with no big jumps, and (4) summarize the final answer clearly. I have attached screenshots of each problem.Summary: GPT-5.1 handled these questions very well. It reliably extracted equations from screenshots, followed the structured prompt, and produced organized, readable solutions. Most final answers matched the official solutions.Strengths: GPT-5.1 did very well across all the written parts. It was able to correctly parse mathematical expressions, notation, and multi-step problems from screenshots reliably, even when the problem statements was spread across multiple images. The solutions given by the LLM followed the structure that was specified in the prompt, with each answer including a restatement of the subproblem, a plan, detailed step by step derivations, and a summary. For most subparts, GPT-5.1 was able to one-shot the correct solution with no iteration needed. Many explanations were not only correct, but also more detailed than the official solutions. Overall, it demonstrated strong reasoning and required little follow up guidance.Weaknesses: The main issue I encountered was an occasional misinterpretations of notation or implicit conventions in the problem statement. The main example was in Q6, where GPT-5.1 defaulted to the homogeneous quadratic kernel rather than the degree-2 polynomial kernel expected by the assignment. Once I explicitly asked about the missing constant term, it corrected itself immediately, so this was not a hallucination but more so like falling back on a common default kernel definition. In addition, some of the complexity analyses, while correct, could have been clearer, and would benefit from including pseudocode. Overall, most of the issues were minor and the model had sound reasoning.",https://us.edstem.org/courses/84647/discussion/7405370,7405370,Special Participation A: GPT-5.1 on HW 9
HW 4 using DeepSeek,DeepSeek,Deepseek,,4,Srikar Babu Gadipudi,"Problem ContextThere are 5 non-coding questions in this homework. Two questions on optimization (specifically Newton-Schultz iteration and MuP scaling) and three questions on CNNs. I took a special interest in how DeepSeek performs when solving these computationally heavy problems when prompted in different formats: purely text, purely images, a PDF and a hybrid of text and images. I also tested DeepSeek's ability to understand and retain multi-part instructions in between these different modalities. The idea was to evaluate DeepSeek's overall problem-solving performance while also attempting to understand which prompt type yielded the best results. In this experiment:DeepSeek solved four out of five questions correctly in a single attempt.Question 2 was the only one it got wrong, though it could solve it with further prompting.Question 3 required a brief nudge from me to complete the final computation, likely because the context limit was reached mid-session.Executive SummaryFrom testing out different formats of the prompts, I observed the following:When only text was provided (Question 2), DeepSeek tended to lose context more easily. it sometimes failed to connect earlier parts of the questions or instructions.With images or PDFs, it demonstrated significant better context retention and reasoning continuity. It appeared to process the questions more holistically, possibly due to the visual cues.A hybrid approach (text +images) also performed better. It was able to mix information from images and text efficiently to provide the correct solution (see Question 4).In essence, DeepSeek performs impressively well on conceptual and numerical reasoning when the sufficient structured context is provided. Its performance, however, degraded slightly in purely textual prompts, indicating that context formatting plays a role in achieving accurate results.Note: This analysis is based on a limited sample of five questions, and is therefore not exhaustive or conclusive.Here is a link to the conversation I had with DeepSeek: https://chat.deepseek.com/share/jsuxr35m8bhnxyzz7bHere is the file containing the conversation, with question-wise comments (annotated in the PDF):",https://us.edstem.org/courses/84647/discussion/7132324,7132324,Special Participation A: HW 4 using DeepSeek
MistralAI's Le Chat on HW10 Written portion,MistralAI's Le Chat,MistralAI,'s Le Chat,10,Fantine Mpacko Priso,"For special participation A, I used MistralAI's Le Chat to solve HW10 written portion.Overall, the model did quite well on the conceptual and algebraic parts, but struggled on the subtle complexity analysis:For the math derivations (e.g., rewriting softmax with a Gaussian kernel) and the conceptual questions (kernel intuition, causal masking, FaceNet, triplet loss), its answers were correct and aligned with the official solutions, often with clear step-by-step reasoning.For the more delicate algorithmic complexity question (kernelized attention with random features), it gave a plausible but wrong answer, keeping an unnecessary (N^2) term and missing the whole “linear in (N)” point of the trick.When I confronted it with the official solution, it was good at post-hoc analysis: it compared its result, admitted the mistake, and explained why its reasoning had been too coarse.It did not spontaneously flag uncertainty on that hard question; it sounded confident while being wrong.So: strong on standard derivations and conceptual ML, weaker and over-confident on fine-grained complexity / algorithmic details — which is exactly the type of behavior we were aware it could have.The raw chat is available here.",https://us.edstem.org/courses/84647/discussion/7405559,7405559,Special Participation A - MistralAI's Le Chat on HW10 Written portion
Gemini 2.5 Pro on HW5,Gemini 2.5 Pro,Gemini,2.5 Pro,5,Kithmini Herath,"I used Gemini 2.5 Pro to solve the written parts of Homework 5 (Q1-4). I started by mentioning that I wanted to solve a problem set related to a specific topic (in this case, basics of CNNs) and went over the four problems one by one with attached screenshots of the entire problem. I’ve attached an annotated pdf of my interactions with the LLM and a link to my original chat. In summary, even without extensive context in my typed prompt, Gemini 2.5 Pro was able to extract information included within the problem from the screenshots themselves and one-shot solve almost all the problems correctly without any hallucinations. Notable observations: Good parsing of information - Most of the time it correctly parsed all mathematical equations, figures and text from the screenshots, even when a problem was spread across multiple screenshots. This is evident because it often types back key information from the problem statement to formulate the answer. All final answers (after corrections noted below) had no mathematical or conceptual errors. Misinterpretation of a hint - In question 1b), it used the hint to solve the problem itself. I wonder if this is because it mostly encountered instances of math problems where the hint is used to solve the problem itself. However, when I went through the other special participation threads for HW5 (that were posted as of 11/12/25), I noticed that none of those LLMs used the hint this way and often correctly interpreted it. So, it was interesting to see this behavior from Gemini. When I pointed this misinterpretation to Gemini, it acknowledged the mistake and solved the problem correctly using substitutions for the system of equations. Overly complicated solutions - In this homework I would like to point out to its answers for the subparts in question 2b) in the attached pdf. While none of the approaches were incorrect, there were steps that were unnecessary to build the solution (annotated in the pdf). Incomplete reasoning - The answer to 4b) could have been more complete. One could see that from the first term we can derive the transformation we need to apply to $\check{w}$ to get to equation (3) (as the LLM did). However, I think the answer would be complete if it mentioned what should also happen to the $\Gamma$ matrix (even though this was not explicitly asked in the question) for the equation to achieve the form in equation (3) rather than treating it as something obvious. In conclusion, I think Gemini 2.5 Pro is fully capable of answering all questions correctly with good mathematical and conceptual reasoning. However, some of the solution approaches could be more concise and complete.",https://us.edstem.org/courses/84647/discussion/7297480,7297480,Special Participation A: Gemini 2.5 Pro on HW5
Gemini 2.5 Fast on Homework 08,Gemini 2.5 Fast,Gemini,2.5 Fast,8,Mishty Dhekial,"I utilized the Gemini 2.5 Fast model to tackle the non-coding problems of Homework 8. I first solved Questions 1, 3 and 4 based solely on the problem description in the uploaded PDF. I then used the provided solution key to perform a comparison and evaluate my one-shot performance.The model was able to one-shot the vast majority of the analytical and conceptual parts correctly on the first attempt. However, it still struggled a bit in a few of the problem portions.Here are the per-question results on the first run through:Question 1Correct The derivation of the kernel $K$ (part a), the concrete examples (part b), the critical path comparison (part c), and the conceptual analysis for diagonal (part e) and DPLR matrices (part f) were all correct.Struggled In part (d), while the correct highly parallel method and the $\mathbf{O(\log L)}$ dependency on sequence length were correctly identified, the final critical path expression was $\mathbf{O(\log L \cdot n^3)}$. The class solution omits the matrix dimension factor $n$, which is not the case for Gemini. Similar patterns are present in $e$ and $f$.Question 3CorrectThe mathematical derivation for the optimal weight matrix $\hat{W}$ in part (b) ii was correct and followed the most elegant geometric invariance approach shown in the key. The determined range for $\lambda$ in part (c), $\mathbf{1 \le \lambda \le 4}$, was also correct based on the $\mathbf{80\%}$ preservation and $\mathbf{50\%}$ attenuation requirements specified in the problem.StruggledIn part (a) Gemini, was unable to fill in the missing Regularization Loss for encoder $\mathbf{W^{(\beta)}}$. This is because the matrix $W^{(\beta)}$ was not properly extracted from the uploaded PDF.Question 4CorrectAll conceptual and analytical parts were solved correctly. This included the efficient average update in part (a), the decomposition of ridge components in part (b), the computational cost of the non-causal attention $O(n d^2)$ in part (c), the Sherman-Morrison formula cost $O(d^2)$ in part (e), and the conceptual form for the attention weights in part (g).StruggledIn part (d) \lambda$ isn't chosen and in part (f) (Efficient Causal Ridge-Self-Attention), Gemini's initial complexity analysis for the recursive update was $\mathbf{O(n d^3)}$ total. However, the optimal solution (provided in the key) demonstrates that the output can be calculated using only matrix-vector products, which cost only $O(d^2)$ per time step. This optimal path yields an overall complexity of $\mathbf{O(n d^2)}$. Gemini struggled to identify the most computationally efficient $O(d^2)$ path for the final calculation step.I have attached my  annotated conversation trace below.",https://us.edstem.org/courses/84647/discussion/7417556,7417556,Special Participation A: Gemini 2.5 Fast on Homework 08
Gemini 3.0 Pro (Thinking) on HW4,Gemini 3.0 Pro (Thinking),Gemini,3.0 Pro (Thinking),4,Tiger Zhang,"Executive summary:Following the release of Gemini 3.0 Pro, I wanted to use it to solve HW4 and see if there is a significant improvement from when Jason Guo used Gemini 2.5 Pro to solve it.I split the homework pdf into a few pdfs, one with each problem, and gave those to Gemini sequentially.Results:There is a noticeable improvement in Gemini's performance on HW4. In particular, Gemini 3.0 Pro almost one-shotted all the problems, even more than did Gemini 2.5 Pro. It did make one reading mistake for two matrices (and so did Gemini 2.5 Pro), but it made less reading mistakes, and was able to quickly fix its mistake when prompted again.Outside of that, Gemini only made a couple of very small and understandable mistakes. Even its calculations were fully correct. I have no significant complaints about how Gemini did for any problem.Annotated log:",https://us.edstem.org/courses/84647/discussion/7428749,7428749,Special Participation A: Gemini 3.0 Pro (Thinking) on HW4
Getting GPT 5.1 to answer Homework 12,Getting GPT 5.1 to answer,ChatGPT,5.1,12,Sriram Srivatsan,"I got OpenAI's GPT 5.1 model to answer questions 1, 2, and 4 in homework 12. Overall, it seems that this model is able to answer questions about the material extremely accurately, and sometimes it even notes that specific ambiguities exist within the questions themselves. This is quite interesting, because it solved parts of question 2 that I wasn't able to approach myself without getting guidance from some of its answers.Here is the writeup: Here is the chat history: And the solutions and ambiguities it noted in for the homework:",https://us.edstem.org/courses/84647/discussion/7425035,7425035,Special Participation A: Getting GPT 5.1 to answer Homework 12